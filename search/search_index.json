{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AI Code \ud83d\udc4b","text":"<p>AI-Code is an open-source project designed to help individuals learn and understand foundational code implementations of various AI algorithms, providing structured guides, resources, and hands-on projects across multiple AI domains like ML, DL, NLP, and GANs.</p>"},{"location":"#overview","title":"\ud83c\udf1f Overview","text":"<p>AI-Code simplifies learning AI technologies with easy-to-follow code and real-world project guides for ML, DL, GAN, NLP, OpenCV, and more.</p>"},{"location":"#core-features","title":"\ud83d\udd11 Core Features","text":"<ul> <li>Scratch-level implementations of AI algorithms \ud83e\udde0</li> <li>Guides, datasets, research papers, and step-by-step tutorials \ud83d\udcd8</li> <li>Clear directories with focused README files \ud83d\udcc2</li> <li>Fast learning with minimal complexity \ud83d\ude80</li> </ul>"},{"location":"#tech-stack","title":"\ud83d\udee0\ufe0f Tech Stack","text":"<ul> <li>Python 3.9+</li> <li>Mk Docs (A Python Package)</li> <li>Markdown</li> <li>Git/GitHub</li> <li>VS Code</li> </ul>"},{"location":"contribute/","title":"How to Contribute? \ud83d\ude80","text":"<p>Welcome to the AI-Code project! Follow these easy steps to get started. Let's build some awesome AI projects together! \ud83d\ude04</p>"},{"location":"contribute/#star-this-repository","title":"\ud83c\udf1f Star this Repository","text":"<p>Help us grow by starring the project! \ud83c\udf1f This lets others discover and contribute to the repository.</p>"},{"location":"contribute/#fork-this-repository","title":"\ud83c\udf74 Fork this Repository","text":"<p>Click the Fork button in the top right corner of this page to create a personal copy of this repository.</p>"},{"location":"contribute/#clone-the-forked-repository","title":"\ud83d\udce5 Clone the Forked Repository","text":"<p>Once you've forked the repository, clone it to your local system:</p> <pre><code>git clone https://github.com/&lt;your-github-username&gt;/AI-Code.git\n</code></pre>"},{"location":"contribute/#navigate-to-the-project-directory","title":"\ud83d\udcc2 Navigate to the Project Directory","text":"<p>Move into the directory where you've cloned the project:</p> <pre><code>cd AI-Code\n</code></pre>"},{"location":"contribute/#create-a-new-branch","title":"\ud83c\udf31 Create a New Branch","text":"<p>It's best to work on a new branch for your changes:</p> <pre><code>git checkout -b &lt;your_branch_name&gt;\n</code></pre>"},{"location":"contribute/#create-a-virtual-environment","title":"\ud83d\udee0\ufe0f Create a Virtual Environment","text":"<p>Creating a virtual environment helps manage dependencies and avoid conflicts:</p> <pre><code>python -m venv myenv\n</code></pre>"},{"location":"contribute/#activate-the-virtual-environment","title":"\u26a1 Activate the Virtual Environment","text":"<p>Activate the environment to install the required dependencies.</p> <ul> <li>Windows:</li> </ul> <pre><code>myenv\\Scripts\\activate\n</code></pre> <ul> <li>macOS/Linux:</li> </ul> <pre><code>source myenv/bin/activate\n</code></pre>"},{"location":"contribute/#install-required-python-packages","title":"\ud83d\udce6 Install Required Python Packages","text":"<p>Install all necessary dependencies using pip:</p> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"contribute/#start-the-development-server","title":"\ud83d\ude80 Start the Development Server","text":"<p>Run the following command to start the server locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>Open your browser and go to:</p> <pre><code>http://127.0.0.1:8000/AI-Code/\n</code></pre> <p>This will allow you to preview the website locally. You're ready to start contributing!</p>"},{"location":"contribute/#make-changes","title":"\u270f\ufe0f Make Changes","text":"<p>Make any changes or improvements you want to the code or documentation. Be sure to follow the project's guidelines!</p>"},{"location":"contribute/#stage-your-changes-and-commit","title":"\ud83d\udcbe Stage Your Changes and Commit","text":"<p>Once you're satisfied with your changes, stage and commit them:</p> <pre><code>git add .\ngit commit -m \"&lt;your_commit_message&gt;\"\n</code></pre>"},{"location":"contribute/#push-your-changes","title":"\ud83d\ude80 Push Your Changes","text":"<p>Push your local commits to your forked repository:</p> <pre><code>git push -u origin &lt;your_branch_name&gt;\n</code></pre>"},{"location":"contribute/#create-a-pull-request","title":"\ud83d\udcdd Create a Pull Request","text":"<p>Go to your forked repository on GitHub, and click the Create Pull Request button. Describe your changes and submit the PR.</p>"},{"location":"contribute/#congratulations","title":"\ud83c\udf89 Congratulations!","text":"<p>You've made your first contribution to AI-Code! \ud83c\udf89 Thank you for helping improve the project! Keep it up and happy coding! \ud83d\ude80</p>"},{"location":"contribute/#need-help","title":"\ud83d\udcac Need Help?","text":"<p>If you run into any issues, feel free to open an issue on GitHub Issues. We're here to help! \ud83e\udd17</p>"},{"location":"algorithms/","title":"Algorithms \ud83d\udd37","text":"Statistics <p>Understanding data through statistical analysis and inference methods.</p> Machine Learning <p>Dive into the world of algorithms and models in Machine Learning.</p> Deep Learning <p>Explore the fascinating world of deep learning.</p> Computer Vision <p>Learn computer vision with OpenCV for real-time image processing applications.</p> Natural Language Processing <p>Dive into how machines understand and generate human language.</p> Generative Adversarial Networks <p>Learn about the power of Generative Adversarial Networks for creative AI solutions.</p> Large Language Models <p>Explore the cutting-edge techniques behind large language models like GPT and BERT.</p> Artificial Intelligence <p>Explore the fundamentals and advanced concepts of Artificial Intelligence.</p>"},{"location":"algorithms/artificial-intelligence/","title":"Artificial Intelligence \ud83d\udca1","text":"Search and Optimization <p>Efficient problem-solving through systematic searching and optimization.</p> Knowledge Based Systems <p>AI-driven systems that mimic human expertise for decision-making.</p> Expert Systems <p>Simulating expert knowledge to solve complex real-world problems.</p> Reinforcement Learning <p>Learning through interaction to maximize rewards and improve performance.</p> Evolutionary Algorithms <p>Optimizing solutions through processes inspired by natural selection.</p>"},{"location":"algorithms/artificial-intelligence/evolutionary-algorithms/","title":"Evolutionary Algorithms \ud83d\udca1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/artificial-intelligence/expert-systems/","title":"Expert Systems \ud83d\udca1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/artificial-intelligence/knowledge-based-systems/","title":"Knowledge Based Systems \ud83d\udca1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/artificial-intelligence/reinforcement-learning/","title":"Reinforcement Learning \ud83d\udca1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/artificial-intelligence/search-and-optimization/","title":"Search and Optimization \ud83d\udca1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/computer-vision/","title":"Computer Vision \ud83c\udfa5","text":"Image Processing <p>Techniques for manipulating images to improve quality or extract features.</p> Object Detection <p>Identifying and localizing objects within images or video.</p> Semantic Segmentation <p>Dividing an image into regions with semantic meaning for analysis.</p> Image Augmentation <p>Enhancing training data by applying transformations to images.</p>"},{"location":"algorithms/computer-vision/image-augmentation/","title":"Image Augmentation \ud83c\udfa5","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/computer-vision/image-processing/","title":"Image Processing \ud83c\udfa5","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/computer-vision/object-detection/","title":"Object Detection \ud83c\udfa5","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/computer-vision/semantic-segmentation/","title":"Semantic Segmentation \ud83c\udfa5","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/deep-learning/","title":"Deep Learning \u2728","text":"Neural Networks <p>Fundamental models of deep learning, inspired by biological neurons.</p> Optimization Algorithms <p>Methods to improve the performance of neural networks through efficient learning.</p> Architectures <p>Different structures for neural networks, designed for specific tasks and challenges.</p> Pre-Trained Models <p>Ready-to-use models that can be fine-tuned for specific tasks.</p>"},{"location":"algorithms/deep-learning/architectures/","title":"Architectures \u2728","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/deep-learning/neural-networks/","title":"Neural Networks \u2728","text":"Convolutional Neural Networks <p>Deep learning algorithm for image and video recognition.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 3 mins</p>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/","title":"Convolutional Neural Networks","text":""},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#overview","title":"Overview","text":"<p>Convolutional Neural Networks (CNNs) are a type of deep learning algorithm specifically designed for processing structured grid data such as images. They are widely used in computer vision tasks like image classification, object detection, and image segmentation.</p>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#how-cnns-work","title":"How CNNs Work","text":""},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#1-architecture","title":"1. Architecture","text":"<p>CNNs are composed of the following layers: - Convolutional Layers: Extract spatial features from the input data. - Pooling Layers: Reduce the spatial dimensions of feature maps to lower computational costs. - Fully Connected Layers: Perform high-level reasoning for final predictions.</p>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#2-key-concepts","title":"2. Key Concepts","text":"<ul> <li>Filters (Kernels): Small matrices that slide over the input to extract features.</li> <li>Strides: Step size of the filter movement.</li> <li>Padding: Adding borders to the input for better filter coverage.</li> <li>Activation Functions: Introduce non-linearity (e.g., ReLU).</li> </ul>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#cnn-algorithms","title":"CNN Algorithms","text":""},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#1-lenet","title":"1. LeNet","text":"<ul> <li>Proposed By: Yann LeCun (1998)</li> <li>Use Case: Handwritten digit recognition (e.g., MNIST dataset).</li> <li>Architecture:</li> <li>Input \u2192 Convolution \u2192 Pooling \u2192 Convolution \u2192 Pooling \u2192 Fully Connected \u2192 Output</li> </ul>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#2-alexnet","title":"2. AlexNet","text":"<ul> <li>Proposed By: Alex Krizhevsky (2012)</li> <li>Use Case: ImageNet classification challenge.</li> <li>Key Features:</li> <li>Uses ReLU for activation.</li> <li>Includes dropout to prevent overfitting.</li> <li>Designed for GPUs for faster computation.</li> </ul>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#3-vggnet","title":"3. VGGNet","text":"<ul> <li>Proposed By: Visual Geometry Group (2014)</li> <li>Use Case: Image classification and transfer learning.</li> <li>Key Features:</li> <li>Uses small 3x3 filters.</li> <li>Depth of the network increases (e.g., VGG-16, VGG-19).</li> </ul>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#4-resnet","title":"4. ResNet","text":"<ul> <li>Proposed By: Kaiming He et al. (2015)</li> <li>Use Case: Solving vanishing gradient problems in deep networks.</li> <li>Key Features:</li> <li>Introduces residual blocks with skip connections.</li> <li>Enables training of very deep networks (e.g., ResNet-50, ResNet-101).</li> </ul>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#5-mobilenet","title":"5. MobileNet","text":"<ul> <li>Proposed By: Google (2017)</li> <li>Use Case: Mobile and embedded vision applications.</li> <li>Key Features:</li> <li>Utilizes depthwise separable convolutions.</li> <li>Lightweight architecture suitable for mobile devices.</li> </ul>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#code-example-implementing-a-simple-cnn","title":"Code Example: Implementing a Simple CNN","text":"<p>Here\u2019s a Python example of a CNN using TensorFlow/Keras:</p> <ul> <li>Sequential:   Used to stack layers to create a neural network model.</li> <li>Conv2D:       Implements the convolutional layers to extract features from input images.</li> <li>MaxPooling2D: Reduces the size of feature maps while retaining important features.</li> <li>Flatten:      Converts 2D feature maps into a 1D vector to pass into fully connected layers.</li> <li>Dense:        Implements fully connected (dense) layers, responsible for decision-making.</li> </ul> <pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Build the CNN\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')  # Replace 10 with the number of classes in your dataset\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Summary\nmodel.summary()\n</code></pre>"},{"location":"algorithms/deep-learning/neural-networks/convolutional-neural-network/#visualizations","title":"Visualizations","text":"<ul> <li>Filters and Feature Maps: Visualizing how the CNN learns features from images.</li> <li>Training Metrics: Plotting accuracy and loss during training.</li> </ul> <pre><code>import matplotlib.pyplot as plt\n\n# Example: Visualizing accuracy and loss\nplt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n</code></pre>"},{"location":"algorithms/deep-learning/optimization-algorithms/","title":"Optimization Algorithms \u2728","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/deep-learning/pre-trained-models/","title":"Pre-Trained Models \u2728","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/generative-adversarial-networks/","title":"Generative Adversarial Networks \ud83d\udcb1","text":"Auxiliary Classifier Generative Adversarial Network <p>Empowering GANs with Class-Specific Data Generation.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 3 mins</p> Basic Generative Adversarial Network <p>Hand writing digit resembler from MNIST dataset</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p> Conditional Generative Adversarial Network <p>Controlled Image synthesis</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p> Energy Based Generative Adversarial Network <p>Minimize the energy functiom to more stable training process.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p> Information Maximizing Generative Adversarial Network <p>Empowering Data-Driven Insights with Generative Adversarial Networks for Advanced Information Synthesis.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/","title":"AC GAN","text":""},{"location":"algorithms/generative-adversarial-networks/ac-gan/#overview","title":"Overview","text":"<p>Auxiliary Classifier Generative Adversarial Network (ACGAN) is an extension of the traditional GAN architecture. It incorporates class information into both the generator and discriminator, enabling controlled generation of samples with specific characteristics.</p> <p>ACGANs can: - Generate high-quality images conditioned on specific classes. - Predict class labels of generated images via the discriminator.</p> <p>This dual capability allows for more controlled and targeted image synthesis.</p>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#key-concepts","title":"Key Concepts","text":"<ol> <li> <p>Generator:</p> <ul> <li>Takes random noise and class labels as input to generate synthetic images conditioned on the class labels.</li> </ul> </li> <li> <p>Discriminator:</p> <ul> <li>Differentiates between real and fake images.</li> <li>Predicts the class labels of images.</li> </ul> </li> <li> <p>Class Conditioning:</p> <ul> <li>By integrating label embeddings, the generator learns to associate specific features with each class, enhancing image quality and controllability.</li> </ul> </li> </ol>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#implementation-overview","title":"Implementation Overview","text":"<p>Below is a high-level explanation of the ACGAN implementation:</p> <ol> <li> <p>Dataset:</p> <ul> <li>The MNIST dataset is used for training, consisting of grayscale images of digits (0-9).</li> </ul> </li> <li> <p>Model Architecture:</p> <ul> <li>Generator:<ul> <li>Takes random noise (latent vector) and class labels as input.</li> <li>Outputs images that correspond to the input class labels.</li> </ul> </li> <li>Discriminator:<ul> <li>Classifies images as real or fake.</li> <li>Simultaneously predicts the class label of the image.</li> </ul> </li> </ul> </li> <li> <p>Training Process:</p> <ul> <li>The generator is trained to fool the discriminator into classifying fake images as real.</li> <li>The discriminator is trained to:<ul> <li>Differentiate real from fake images.</li> <li>Accurately predict the class labels of real images.</li> </ul> </li> </ul> </li> <li> <p>Loss Functions:</p> <ul> <li>Binary Cross-Entropy Loss for real/fake classification.</li> <li>Categorical Cross-Entropy Loss for class label prediction.</li> </ul> </li> </ol>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#implementation-code","title":"Implementation Code","text":""},{"location":"algorithms/generative-adversarial-networks/ac-gan/#core-components","title":"Core Components","text":""},{"location":"algorithms/generative-adversarial-networks/ac-gan/#discriminator","title":"Discriminator","text":"<pre><code>class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        self.model = nn.Sequential(\n            nn.Linear(image_size + num_classes, hidden_size),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, hidden_size),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x, labels):\n        x = x.view(x.size(0), image_size)\n        c = self.label_emb(labels)\n        x = torch.cat([x, c], 1)\n        return self.model(x)\n</code></pre>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#generator","title":"Generator","text":"<pre><code>class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        self.model = nn.Sequential(\n            nn.Linear(latent_size + num_classes, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        z = z.view(z.size(0), latent_size)\n        c = self.label_emb(labels)\n        x = torch.cat([z, c], 1)\n        return self.model(x)\n</code></pre>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#training-loop","title":"Training Loop","text":"<pre><code>for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        batch_size = images.size(0)\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Real and fake labels\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Train Discriminator\n        outputs = D(images, labels)\n        d_loss_real = criterion(outputs, real_labels)\n\n        z = create_noise(batch_size, latent_size)\n        fake_images = G(z, labels)\n        outputs = D(fake_images, labels)\n        d_loss_fake = criterion(outputs, fake_labels)\n\n        d_loss = d_loss_real + d_loss_fake\n        D.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Train Generator\n        z = create_noise(batch_size, latent_size)\n        fake_images = G(z, labels)\n        outputs = D(fake_images, labels)\n        g_loss = criterion(outputs, real_labels)\n\n        G.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        if (i+1) % 200 == 0:\n            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n</code></pre>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#applications-of-acgan","title":"Applications of ACGAN","text":"<ol> <li> <p>Image Synthesis:</p> <ul> <li>Generate diverse images conditioned on specific labels.</li> </ul> </li> <li> <p>Data Augmentation:</p> <ul> <li>Create synthetic data to augment existing datasets.</li> </ul> </li> <li> <p>Creative Domains:</p> <ul> <li>Design tools for controlled image generation in fashion, gaming, and media.</li> </ul> </li> </ol>"},{"location":"algorithms/generative-adversarial-networks/ac-gan/#additional-resources","title":"Additional Resources","text":"<ul> <li>PyTorch Documentation</li> <li>Original ACGAN Paper</li> <li>MNIST Dataset</li> </ul>"},{"location":"algorithms/generative-adversarial-networks/basic-gan/","title":"Basic GAN","text":"<pre><code>Basic GAN stands for Basic Generative Adversarial Network\n</code></pre> <p>This folder contains a basic implementation of a Generative Adversarial Network (GAN) using PyTorch. GANs are a type of neural network architecture that consists of two networks: a generator and a discriminator. The generator learns to create realistic data samples (e.g., images) from random noise, while the discriminator learns to distinguish between real and generated samples.</p>"},{"location":"algorithms/generative-adversarial-networks/basic-gan/#overview","title":"Overview","text":"<p>This project implements a simple GAN architecture to generate hand-written digits resembling those from the MNIST dataset. The generator network creates fake images, while the discriminator network tries to differentiate between real and generated images. The networks are trained simultaneously in a minimax game until the generator produces realistic images.</p>"},{"location":"algorithms/generative-adversarial-networks/basic-gan/#files","title":"Files","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nimage_size = 784  # 28x28\nhidden_size = 256\nlatent_size = 64\nnum_epochs = 200\nbatch_size = 100\nlearning_rate = 0.0002\n\n# MNIST dataset\ndataset = dsets.MNIST(root='../data/',\n                      train=True,\n                      transform=transforms.ToTensor(),\n                      download=True)\n\n# Data loader\ndata_loader = DataLoader(dataset=dataset,\n                         batch_size=batch_size, \n                         shuffle=True)\n\n# Discriminator\nD = nn.Sequential(\n    nn.Linear(image_size, hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size, hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size, 1),\n    nn.Sigmoid())\n\n# Generator\nG = nn.Sequential(\n    nn.Linear(latent_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, image_size),\n    nn.Tanh())\n\n# Device setting\nD = D.to(device)\nG = G.to(device)\n\n# Binary cross entropy loss and optimizer\ncriterion = nn.BCELoss()\nd_optimizer = optim.Adam(D.parameters(), lr=learning_rate)\ng_optimizer = optim.Adam(G.parameters(), lr=learning_rate)\n\n# Utility function to create real and fake labels\ndef create_real_labels(size):\n    data = torch.ones(size, 1)\n    return data.to(device)\n\ndef create_fake_labels(size):\n    data = torch.zeros(size, 1)\n    return data.to(device)\n\n# Utility function to generate random noise\ndef create_noise(size, latent_dim):\n    return torch.randn(size, latent_dim).to(device)\n\n# Training the GAN\ntotal_step = len(data_loader)\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(data_loader):\n        batch_size = images.size(0)\n        images = images.reshape(batch_size, -1).to(device)\n\n        # Create the labels which are later used as input for the BCE loss\n        real_labels = create_real_labels(batch_size)\n        fake_labels = create_fake_labels(batch_size)\n\n        # ================================================================== #\n        #                      Train the discriminator                       #\n        # ================================================================== #\n        # Compute BCELoss using real images\n        # Second term of the loss is always zero since real_labels == 1\n        outputs = D(images)\n        d_loss_real = criterion(outputs, real_labels)\n        real_score = outputs\n\n        # Compute BCELoss using fake images\n        noise = create_noise(batch_size, latent_size)\n        fake_images = G(noise)\n        outputs = D(fake_images)\n        d_loss_fake = criterion(outputs, fake_labels)\n        fake_score = outputs\n\n        # Backprop and optimize\n        d_loss = d_loss_real + d_loss_fake\n        d_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # ================================================================== #\n        #                        Train the generator                         #\n        # ================================================================== #\n        # Compute loss with fake images\n        noise = create_noise(batch_size, latent_size)\n        fake_images = G(noise)\n        outputs = D(fake_images)\n\n        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n        # For the reason, look at the last part of section 3 of the paper:\n        # https://arxiv.org/pdf/1406.2661.pdf\n        g_loss = criterion(outputs, real_labels)\n\n        # Backprop and optimize\n        g_optimizer.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        if (i+1) % 200 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')\n\n# Save the trained models\ntorch.save(G.state_dict(), 'G.pth')\ntorch.save(D.state_dict(), 'D.pth')\n\n# Plot some generated images\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\nG.eval()\nwith torch.no_grad():\n    noise = create_noise(64, latent_size)\n    fake_images = G(noise)\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    fake_images = denorm(fake_images)\n    grid = np.transpose(fake_images.cpu(), (0, 2, 3, 1)).numpy()\n\n    plt.figure(figsize=(8, 8))\n    for i in range(grid.shape[0]):\n        plt.subplot(8, 8, i+1)\n        plt.imshow(grid[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n    plt.show()\n</code></pre> <ul> <li><code>BasicGAN.py</code>: Contains the implementation of the GAN model, training loop, and saving of trained models.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nlatent_size = 64\nhidden_size = 256\nimage_size = 784  # 28x28\n\n# Generator\nG = nn.Sequential(\n    nn.Linear(latent_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, image_size),\n    nn.Tanh())\n\n# Load the trained generator model\nG.load_state_dict(torch.load('G.pth'))\nG.to(device)\nG.eval()\n\n# Utility function to generate random noise\ndef create_noise(size, latent_dim):\n    return torch.randn(size, latent_dim).to(device)\n\n# Utility function to denormalize the images\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Generate images\nwith torch.no_grad():\n    noise = create_noise(64, latent_size)\n    fake_images = G(noise)\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    fake_images = denorm(fake_images)\n    grid = np.transpose(fake_images.cpu(), (0, 2, 3, 1)).numpy()\n\n    plt.figure(figsize=(8, 8))\n    for i in range(grid.shape[0]):\n        plt.subplot(8, 8, i+1)\n        plt.imshow(grid[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n    plt.show()\n</code></pre> <ul> <li><code>test_BasicGAN.py</code>: Uses the trained generator to generate sample images after training.</li> </ul>"},{"location":"algorithms/generative-adversarial-networks/c-gan/","title":"C GAN","text":"<p>This folder contains an implementation of a Conditional Generative Adversarial Network (cGAN) using PyTorch. cGANs generate images conditioned on specific class labels, allowing for controlled image synthesis.</p>"},{"location":"algorithms/generative-adversarial-networks/c-gan/#overview","title":"Overview","text":"<p>cGANs extend the traditional GAN architecture by including class information in both the generator and discriminator. The generator learns to generate images conditioned on given class labels, while the discriminator not only distinguishes between real and fake images but also predicts the class labels of the generated images.</p>"},{"location":"algorithms/generative-adversarial-networks/c-gan/#files","title":"Files","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nimage_size = 28 * 28\nnum_classes = 10\nlatent_size = 100\nhidden_size = 256\nnum_epochs = 100\nbatch_size = 64\nlearning_rate = 0.0002\n\n# MNIST dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True,\n                            transform=transform,\n                            download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True)\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(image_size + num_classes, hidden_size),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, hidden_size),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_size, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x, labels):\n        x = x.view(x.size(0), image_size)\n        c = self.label_emb(labels)\n        x = torch.cat([x, c], 1)\n        out = self.model(x)\n        return out\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_size + num_classes, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        z = z.view(z.size(0), latent_size)\n        c = self.label_emb(labels)\n        x = torch.cat([z, c], 1)\n        out = self.model(x)\n        return out\n\n# Initialize models\nD = Discriminator().to(device)\nG = Generator().to(device)\n\n# Loss function and optimizer\ncriterion = nn.BCELoss()\nd_optimizer = optim.Adam(D.parameters(), lr=learning_rate)\ng_optimizer = optim.Adam(G.parameters(), lr=learning_rate)\n\n# Utility functions\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\ndef create_noise(batch_size, latent_size):\n    return torch.randn(batch_size, latent_size).to(device)\n\ndef create_labels(batch_size):\n    return torch.randint(0, num_classes, (batch_size,)).to(device)\n\n# Training the cGAN\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        batch_size = images.size(0)\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Create the labels which are later used as input for the discriminator\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # ================================================================== #\n        #                      Train the discriminator                       #\n        # ================================================================== #\n\n        # Compute BCELoss using real images\n        outputs = D(images, labels)\n        d_loss_real = criterion(outputs, real_labels)\n        real_score = outputs\n\n        # Compute BCELoss using fake images\n        z = create_noise(batch_size, latent_size)\n        fake_images = G(z, labels)\n        outputs = D(fake_images, labels)\n        d_loss_fake = criterion(outputs, fake_labels)\n        fake_score = outputs\n\n        # Backprop and optimize\n        d_loss = d_loss_real + d_loss_fake\n        D.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # ================================================================== #\n        #                        Train the generator                         #\n        # ================================================================== #\n\n        # Compute loss with fake images\n        z = create_noise(batch_size, latent_size)\n        fake_images = G(z, labels)\n        outputs = D(fake_images, labels)\n\n        # We train G to maximize log(D(G(z)))\n        g_loss = criterion(outputs, real_labels)\n\n        # Backprop and optimize\n        G.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        if (i+1) % 200 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')\n\n# Save the trained models\ntorch.save(G.state_dict(), 'G_cgan.pth')\ntorch.save(D.state_dict(), 'D_cgan.pth')\n</code></pre> <ul> <li><code>cGAN.py</code>: Contains the implementation of the ACGAN model, training loop, and saving of trained models.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nlatent_size = 100\nnum_classes = 10\nimage_size = 28 * 28\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_size + num_classes, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        z = z.view(z.size(0), latent_size)\n        c = self.label_emb(labels)\n        x = torch.cat([z, c], 1)\n        out = self.model(x)\n        return out\n\n# Load the trained generator model\nG = Generator()\nG.load_state_dict(torch.load('G_cgan.pth', map_location=torch.device('cpu')))\nG.eval()\n\n# Utility function to generate random noise\ndef create_noise(size, latent_dim):\n    return torch.randn(size, latent_dim)\n\n# Utility function to generate labels\ndef create_labels(size):\n    return torch.randint(0, num_classes, (size,))\n\n# Utility function to denormalize the images\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Generate images\nwith torch.no_grad():\n    noise = create_noise(64, latent_size)\n    labels = create_labels(64)\n    fake_images = G(noise, labels)\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    fake_images = denorm(fake_images)\n    grid = np.transpose(fake_images, (0, 2, 3, 1)).numpy()\n\n    plt.figure(figsize=(8, 8))\n    for i in range(grid.shape[0]):\n        plt.subplot(8, 8, i+1)\n        plt.imshow(grid[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n    plt.show()\n</code></pre> <ul> <li><code>test_cGAN.py</code>: Uses the trained generator to generate sample images after training.</li> </ul>"},{"location":"algorithms/generative-adversarial-networks/eb-gan/","title":"EB GAN","text":"<p>This folder contains an implementation of an Energy-Based Generative Adversarial Network (EBGAN) using PyTorch. EBGAN focuses on matching the energy distribution of generated samples to that of real data, optimizing both a discriminator and a generator network.</p>"},{"location":"algorithms/generative-adversarial-networks/eb-gan/#overview","title":"Overview","text":"<p>EBGAN introduces an energy function that is used to measure the quality of generated samples. The discriminator (autoencoder-like) network tries to minimize this energy function while the generator tries to maximize it. This results in a more stable training process compared to traditional GANs.</p>"},{"location":"algorithms/generative-adversarial-networks/eb-gan/#files","title":"Files","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nimage_size = 28 * 28\nlatent_size = 64\nhidden_size = 256\nnum_epochs = 100\nbatch_size = 64\nlearning_rate = 0.0002\nk = 3  # Number of iterations for optimizing D\n\n# MNIST dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True,\n                            transform=transform,\n                            download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True)\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(image_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, latent_size)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded, encoded\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(latent_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        out = self.model(z)\n        return out\n\n# Initialize models\nD = Discriminator().to(device)\nG = Generator().to(device)\n\n# Loss function and optimizer\ncriterion_rec = nn.MSELoss()\nd_optimizer = optim.Adam(D.parameters(), lr=learning_rate)\ng_optimizer = optim.Adam(G.parameters(), lr=learning_rate)\n\n# Utility functions\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Training the EBGAN\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(train_loader):\n        batch_size = images.size(0)\n        images = images.view(-1, image_size).to(device)\n\n        # ================================================================== #\n        #                      Train the discriminator                       #\n        # ================================================================== #\n\n        encoded_real, _ = D(images)\n        decoded_real = D.decoder(encoded_real)\n\n        rec_loss_real = criterion_rec(decoded_real, images)\n\n        z = torch.randn(batch_size, latent_size).to(device)\n        fake_images = G(z)\n        encoded_fake, _ = D(fake_images.detach())\n        decoded_fake = D.decoder(encoded_fake)\n\n        rec_loss_fake = criterion_rec(decoded_fake, fake_images.detach())\n\n        d_loss = rec_loss_real + torch.max(torch.zeros(1).to(device), k * rec_loss_real - rec_loss_fake)\n\n        D.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n\n        # ================================================================== #\n        #                        Train the generator                         #\n        # ================================================================== #\n\n        z = torch.randn(batch_size, latent_size).to(device)\n        fake_images = G(z)\n        encoded_fake, _ = D(fake_images)\n        decoded_fake = D.decoder(encoded_fake)\n\n        rec_loss_fake = criterion_rec(decoded_fake, fake_images)\n\n        g_loss = rec_loss_fake\n\n        G.zero_grad()\n        g_loss.backward()\n        g_optimizer.step()\n\n        if (i+1) % 200 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, Rec_loss_real: {rec_loss_real.item():.4f}, Rec_loss_fake: {rec_loss_fake.item():.4f}')\n\n# Save the trained models\ntorch.save(G.state_dict(), 'G_ebgan.pth')\ntorch.save(D.state_dict(), 'D_ebgan.pth')\n</code></pre> <ul> <li><code>EBGAN.py</code>: Contains the implementation of the ACGAN model, training loop, and saving of trained models.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nlatent_size = 64\nimage_size = 28 * 28\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(latent_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        out = self.model(z)\n        return out\n\n# Load the trained generator model\nG = Generator()\nG.load_state_dict(torch.load('G_ebgan.pth', map_location=torch.device('cpu')))\nG.eval()\n\n# Utility function to generate random noise\ndef create_noise(size, latent_dim):\n    return torch.randn(size, latent_dim)\n\n# Utility function to denormalize the images\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Generate images\nwith torch.no_grad():\n    noise = create_noise(64, latent_size)\n    fake_images = G(noise)\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    fake_images = denorm(fake_images)\n    grid = np.transpose(fake_images, (0, 2, 3, 1)).numpy()\n\n    plt.figure(figsize=(8, 8))\n    for i in range(grid.shape[0]):\n        plt.subplot(8, 8, i+1)\n        plt.imshow(grid[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n    plt.show()\n</code></pre> <ul> <li><code>test_EBGAN.py</code>: Uses the trained generator to generate sample images after training.</li> </ul>"},{"location":"algorithms/generative-adversarial-networks/info-gan/","title":"Info GAN","text":"<pre><code>Information Maximizing Generative Adversarial Network\n</code></pre> <p>This folder contains an implementation of InfoGAN using PyTorch. InfoGAN extends the traditional GAN framework by incorporating unsupervised learning of interpretable and disentangled representations.</p>"},{"location":"algorithms/generative-adversarial-networks/info-gan/#overview","title":"Overview","text":"<p>InfoGAN introduces latent codes that can be split into categorical and continuous variables, allowing for more control over the generated outputs. The generator is conditioned on these latent codes, which are learned in an unsupervised manner during training.</p>"},{"location":"algorithms/generative-adversarial-networks/info-gan/#files","title":"Files","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nimage_size = 28 * 28\nnum_epochs = 50\nbatch_size = 100\nlatent_size = 62\nnum_continuous = 2\nnum_categories = 10\nlearning_rate = 0.0002\n\n# MNIST dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\ntrain_dataset = dsets.MNIST(root='../data/',\n                            train=True,\n                            transform=transform,\n                            download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          shuffle=True)\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(latent_size + num_categories + num_continuous, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z, c_cat, c_cont):\n        inputs = torch.cat([z, c_cat, c_cont], dim=1)\n        return self.fc(inputs)\n\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(image_size, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n        )\n\n        self.fc_disc = nn.Linear(512, num_categories)\n        self.fc_mu = nn.Linear(512, num_continuous)\n        self.fc_var = nn.Linear(512, num_continuous)\n\n    def forward(self, x):\n        x = self.fc(x)\n        disc_logits = self.fc_disc(x)\n        mu = self.fc_mu(x)\n        var = torch.exp(self.fc_var(x))\n        return disc_logits, mu, var\n\n\n# Initialize networks\nG = Generator().to(device)\nD = Discriminator().to(device)\n\n# Loss functions\ncriterion_cat = nn.CrossEntropyLoss()\ncriterion_cont = nn.MSELoss()\n\n# Optimizers\ng_optimizer = optim.Adam(G.parameters(), lr=learning_rate)\nd_optimizer = optim.Adam(D.parameters(), lr=learning_rate)\n\n# Utility functions\ndef sample_noise(batch_size, latent_size):\n    return torch.randn(batch_size, latent_size).to(device)\n\ndef sample_categorical(batch_size, num_categories):\n    return torch.randint(0, num_categories, (batch_size,)).to(device)\n\ndef sample_continuous(batch_size, num_continuous):\n    return torch.rand(batch_size, num_continuous).to(device)\n\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Training InfoGAN\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        batch_size = images.size(0)\n        images = images.view(-1, image_size).to(device)\n\n        # Create labels for discriminator\n        real_labels = torch.ones(batch_size, dtype=torch.long, device=device)\n        fake_labels = torch.zeros(batch_size, dtype=torch.long, device=device)\n\n        # Sample noise, categorical, and continuous latent codes\n        z = sample_noise(batch_size, latent_size)\n        c_cat = sample_categorical(batch_size, num_categories)\n        c_cont = sample_continuous(batch_size, num_continuous)\n\n        # Generate fake images\n        fake_images = G(z, c_cat, c_cont)\n\n        # Train discriminator\n        d_optimizer.zero_grad()\n        d_real_cat, d_real_mu, d_real_var = D(images)\n        d_real_loss_cat = criterion_cat(d_real_cat, labels)\n        d_fake_cat, d_fake_mu, d_fake_var = D(fake_images.detach())\n        d_fake_loss_cat = criterion_cat(d_fake_cat, c_cat)\n\n        d_loss_cat = d_real_loss_cat + d_fake_loss_cat\n\n        d_real_loss_cont = torch.mean(0.5 * torch.sum(torch.div((d_real_mu - c_cont)**2, d_real_var), dim=1))\n        d_fake_loss_cont = torch.mean(0.5 * torch.sum(torch.div((d_fake_mu - c_cont)**2, d_fake_var), dim=1))\n\n        d_loss_cont = d_real_loss_cont + d_fake_loss_cont\n\n        d_loss = d_loss_cat + d_loss_cont\n        d_loss.backward()\n        d_optimizer.step()\n\n        # Train generator\n        g_optimizer.zero_grad()\n        _, d_fake_mu, d_fake_var = D(fake_images)\n\n        g_loss_cat = criterion_cat(_, c_cat)\n        g_loss_cont = torch.mean(0.5 * torch.sum(torch.div((d_fake_mu - c_cont)**2, d_fake_var), dim=1))\n\n        g_loss = g_loss_cat + g_loss_cont\n        g_loss.backward()\n        g_optimizer.step()\n\n        if (i+1) % 200 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_step}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n\n# Save the trained models\ntorch.save(G.state_dict(), 'G_infogan.pth')\ntorch.save(D.state_dict(), 'D_infogan.pth')\n</code></pre> <ul> <li><code>InfoGAN.py</code>: Contains the implementation of the ACGAN model, training loop, and saving of trained models.</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nlatent_size = 62\nnum_categories = 10\nnum_continuous = 2\nimage_size = 28 * 28\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(latent_size + num_categories + num_continuous, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, image_size),\n            nn.Tanh()\n        )\n\n    def forward(self, z, c_cat, c_cont):\n        inputs = torch.cat([z, c_cat, c_cont], dim=1)\n        return self.fc(inputs)\n\n# Load the trained generator model\nG = Generator().to(device)\nG.load_state_dict(torch.load('G_infogan.pth', map_location=torch.device('cpu')))\nG.eval()\n\n# Utility functions to generate samples\ndef sample_noise(batch_size, latent_size):\n    return torch.randn(batch_size, latent_size).to(device)\n\ndef sample_categorical(batch_size, num_categories):\n    return torch.randint(0, num_categories, (batch_size,)).to(device)\n\ndef sample_continuous(batch_size, num_continuous):\n    return torch.rand(batch_size, num_continuous).to(device)\n\ndef denorm(x):\n    out = (x + 1) / 2\n    return out.clamp(0, 1)\n\n# Generate images\nwith torch.no_grad():\n    noise = sample_noise(64, latent_size)\n    c_cat = sample_categorical(64, num_categories)\n    c_cont = sample_continuous(64, num_continuous)\n    fake_images = G(noise, c_cat, c_cont)\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    fake_images = denorm(fake_images)\n    grid = np.transpose(fake_images, (0, 2, 3, 1)).numpy()\n\n    plt.figure(figsize=(8, 8))\n    for i in range(grid.shape[0]):\n        plt.subplot(8, 8, i+1)\n        plt.imshow(grid[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n    plt.show()\n</code></pre> <ul> <li><code>test_InfoGAN.py</code>: Uses the trained generator to generate sample images after training.</li> </ul>"},{"location":"algorithms/large-language-models/","title":"Large Language Models \ud83e\udde0","text":"GPT-Series <p>Generative Pretrained Transformers for advanced natural language processing tasks.</p> BERT <p>Bidirectional Encoder Representations for language understanding and NLP tasks.</p> T5 <p>Text-to-Text Transfer Transformer for a unified approach to various NLP tasks.</p> Bloom <p>A large multilingual model designed for natural language understanding and generation.</p>"},{"location":"algorithms/large-language-models/bert/","title":"BERT \ud83e\udde0","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/large-language-models/bloom/","title":"Bloom \ud83e\udde0","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/large-language-models/gpt-series/","title":"GPT Series \ud83e\udde0","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/large-language-models/t5/","title":"T5 \ud83e\udde0","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/machine-learning/","title":"Machine Learning \ud83e\udd16","text":"Data Preprocessing <p> Technique that is used to convert the raw data into a clean data set.</p> Supervised <p>Uses labeled datasets to train algorithms to predict outcomes and recognize patterns.</p> Unsupervised <p>Uses algorithms to analyze unlabeled data without human intervention</p> Boosting <p>Modeling technique that attempts to build a strong classifier from the number of weak classifiers.</p>"},{"location":"algorithms/machine-learning/boosting/","title":"Boosting \ud83e\udd16","text":"Light Gradient Boosting Machine <p>Powerful gradient-boosting framework that can be used for both regression and classification tasks.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 4 mins</p>"},{"location":"algorithms/machine-learning/boosting/light-gbm/","title":"Light gbm","text":""},{"location":"algorithms/machine-learning/boosting/light-gbm/#lightgbm-a-comprehensive-guide-to-scratch-implementation","title":"LightGBM: A Comprehensive Guide to Scratch Implementation","text":"<p>Overview: LightGBM (Light Gradient Boosting Machine) is an advanced gradient boosting framework that efficiently handles large datasets. Unlike traditional boosting methods, LightGBM uses leaf-wise tree growth, which improves accuracy and reduces computation time.  </p>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#key-highlights","title":"Key Highlights:","text":"<ul> <li>Speed and Efficiency: Faster training on large datasets compared to XGBoost.  </li> <li>Memory Optimization: Lower memory usage, making it scalable.  </li> <li>Built-in Handling of Categorical Data: No need for manual one-hot encoding.  </li> <li>Parallel and GPU Training: Supports multi-threading and GPU acceleration for faster computation.  </li> </ul>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#how-lightgbm-works-scratch-implementation-guide","title":"How LightGBM Works (Scratch Implementation Guide):","text":""},{"location":"algorithms/machine-learning/boosting/light-gbm/#1-core-concept-leaf-wise-tree-growth","title":"1. Core Concept (Leaf-Wise Tree Growth):","text":"<ul> <li>Level-wise (XGBoost): Grows all leaves at the same depth before moving to the next.  </li> <li>Leaf-wise (LightGBM): Grows the leaf that reduces the most loss, potentially leading to deeper, more accurate trees.  </li> </ul> <p>Example Visualization: <pre><code>Level-wise (XGBoost)                Leaf-wise (LightGBM)\n        O                                 O\n       / \\                               / \\\n      O   O                             O   O\n     / \\                                 \\\n    O   O                                 O\n</code></pre></p>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#algorithm-breakdown","title":"Algorithm Breakdown:","text":"<ol> <li>Initialize Model: Start with a simple model (like mean predictions).  </li> <li>Compute Residuals: Calculate errors between actual and predicted values.  </li> <li>Train Trees to Predict Residuals: Fit new trees to minimize residuals.  </li> <li>Update Model: Adjust predictions by adding the new tree\u2019s results.  </li> <li>Repeat Until Convergence or Early Stopping. </li> </ol>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#parameters-explained","title":"Parameters Explained:","text":"<ul> <li>num_leaves: Limits the number of leaves in a tree (complexity control).  </li> <li>max_depth: Constrains tree depth to prevent overfitting.  </li> <li>learning_rate: Scales the contribution of each tree to control convergence.  </li> <li>n_estimators: Number of boosting rounds (trees).  </li> <li>min_data_in_leaf: Minimum number of data points in a leaf to avoid overfitting small branches.  </li> </ul>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#scratch-code-example-from-the-ground-up","title":"Scratch Code Example (From the Ground Up):","text":"<p>File: <code>lightgbm_model.py</code> <pre><code>import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nclass LightGBMModel:\n    def __init__(self, params=None):\n        self.params = params if params else {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'n_estimators': 100\n        }\n        self.model = None\n\n    def fit(self, X_train, y_train):\n        d_train = lgb.Dataset(X_train, label=y_train)\n        self.model = lgb.train(self.params, d_train)\n\n    def predict(self, X_test):\n        return self.model.predict(X_test)\n</code></pre></p>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#testing-the-model","title":"Testing the Model:","text":"<p>File: <code>lightgbm_model_test.py</code> <pre><code>import unittest\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm_model import LightGBMModel\n\nclass TestLightGBMModel(unittest.TestCase):\n\n    def test_lightgbm(self):\n        # Load Dataset\n        data = load_diabetes()\n        X_train, X_test, y_train, y_test = train_test_split(\n            data.data, data.target, test_size=0.2, random_state=42)\n\n        # Train Model\n        model = LightGBMModel()\n        model.fit(X_train, y_train)\n\n        # Predict and Evaluate\n        predictions = model.predict(X_test)\n        mse = mean_squared_error(y_test, predictions)\n        self.assertTrue(mse &lt; 3500, \"MSE is too high, LightGBM not performing well\")\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre></p>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#additional-insights-to-aid-understanding","title":"Additional Insights to Aid Understanding:","text":"<ul> <li>Feature Importance: <pre><code>lgb.plot_importance(model.model)\n</code></pre></li> <li>Early Stopping Implementation: <pre><code>self.model = lgb.train(self.params, d_train, valid_sets=[d_train], early_stopping_rounds=10)\n</code></pre></li> </ul>"},{"location":"algorithms/machine-learning/boosting/light-gbm/#testing-and-validation","title":"Testing and Validation:","text":"<p>Use <code>sklearn</code> datasets to validate the implementation. Compare performance with other boosting models to highlight LightGBM\u2019s efficiency.  </p>"},{"location":"algorithms/machine-learning/data-preprocessing/","title":"Data Pre-processing \ud83e\udd16","text":"Encoding <p>Process of converting categorical data into numerical values</p> Imputation <p>Technique that replaces missing values in a dataset with estimated values.</p> Scaling and Normalization  <p>Techniques that transform numerical data values into a common scale.</p>"},{"location":"algorithms/machine-learning/data-preprocessing/encoding/","title":"Encoding Algorithms \ud83e\udd16","text":"Ordinal Encoder <p>Transforming categorical data into meaningful numerical order.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 3 mins</p>"},{"location":"algorithms/machine-learning/data-preprocessing/encoding/ordinal-encoder/","title":"ORDINAL ENCODER","text":"<p>A custom implementation of an OrdinalEncoder class for encoding categorical data into ordinal integers using a pandas DataFrame. The class maps each unique category to an integer based on the order of appearance.</p>"},{"location":"algorithms/machine-learning/data-preprocessing/encoding/ordinal-encoder/#features","title":"Features","text":"<ul> <li>fit: Learn the mapping of categories to ordinal integers for each column.</li> <li>transform: Transform the categorical data to ordinal integers based on the learned mapping.</li> <li>fit_transform: Fit the encoder and transform the data in one step.</li> </ul>"},{"location":"algorithms/machine-learning/data-preprocessing/encoding/ordinal-encoder/#methods","title":"Methods","text":"<ol> <li><code>__init__(self)</code><ul> <li>Initializes the OrdinalEncoding class.</li> <li>No parameters are required.</li> </ul> </li> <li><code>fit(self, data)</code><ul> <li>Learns the mapping of categories to ordinal integers for each column.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame): The data to fit.</li> </ul> </li> <li>Raises:<ul> <li>TypeError: If the input data is not a pandas DataFrame.</li> </ul> </li> </ul> </li> <li><code>transform(self, data)</code><ul> <li>Transforms the categorical data to ordinal integers based on the learned mapping.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame): The data to transform.</li> </ul> </li> <li>Returns:<ul> <li>pandas.DataFrame: The transformed data.</li> </ul> </li> <li>Raises:<ul> <li>Error: If transform is called before fit or fit_transform.</li> </ul> </li> </ul> </li> <li><code>fit_transform(self, data)</code><ul> <li>Fits the encoder to the data and transforms the data in one step.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame): The data to fit and transform.</li> </ul> </li> <li>Returns:<ul> <li>pandas.DataFrame: The transformed data.</li> </ul> </li> </ul> </li> </ol>"},{"location":"algorithms/machine-learning/data-preprocessing/encoding/ordinal-encoder/#error-handling","title":"Error Handling","text":"<ul> <li>Raises a TypeError if the input data is not a pandas DataFrame in the fit method.</li> <li>Raises an error if transform is called before fit or fit_transform.</li> </ul>"},{"location":"algorithms/machine-learning/data-preprocessing/encoding/ordinal-encoder/#use-case","title":"Use Case","text":""},{"location":"algorithms/machine-learning/data-preprocessing/encoding/ordinal-encoder/#output","title":"Output","text":"<ul> <li>ordinal_encoder.py file </li> </ul> <pre><code>import pandas as pd\n\nclass OrdinalEncoding:\n    def __init__(self):\n        self.category_mapping = {}\n\n    def fit(self, data):\n        # Fit the encoder to the data (pandas DataFrame).\n        # type check\n        if not type(data)==pd.DataFrame:\n            raise f\"Type of data should be Pandas.DataFrame; {type(data)} found\"\n        for column in data.columns:\n            unique_categories = sorted(set(data[column]))\n            self.category_mapping[column] = {category: idx for idx, category in enumerate(unique_categories)}\n\n    def transform(self, data):\n        # Transform the data (pandas DataFrame) to ordinal integers.\n        # checking for empty mapping\n        if not self.category_mapping:\n            raise \"Catrgorical Mapping not found. Call OrdinalExcoding.fit() method or call OrdinalEncoding.fit_transform() method\"\n\n        data_transformed = data.copy()\n        for column in data.columns:\n            data_transformed[column] = data[column].map(self.category_mapping[column])\n        return data_transformed\n\n    def fit_transform(self, data):\n        # Fit the encoder and transform the data in one step.\n        self.fit(data)\n        return self.transform(data)\n</code></pre> <ul> <li>test_ordinal_encoder.py file </li> </ul> <pre><code>import os\nimport sys\n# for resolving any path conflict\ncurrent = os.path.dirname(os.path.realpath(\"ordinal_encoder.py\"))\nparent = os.path.dirname(current)\nsys.path.append(current)\n\nimport pandas as pd\n\nfrom Ordinal_Encoder.ordinal_encoder import OrdinalEncoding\n\n# Example usage\ndata = {\n    'Category1': ['low', 'medium', 'high', 'medium', 'low', 'high', 'medium'],\n    'Category2': ['A', 'B', 'A', 'B', 'A', 'B', 'A'],\n    'Category3': ['X', 'Y', 'X', 'Y', 'X', 'Y', 'X']\n}\ndf = pd.DataFrame(data)\n\nencoder = OrdinalEncoding()\nencoded_df = encoder.fit_transform(df)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nEncoded DataFrame:\")\nprint(encoded_df)\n</code></pre>"},{"location":"algorithms/machine-learning/data-preprocessing/imputation/","title":"Imputation Algorithm \ud83e\udd16","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/","title":"Scaling and Normalization \ud83e\udd16","text":"Min Max Scaler <p>Scaling features to a standardized range for improved model performance.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 3 mins</p> Standard Scaler <p>Centering and scaling features for a balanced dataset.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 3 mins</p>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/min-max-scaler/","title":"MIN MAX SCALER","text":"<p>A custom implementation of a MinMaxScaler class for scaling numerical data in a pandas DataFrame. The class scales the features to a specified range, typically between 0 and 1.</p>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/min-max-scaler/#features","title":"Features","text":"<ul> <li>fit: Calculate the minimum and maximum values of the data.</li> <li>transform: Scale the data to the specified feature range.</li> <li>fit_transform: Fit the scaler and transform the data in one step.</li> <li>get_params: Retrieve the minimum and maximum values calculated during fitting.</li> </ul>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/min-max-scaler/#methods","title":"Methods","text":"<ol> <li><code>__init__(self, feature_range=(0, 1))</code><ul> <li>Initializes the MinMaxScaling class.</li> <li>Parameters:<ul> <li>feature_range (tuple): Desired range of transformed data. Default is (0, 1).</li> </ul> </li> </ul> </li> <li><code>fit(self, data)</code><ul> <li>Calculates the minimum and maximum values of the data.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame): The data to fit.</li> </ul> </li> </ul> </li> <li><code>transform(self, data)</code><ul> <li>Transforms the data to the specified feature range.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame): The data to transform.</li> </ul> </li> <li>Returns:<ul> <li>pandas.DataFrame: The scaled data.</li> </ul> </li> </ul> </li> <li><code>fit_transform(self, data)</code><ul> <li>Fits the scaler to the data and transforms the data in one step.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame): The data to fit and transform.</li> </ul> </li> <li>Returns:<ul> <li>pandas.DataFrame: The scaled data.</li> </ul> </li> </ul> </li> <li><code>get_params(self)</code><ul> <li>Retrieves the minimum and maximum values calculated during fitting.</li> <li>Returns:<ul> <li>dict: Dictionary containing the minimum and maximum values.</li> </ul> </li> </ul> </li> </ol>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/min-max-scaler/#error-handling","title":"Error Handling","text":"<ul> <li>Raises a TypeError if the input data is not a pandas DataFrame in the fit method.</li> <li>Raises an error if transform is called before fit or fit_transform.</li> <li>Raises an error in get_params if called before fit.</li> </ul>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/min-max-scaler/#use-case","title":"Use Case","text":""},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/min-max-scaler/#output","title":"Output","text":"<ul> <li>min_max_scaler.py file </li> </ul> <pre><code>import pandas as pd\n\n# Custom MinMaxScaler class\nclass MinMaxScaling:\n    # init function\n    def __init__(self, feature_range=(0, 1)):  # feature range can be specified by the user else it takes (0,1)\n        self.min = feature_range[0]\n        self.max = feature_range[1]\n        self.data_min_ = None\n        self.data_max_ = None\n\n    # fit function to calculate min and max value of the data\n    def fit(self, data):\n        # type check\n        if not type(data)==pd.DataFrame:\n            raise f\"TypeError : parameter should be a Pandas.DataFrame; {type(data)} found\"\n        else:\n            self.data_min_ = data.min()\n            self.data_max_ = data.max()\n\n    # transform function\n    def transform(self, data):\n        if self.data_max_ is None or self.data_min_ is None:\n            raise \"Call MinMaxScaling.fit() first or call MinMaxScaling.fit_transform() as the required params not found\"\n        else:\n            data_scaled = (data - self.data_min_) / (self.data_max_ - self.data_min_)\n            data_scaled = data_scaled * (self.max - self.min) + self.min\n            return data_scaled\n\n    # fit_tranform function\n    def fit_transform(self, data):\n        self.fit(data)\n        return self.transform(data)\n\n    # get_params function\n    def get_params(self):\n        if self.data_max_ is None or self.data_min_ is None:\n            raise \"Params not found! Call MinMaxScaling.fit() first\"\n        else:\n            return {\"Min\" : self.data_min_,\n                    \"Max\" : self.data_max_}\n</code></pre> <ul> <li>test_min_max_scaler.py file </li> </ul> <pre><code>import os\nimport sys\n# for resolving any path conflict\ncurrent = os.path.dirname(os.path.realpath(\"min_max_scaler.py\"))\nparent = os.path.dirname(current)\nsys.path.append(current)\n\nimport pandas as pd\n\nfrom Min_Max_Scaler.min_max_scaler import MinMaxScaling\n\n# Example DataFrame\ndata = {\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n}\n\ndf = pd.DataFrame(data)\n\n# Initialize the CustomMinMaxScaler\nscaler = MinMaxScaling()\n\n# Fit the scaler to the data and transform the data\nscaled_df = scaler.fit_transform(df)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nScaled DataFrame:\")\nprint(scaled_df)\n</code></pre>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/standard-scaler/","title":"STANDARD SCALER","text":"<p>A custom implementation of a StandardScaler class for scaling numerical data in a pandas DataFrame or NumPy array. The class scales the features to have zero mean and unit variance.</p>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/standard-scaler/#features","title":"Features","text":"<ul> <li>fit: Calculate the mean and standard deviation of the data.</li> <li>transform: Scale the data to have zero mean and unit variance.</li> <li>fit_transform: Fit the scaler and transform the data in one step.</li> <li>get_params: Retrieve the mean and standard deviation calculated during fitting.</li> </ul>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/standard-scaler/#methods","title":"Methods","text":"<ol> <li><code>__init__(self)</code><ul> <li>Initializes the StandardScaling class.</li> <li>No parameters are required.</li> </ul> </li> <li><code>fit(self, data)</code><ul> <li>Calculates the mean and standard deviation of the data.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame or numpy.ndarray): The data to fit.</li> </ul> </li> <li>Raises:<ul> <li>TypeError: If the input data is not a pandas DataFrame or NumPy array.</li> </ul> </li> </ul> </li> <li><code>transform(self, data)</code><ul> <li>Transforms the data to have zero mean and unit variance.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame or numpy.ndarray): The data to transform.</li> </ul> </li> <li>Returns:<ul> <li>numpy.ndarray: The scaled data.</li> </ul> </li> <li>Raises:<ul> <li>Error: If transform is called before fit or fit_transform.</li> </ul> </li> </ul> </li> <li><code>fit_transform(self, data)</code><ul> <li>Fits the scaler to the data and transforms the data in one step.</li> <li>Parameters:<ul> <li>data (pandas.DataFrame or numpy.ndarray): The data to fit and transform.</li> </ul> </li> <li>Returns:<ul> <li>numpy.ndarray: The scaled data.</li> </ul> </li> </ul> </li> <li><code>get_params(self)</code><ul> <li>Retrieves the mean and standard deviation calculated during fitting.</li> <li>Returns:<ul> <li>dict: Dictionary containing the mean and standard deviation.</li> </ul> </li> <li>Raises:<ul> <li>Error: If get_params is called before fit.</li> </ul> </li> </ul> </li> </ol>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/standard-scaler/#error-handling","title":"Error Handling","text":"<ul> <li>Raises a TypeError if the input data is not a pandas DataFrame or NumPy array in the fit method.</li> <li>Raises an error if transform is called before fit or fit_transform.</li> <li>Raises an error in get_params if called before fit.</li> </ul>"},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/standard-scaler/#use-case","title":"Use Case","text":""},{"location":"algorithms/machine-learning/data-preprocessing/scaling-and-normalization/standard-scaler/#output","title":"Output","text":"<ul> <li>standard_scaler.py file </li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\n# Custom MinMaxScaler class\nclass StandardScaling:\n    # init function\n    def __init__(self):     \n        self.data_mean_ = None\n        self.data_std_ = None\n\n    # fit function to calculate min and max value of the data\n    def fit(self, data):\n        # type check\n        if not (type(data)==pd.DataFrame or type(data)==np.ndarray):\n            raise f\"TypeError : parameter should be a Pandas.DataFrame or Numpy.ndarray; {type(data)} found\"\n        elif type(data)==pd.DataFrame:\n            data = data.to_numpy()\n\n        self.data_mean_ = np.mean(data, axis=0)\n        self.data_std_ = np.sqrt(np.var(data, axis=0))\n\n    # transform function\n    def transform(self, data):\n        if self.data_mean_ is None or self.data_std_ is None:\n            raise \"Call StandardScaling.fit() first or call StandardScaling.fit_transform() as the required params not found\"\n        else:\n            data_scaled = (data - self.data_mean_) / (self.data_std_)\n            return data_scaled\n\n    # fit_tranform function\n    def fit_transform(self, data):\n        self.fit(data)\n        return self.transform(data)\n\n    # get_params function\n    def get_params(self):\n        if self.data_mean_ is None or self.data_std_ is None:\n            raise \"Params not found! Call StandardScaling.fit() first\"\n        else:\n            return {\"Mean\" : self.data_mean_,\n                    \"Standard Deviation\" : self.data_std_}\n</code></pre> <ul> <li>test_standard_scaler.py file </li> </ul> <pre><code>import os\nimport sys\n# for resolving any path conflict\ncurrent = os.path.dirname(os.path.realpath(\"standard_scaler.py\"))\nparent = os.path.dirname(current)\nsys.path.append(current)\n\nimport pandas as pd\n\nfrom Standard_Scaler.standard_scaler import StandardScaling\n\n# Example DataFrame\ndata = {\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500]\n}\n\ndf = pd.DataFrame(data)\n\n# Initialize the CustomMinMaxScaler\nscaler = StandardScaling()\n\n# Fit the scaler to the data and transform the data\nscaled_df = scaler.fit_transform(df)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nScaled DataFrame:\")\nprint(scaled_df)\nprint(\"\\nAssociated Parameters:\")\nprint(scaler.get_params())\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/","title":"Supervised Machine Learning \ud83e\udd16","text":"Regression <p>Predicting continuous outcomes with precision.</p> Classification <p>Categorizing data for informed decisions.</p>"},{"location":"algorithms/machine-learning/supervised/classifications/","title":"Classification Algorithms \ud83e\udd16","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/machine-learning/supervised/regressions/","title":"Regression Algorithms \ud83e\udd16","text":"Linear Regression <p>Understanding the relationship between two variables.</p> <p>\ud83d\udcc5 2025-01-19 | \u23f1\ufe0f 2 mins</p> Bayesian Regression <p>Infusing uncertainty with predictions for smarter decision-making.</p> <p>\ud83d\udcc5 2025-01-19 | \u23f1\ufe0f 3 mins</p>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/","title":"AdaBoost","text":"<p>Overview: AdaBoost (Adaptive Boosting) is one of the most popular ensemble methods for boosting weak learners to create a strong learner. It works by combining multiple \"weak\" models, typically decision stumps, and focusing more on the errors from previous models. This iterative process improves accuracy and reduces bias.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#key-highlights","title":"Key Highlights:","text":"<ul> <li>Boosting Concept: Builds an ensemble by sequentially focusing on harder-to-classify instances.  </li> <li>Adaptive Weighting: Misclassified instances get higher weights, and correctly classified instances get lower weights in subsequent rounds.  </li> <li>Simple and Effective: Often uses decision stumps (single-level decision trees) as base models.  </li> <li>Versatility: Applicable to both regression and classification problems.  </li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#how-adaboost-works-scratch-implementation-guide","title":"How AdaBoost Works (Scratch Implementation Guide):","text":""},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#1-core-concept-error-weight-adjustment","title":"1. Core Concept (Error Weight Adjustment):","text":"<ul> <li>Assigns equal weights to all data points initially.  </li> <li>In each iteration:</li> <li>A weak model (e.g., a decision stump) is trained on the weighted dataset.  </li> <li>Misclassified points are assigned higher weights for the next iteration.  </li> <li>A final strong model is constructed by combining all weak models, weighted by their accuracy.  </li> </ul> <p>Visualization: <pre><code>Iteration 1: Train weak model -&gt; Update weights  \nIteration 2: Train weak model -&gt; Update weights  \n...  \nFinal Model: Combine weak models with weighted contributions  \n</code></pre></p>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#algorithm-breakdown","title":"Algorithm Breakdown:","text":"<ol> <li>Initialize Weights: Assign equal weights to all instances.  </li> <li>Train a Weak Model: Use weighted data to train a weak learner.  </li> <li>Calculate Model Error: Compute the weighted error rate of the model.  </li> <li>Update Instance Weights: Increase weights for misclassified points and decrease weights for correctly classified points.  </li> <li>Update Model Weight: Calculate the model\u2019s contribution based on its accuracy.  </li> <li>Repeat for a Set Number of Iterations or Until Convergence. </li> </ol>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#parameters-explained","title":"Parameters Explained:","text":"<ul> <li>n_estimators: Number of weak learners (iterations).  </li> <li>learning_rate: Shrinks the contribution of each weak learner to avoid overfitting.  </li> <li>base_estimator: The weak learner used (e.g., <code>DecisionTreeRegressor</code> or <code>DecisionTreeClassifier</code>).  </li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#scratch-code-example-from-the-ground-up","title":"Scratch Code Example (From the Ground Up):","text":"<p>File: <code>adaboost_model.py</code> <pre><code>import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nclass AdaBoostRegressor:\n    def __init__(self, n_estimators=50, learning_rate=1.0):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.models = []\n        self.model_weights = []\n\n    def fit(self, X, y):\n        n_samples = X.shape[0]\n        # Initialize weights\n        sample_weights = np.ones(n_samples) / n_samples\n\n        for _ in range(self.n_estimators):\n            # Train weak model\n            model = DecisionTreeRegressor(max_depth=1)\n            model.fit(X, y, sample_weight=sample_weights)\n            predictions = model.predict(X)\n\n            # Calculate weighted error\n            error = np.sum(sample_weights * (y != predictions)) / np.sum(sample_weights)\n            if error &gt; 0.5:\n                break\n\n            # Calculate model weight\n            model_weight = self.learning_rate * np.log((1 - error) / error)\n\n            # Update sample weights\n            sample_weights *= np.exp(model_weight * (y != predictions))\n            sample_weights /= np.sum(sample_weights)\n\n            self.models.append(model)\n            self.model_weights.append(model_weight)\n\n    def predict(self, X):\n        # Combine predictions from all models\n        final_prediction = sum(weight * model.predict(X) for model, weight in zip(self.models, self.model_weights))\n        return np.sign(final_prediction)\n</code></pre></p>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#testing-the-model","title":"Testing the Model:","text":"<p>File: <code>adaboost_model_test.py</code> <pre><code>import unittest\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_squared_error\nfrom adaboost_model import AdaBoostRegressor\n\nclass TestAdaBoostRegressor(unittest.TestCase):\n\n    def test_adaboost(self):\n        # Generate synthetic dataset\n        X, y = make_regression(n_samples=100, n_features=1, noise=15, random_state=42)\n        y = np.sign(y)  # Convert to classification-like regression\n\n        # Train AdaBoost Regressor\n        model = AdaBoostRegressor(n_estimators=10)\n        model.fit(X, y)\n\n        # Predict and Evaluate\n        predictions = model.predict(X)\n        mse = mean_squared_error(y, predictions)\n        self.assertTrue(mse &lt; 0.5, \"MSE is too high, AdaBoost not performing well\")\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre></p>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#additional-insights-to-aid-understanding","title":"Additional Insights to Aid Understanding:","text":"<ul> <li>Feature Importance: <pre><code>for i, model in enumerate(model.models):\n    print(f\"Model {i} weight: {model_weights[i]}\")\n</code></pre></li> <li>Early Stopping Implementation: Use validation metrics to stop training if performance does not improve over several iterations.  </li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/AdaBoost_Regression/#testing-and-validation","title":"Testing and Validation:","text":"<p>Use datasets from <code>sklearn</code> (e.g., <code>make_regression</code>) to validate the implementation. Compare AdaBoost with other boosting models like Gradient Boosting and LightGBM to analyze performance differences.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Decision_Tree_Regression/","title":"Decision Tree Regression","text":"<p>This module contains an implementation of Decision Tree Regression, a versatile algorithm for predicting a continuous outcome based on input features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Decision_Tree_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>max_depth</code>: Maximum depth of the decision tree. Controls the complexity of the model.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Decision_Tree_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>decision_tree_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass DecisionTreeRegression:\n\n    def __init__(self, max_depth=None):\n        \"\"\"\n        Constructor for the DecisionTreeRegression class.\n\n        Parameters:\n        - max_depth: Maximum depth of the decision tree.\n        \"\"\"\n        self.max_depth = max_depth\n        self.tree = None\n\n    def _calculate_variance(self, y):\n        \"\"\"\n        Calculate the variance of a set of target values.\n\n        Parameters:\n        - y: Target values (numpy array).\n\n        Returns:\n        - Variance of the target values.\n        \"\"\"\n        return np.var(y)\n\n    def _split_dataset(self, X, y, feature_index, threshold):\n        \"\"\"\n        Split the dataset based on a feature and threshold.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        - feature_index: Index of the feature to split on.\n        - threshold: Threshold value for the split.\n\n        Returns:\n        - Left and right subsets of the dataset.\n        \"\"\"\n        left_mask = X[:, feature_index] &lt;= threshold\n        right_mask = ~left_mask\n        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n\n    def _find_best_split(self, X, y):\n        \"\"\"\n        Find the best split for the dataset.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n\n        Returns:\n        - Index of the best feature and the corresponding threshold.\n        \"\"\"\n        m, n = X.shape\n        best_feature_index = None\n        best_threshold = None\n        best_variance_reduction = 0\n\n        initial_variance = self._calculate_variance(y)\n\n        for feature_index in range(n):\n            thresholds = np.unique(X[:, feature_index])\n\n            for threshold in thresholds:\n                # Split the dataset\n                _, _, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n\n                # Calculate variance reduction\n                left_weight = len(y_left) / m\n                right_weight = len(y_right) / m\n                variance_reduction = initial_variance - (left_weight * self._calculate_variance(y_left) + right_weight * self._calculate_variance(y_right))\n\n                # Update the best split if variance reduction is greater\n                if variance_reduction &gt; best_variance_reduction:\n                    best_feature_index = feature_index\n                    best_threshold = threshold\n                    best_variance_reduction = variance_reduction\n\n        return best_feature_index, best_threshold\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"\n        Recursively build the decision tree.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        - depth: Current depth of the tree.\n\n        Returns:\n        - Node of the decision tree.\n        \"\"\"\n        # Check if max depth is reached or if all target values are the same\n        if depth == self.max_depth or np.all(y == y[0]):\n            return {'value': np.mean(y)}\n\n        # Find the best split\n        feature_index, threshold = self._find_best_split(X, y)\n\n        if feature_index is not None:\n            # Split the dataset\n            X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n\n            # Recursively build left and right subtrees\n            left_subtree = self._build_tree(X_left, y_left, depth + 1)\n            right_subtree = self._build_tree(X_right, y_right, depth + 1)\n\n            return {'feature_index': feature_index,\n                    'threshold': threshold,\n                    'left': left_subtree,\n                    'right': right_subtree}\n        else:\n            # If no split is found, return a leaf node\n            return {'value': np.mean(y)}\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Decision Tree Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        self.tree = self._build_tree(X, y, depth=0)\n\n    def _predict_single(self, node, x):\n        \"\"\"\n        Recursively predict a single data point.\n\n        Parameters:\n        - node: Current node in the decision tree.\n        - x: Input features for prediction.\n\n        Returns:\n        - Predicted value.\n        \"\"\"\n        if 'value' in node:\n            return node['value']\n        else:\n            if x[node['feature_index']] &lt;= node['threshold']:\n                return self._predict_single(node['left'], x)\n            else:\n                return self._predict_single(node['right'], x)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        return np.array([self._predict_single(self.tree, x) for x in X])\n</code></pre> <ul> <li>decision_tree_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom DecisionTreeRegressor import DecisionTreeRegression\n\nclass TestDecisionTreeRegressor(unittest.TestCase):\n\n    def setUp(self):\n        # Create sample data for testing\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 2)\n        self.y_train = 2 * self.X_train[:, 0] + 3 * self.X_train[:, 1] + np.random.normal(0, 0.1, 100)\n\n        self.X_test = np.random.rand(10, 2)\n\n    def test_fit_predict(self):\n        # Test if the model can be fitted and predictions are made\n        dt_model = DecisionTreeRegression(max_depth=3)\n        dt_model.fit(self.X_train, self.y_train)\n\n        # Ensure predictions are made without errors\n        predictions = dt_model.predict(self.X_test)\n\n        # Add your specific assertions based on the expected behavior of your model\n        self.assertIsInstance(predictions, np.ndarray)\n        self.assertEqual(predictions.shape, (10,))\n\n    # Add more test cases as needed\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Elastic_Net_Regression/","title":"Elastic Net Regression","text":"<p>This module contains an implementation of Elastic Net Regression, a powerful linear regression technique that combines both L1 (Lasso) and L2 (Ridge) regularization. Elastic Net is particularly useful when dealing with high-dimensional datasets and can effectively handle correlated features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Elastic_Net_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>alpha</code>: The regularization strength. A positive float value.</li> <li><code>l1_ratio</code>: The ratio of L1 regularization to L2 regularization. Should be between 0 and 1.</li> <li><code>max_iter</code>: The maximum number of iterations to run the optimization algorithm.</li> <li><code>tol</code>: The tolerance for the optimization. If the updates are smaller than this value, the optimization will stop.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Elastic_Net_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>elastic_net_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass ElasticNetRegression:\n    def __init__(self, alpha=1.0, l1_ratio=0.5, max_iter=1000, tol=1e-4):\n        self.alpha = alpha\n        self.l1_ratio = l1_ratio\n        self.max_iter = max_iter\n        self.tol = tol\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.coef_ = np.zeros(n_features)\n        self.intercept_ = 0\n        learning_rate = 0.01  \n\n        for iteration in range(self.max_iter):\n            y_pred = np.dot(X, self.coef_) + self.intercept_\n            error = y - y_pred\n\n            gradient_w = (-2 / n_samples) * (X.T.dot(error)) + self.alpha * (self.l1_ratio * np.sign(self.coef_) + (1 - self.l1_ratio) * 2 * self.coef_)\n            gradient_b = (-2 / n_samples) * np.sum(error)\n\n            new_coef = self.coef_ - learning_rate * gradient_w\n            new_intercept = self.intercept_ - learning_rate * gradient_b\n\n            if np.all(np.abs(new_coef - self.coef_) &lt; self.tol) and np.abs(new_intercept - self.intercept_) &lt; self.tol:\n                break\n\n            self.coef_ = new_coef\n            self.intercept_ = new_intercept\n\n    def predict(self, X):\n        return np.dot(X, self.coef_) + self.intercept_\n</code></pre> <ul> <li>elastic_net_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom sklearn.linear_model import ElasticNet\nfrom ElasticNetRegression import ElasticNetRegression\n\nclass TestElasticNetRegression(unittest.TestCase):\n\n    def test_elastic_net_regression(self):\n        np.random.seed(42)\n        X_train = np.random.rand(100, 1) * 10\n        y_train = 2 * X_train.squeeze() + np.random.randn(100) * 2 \n\n        X_test = np.array([[2.5], [5.0], [7.5]])\n\n        custom_model = ElasticNetRegression(alpha=1.0, l1_ratio=0.5)\n        custom_model.fit(X_train, y_train)\n        custom_predictions = custom_model.predict(X_test)\n\n        sklearn_model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=1000, tol=1e-4)\n        sklearn_model.fit(X_train, y_train)\n        sklearn_predictions = sklearn_model.predict(X_test)\n\n        np.testing.assert_allclose(custom_predictions, sklearn_predictions, rtol=1e-1)\n\n        train_predictions_custom = custom_model.predict(X_train)\n        train_predictions_sklearn = sklearn_model.predict(X_train)\n\n        custom_mse = np.mean((y_train - train_predictions_custom) ** 2)\n        sklearn_mse = np.mean((y_train - train_predictions_sklearn) ** 2)\n\n        print(f\"Custom Model MSE: {custom_mse}\")\n        print(f\"Scikit-learn Model MSE: {sklearn_mse}\")\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Gradient_Boosting_Regression/","title":"Gradient Boosting Regression","text":"<p>This module contains an implementation of Gradient Boosting Regression, an ensemble learning method that combines multiple weak learners (typically decision trees) to create a more robust and accurate model for predicting continuous outcomes based on input features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Gradient_Boosting_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>n_estimators</code>: Number of boosting stages (trees) to be run.</li> <li><code>learning_rate</code>: Step size shrinkage to prevent overfitting.</li> <li><code>max_depth</code>: Maximum depth of each decision tree.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Gradient_Boosting_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>gradient_boosting_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass GradientBoostingRegression:\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        \"\"\"\n        Constructor for the GradientBoostingRegression class.\n\n        Parameters:\n        - n_estimators: Number of trees in the ensemble.\n        - learning_rate: Step size for each tree's contribution.\n        - max_depth: Maximum depth of each decision tree.\n        \"\"\"\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the gradient boosting regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        # Initialize predictions with the mean of the target values\n        predictions = np.mean(y) * np.ones_like(y)\n\n        for _ in range(self.n_estimators):\n            # Compute residuals\n            residuals = y - predictions\n\n            # Fit a decision tree to the residuals\n            tree = self._fit_tree(X, residuals, depth=0)\n\n            # Update predictions using the tree's contribution scaled by the learning rate\n            predictions += self.learning_rate * self._predict_tree(X, tree)\n\n            # Save the tree in the ensemble\n            self.trees.append(tree)\n\n    def _fit_tree(self, X, y, depth):\n        \"\"\"\n        Fit a decision tree to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        - depth: Current depth of the tree.\n\n        Returns:\n        - Tree structure (dictionary).\n        \"\"\"\n        if depth == self.max_depth:\n            # If the maximum depth is reached, return the mean of the target values\n            return np.mean(y)\n\n        # Find the best split point\n        feature_index, threshold = self._find_best_split(X, y)\n\n        if feature_index is None:\n            # If no split improves the purity, return the mean of the target values\n            return np.mean(y)\n\n        # Split the data\n        mask = X[:, feature_index] &lt; threshold\n        left_tree = self._fit_tree(X[mask], y[mask], depth + 1)\n        right_tree = self._fit_tree(X[~mask], y[~mask], depth + 1)\n\n        # Return the tree structure\n        return {'feature_index': feature_index, 'threshold': threshold,\n                'left_tree': left_tree, 'right_tree': right_tree}\n\n    def _find_best_split(self, X, y):\n        \"\"\"\n        Find the best split point for a decision tree.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n\n        Returns:\n        - Best feature index and threshold for the split.\n        \"\"\"\n        m, n = X.shape\n        if m &lt;= 1:\n            return None, None  # No split is possible\n\n        # Calculate the initial impurity\n        initial_impurity = self._calculate_impurity(y)\n\n        # Initialize variables to store the best split parameters\n        best_feature_index, best_threshold, best_impurity_reduction = None, None, 0\n\n        for feature_index in range(n):\n            # Sort the feature values and corresponding target values\n            sorted_indices = np.argsort(X[:, feature_index])\n            sorted_X = X[sorted_indices, feature_index]\n            sorted_y = y[sorted_indices]\n\n            # Initialize variables to keep track of impurity and counts for the left and right nodes\n            left_impurity, left_count = 0, 0\n            right_impurity, right_count = initial_impurity, m\n\n            for i in range(1, m):\n                # Update impurity and counts for the left and right nodes\n                value = sorted_X[i]\n                left_impurity += (i / m) * self._calculate_impurity(sorted_y[i-1:i+1])\n                left_count += 1\n                right_impurity -= ((i-1) / m) * self._calculate_impurity(sorted_y[i-1:i+1])\n                right_count -= 1\n\n                # Calculate impurity reduction\n                impurity_reduction = initial_impurity - (left_count / m * left_impurity + right_count / m * right_impurity)\n\n                # Check if this is the best split so far\n                if impurity_reduction &gt; best_impurity_reduction:\n                    best_feature_index = feature_index\n                    best_threshold = value\n                    best_impurity_reduction = impurity_reduction\n\n        return best_feature_index, best_threshold\n\n    def _calculate_impurity(self, y):\n        \"\"\"\n        Calculate the impurity of a node.\n\n        Parameters:\n        - y: Target values (numpy array).\n\n        Returns:\n        - Impurity.\n        \"\"\"\n        # For regression, impurity is the variance of the target values\n        return np.var(y)\n\n    def _predict_tree(self, X, tree):\n        \"\"\"\n        Make predictions using a decision tree.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - tree: Tree structure (dictionary).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        if 'feature_index' not in tree:\n            # If the node is a leaf, return the constant value\n            return tree\n        else:\n            # Recursively traverse the tree\n            mask = X[:, tree['feature_index']] &lt; tree['threshold']\n            return np.where(mask, self._predict_tree(X, tree['left_tree']), self._predict_tree(X, tree['right_tree']))\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data using the Gradient Boosting Regression.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        predictions = np.sum(self.learning_rate * self._predict_tree(X, tree) for tree in self.trees)\n        return predictions\n</code></pre> <ul> <li>gradient_boosting_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom GradientBoostingRegressor import GradientBoostingRegression\n\nclass TestGradientBoostingRegressor(unittest.TestCase):\n\n    def setUp(self):\n        # Create sample data for testing\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 2)\n        self.y_train = 2 * self.X_train[:, 0] + 3 * self.X_train[:, 1] + np.random.normal(0, 0.1, 100)\n\n        self.X_test = np.random.rand(10, 2)\n\n    def test_fit_predict(self):\n        # Test if the model can be fitted and predictions are made\n        gbr_model = GradientBoostingRegression(n_estimators=5, learning_rate=0.1, max_depth=3)\n        gbr_model.fit(self.X_train, self.y_train)\n\n        # Ensure predictions are made without errors\n        predictions = gbr_model.predict(self.X_test)\n\n        # Add your specific assertions based on the expected behavior of your model\n        self.assertIsInstance(predictions, np.ndarray)\n        self.assertEqual(predictions.shape, (10,))\n\n    # Add more test cases as needed\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Huber_Regression/","title":"Huber Regression","text":"<p>This module contains an implementation of Huber Regression, a robust linear regression technique that combines the properties of both least squares and absolute error loss functions. Huber Regression is particularly useful when dealing with datasets that have outliers, as it is less sensitive to outliers compared to standard linear regression.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Huber_Regression/#overview","title":"Overview","text":"<p>Huber Regression is a regression algorithm that adds a penalty based on the Huber loss function. This loss function is quadratic for small errors and linear for large errors, providing robustness against outliers.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Huber_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>alpha</code>: The regularization strength. A positive float value.</li> <li><code>epsilon</code>: The threshold for the Huber loss function. A positive float value.</li> <li><code>max_iter</code>: The maximum number of iterations to run the optimization algorithm.</li> <li><code>tol</code>: The tolerance for the optimization. If the updates are smaller than this value, the optimization will stop.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Huber_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>huber_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass HuberRegression:\n    def __init__(self, alpha=1.0, epsilon=1.35, max_iter=1000, tol=1e-4):\n        self.alpha = alpha\n        self.epsilon = epsilon\n        self.max_iter = max_iter\n        self.tol = tol\n        self.coef_ = None\n        self.intercept_ = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.coef_ = np.zeros(n_features)\n        self.intercept_ = 0\n        learning_rate = 0.01\n\n        for iteration in range(self.max_iter):\n            y_pred = np.dot(X, self.coef_) + self.intercept_\n            error = y - y_pred\n\n            # Compute Huber gradient\n            mask = np.abs(error) &lt;= self.epsilon\n            gradient_w = (-2 / n_samples) * (X.T.dot(error * mask) + self.epsilon * np.sign(error) * (~mask)) + self.alpha * self.coef_\n            gradient_b = (-2 / n_samples) * (np.sum(error * mask) + self.epsilon * np.sign(error) * (~mask))\n\n            new_coef = self.coef_ - learning_rate * gradient_w\n            new_intercept = self.intercept_ - learning_rate * gradient_b\n\n            if np.all(np.abs(new_coef - self.coef_) &lt; self.tol) and np.abs(new_intercept - self.intercept_) &lt; self.tol:\n                break\n\n            self.coef_ = new_coef\n            self.intercept_ = new_intercept\n\n    def predict(self, X):\n        return np.dot(X, self.coef_) + self.intercept_\n</code></pre> <ul> <li>huber_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom sklearn.linear_model import HuberRegressor\nfrom HuberRegression import HuberRegression\n\nclass TestHuberRegression(unittest.TestCase):\n\n    def test_huber_regression(self):\n        np.random.seed(42)\n        X_train = np.random.rand(100, 1) * 10\n        y_train = 2 * X_train.squeeze() + np.random.randn(100) * 2\n\n        X_test = np.array([[2.5], [5.0], [7.5]])\n\n        huber_model = HuberRegression(alpha=1.0, epsilon=1.35)\n        huber_model.fit(X_train, y_train)\n        huber_predictions = huber_model.predict(X_test)\n\n        sklearn_model = HuberRegressor(alpha=1.0, epsilon=1.35, max_iter=1000, tol=1e-4)\n        sklearn_model.fit(X_train, y_train)\n        sklearn_predictions = sklearn_model.predict(X_test)\n\n        np.testing.assert_allclose(huber_predictions, sklearn_predictions, rtol=1e-1)\n\n        train_predictions_huber = huber_model.predict(X_train)\n        train_predictions_sklearn = sklearn_model.predict(X_train)\n\n        huber_mse = np.mean((y_train - train_predictions_huber) ** 2)\n        sklearn_mse = np.mean((y_train - train_predictions_sklearn) ** 2)\n\n        print(f\"Huber Model MSE: {huber_mse}\")\n        print(f\"Scikit-learn Model MSE: {sklearn_mse}\")\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/K_Nearest_Neighbors_Regression/","title":"K Nearest Neighbors Regression","text":"<p>This module contains an implementation of K-Nearest Neighbors Regression, a simple yet effective algorithm for predicting continuous outcomes based on input features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/K_Nearest_Neighbors_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>k</code>: Number of neighbors to consider for prediction.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/K_Nearest_Neighbors_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>k_nearest_neighbors_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass KNNRegression:\n    def __init__(self, k=5):\n        \"\"\"\n        Constructor for the KNNRegression class.\n\n        Parameters:\n        - k: Number of neighbors to consider.\n        \"\"\"\n        self.k = k\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the KNN model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        predictions = []\n        for x in X:\n            # Calculate Euclidean distances between the input point and all training points\n            distances = np.linalg.norm(self.X_train - x, axis=1)\n\n            # Get indices of k-nearest neighbors\n            indices = np.argsort(distances)[:self.k]\n\n            # Average the target values of k-nearest neighbors\n            predicted_value = np.mean(self.y_train[indices])\n            predictions.append(predicted_value)\n\n        return np.array(predictions)\n</code></pre> <ul> <li>k_nearest_neighbors_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom KNearestNeighborsRegression import KNNRegression\n\nclass TestKNNRegression(unittest.TestCase):\n\n    def test_knn_regression(self):\n        # Create synthetic data\n        np.random.seed(42)\n        X_train = np.random.rand(100, 1) * 10\n        y_train = 2 * X_train.squeeze() + np.random.randn(100) * 2  # Linear relationship with noise\n\n        X_test = np.array([[2.5], [5.0], [7.5]])\n\n        # Initialize and fit the KNN Regression model\n        knn_model = KNNRegression(k=3)\n        knn_model.fit(X_train, y_train)\n\n        # Test predictions\n        predictions = knn_model.predict(X_test)\n        expected_predictions = [2 * 2.5, 2 * 5.0, 2 * 7.5]  # Assuming a linear relationship\n\n        # Check if predictions are close to the expected values\n        np.testing.assert_allclose(predictions, expected_predictions, rtol=1e-5)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Lasso_Regression/","title":"Lasso Regression","text":"<p>This module contains an implementation of Lasso Regression, a linear regression technique with L1 regularization.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Lasso_Regression/#overview","title":"Overview","text":"<p>Lasso Regression is a regression algorithm that adds a penalty term based on the absolute values of the coefficients. This penalty term helps in feature selection by driving some of the coefficients to exactly zero, effectively ignoring certain features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Lasso_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>learning_rate</code>: The step size for gradient descent.</li> <li><code>lambda_param</code>: Regularization strength (L1 penalty).</li> <li><code>n_iterations</code>: The number of iterations for gradient descent.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Lasso_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>lasso_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass LassoRegression:\n    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_iterations=1000):\n        \"\"\"\n        Constructor for the LassoRegression class.\n\n        Parameters:\n        - learning_rate: The step size for gradient descent.\n        - lambda_param: Regularization strength.\n        - n_iterations: The number of iterations for gradient descent.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Lasso Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        # Initialize weights and bias\n        num_samples, num_features = X.shape\n        self.weights = np.zeros(num_features)\n        self.bias = 0\n\n        # Perform gradient descent\n        for _ in range(self.n_iterations):\n            predictions = np.dot(X, self.weights) + self.bias\n            errors = y - predictions\n\n            # Update weights and bias\n            self.weights += self.learning_rate * (1/num_samples) * (np.dot(X.T, errors) - self.lambda_param * np.sign(self.weights))\n            self.bias += self.learning_rate * (1/num_samples) * np.sum(errors)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n</code></pre> <ul> <li>lasso_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom LassoRegression import LassoRegression\n\nclass TestLassoRegression(unittest.TestCase):\n    def setUp(self):\n        # Create a sample dataset\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 2)\n        self.y_train = 3 * self.X_train[:, 0] + 2 * self.X_train[:, 1] + np.random.randn(100)\n\n        self.X_test = np.random.rand(10, 2)\n\n    def test_fit_predict(self):\n        # Test the fit and predict methods\n        model = LassoRegression(learning_rate=0.01, lambda_param=0.1, n_iterations=1000)\n        model.fit(self.X_train, self.y_train)\n        predictions = model.predict(self.X_test)\n\n        # Ensure predictions are of the correct shape\n        self.assertEqual(predictions.shape, (10,))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Logistic_Regression/","title":"Logistic Regression","text":"<p>This module contains an implementation of Logistic Regression, a popular algorithm for binary classification.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Logistic_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>learning_rate</code>: Step size for gradient descent.</li> <li><code>n_iterations</code>: Number of iterations for gradient descent.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Logistic_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>logistic_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        \"\"\"\n        Constructor for the LogisticRegression class.\n\n        Parameters:\n        - learning_rate: The step size for gradient descent.\n        - n_iterations: The number of iterations for gradient descent.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def _sigmoid(self, z):\n        \"\"\"\n        Sigmoid activation function.\n\n        Parameters:\n        - z: Linear combination of input features and weights.\n\n        Returns:\n        - Sigmoid of z.\n        \"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    def _initialize_parameters(self, n_features):\n        \"\"\"\n        Initialize weights and bias.\n\n        Parameters:\n        - n_features: Number of input features.\n\n        Returns:\n        - Initialized weights and bias.\n        \"\"\"\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Logistic Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target labels (numpy array).\n        \"\"\"\n        n_samples, n_features = X.shape\n        self._initialize_parameters(n_features)\n\n        for _ in range(self.n_iterations):\n            # Linear combination of features and weights\n            linear_combination = np.dot(X, self.weights) + self.bias\n\n            # Predictions using the sigmoid function\n            predictions = self._sigmoid(linear_combination)\n\n            # Update weights and bias using gradient descent\n            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n            db = (1 / n_samples) * np.sum(predictions - y)\n\n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted labels (numpy array).\n        \"\"\"\n        linear_combination = np.dot(X, self.weights) + self.bias\n        predictions = self._sigmoid(linear_combination)\n\n        # Convert probabilities to binary predictions (0 or 1)\n        return np.round(predictions)\n</code></pre> <ul> <li>logistic_regression_test.py file </li> </ul> <pre><code>import numpy as np\nimport unittest\nfrom LogisticRegression import LogisticRegression\n\nclass TestLogisticRegression(unittest.TestCase):\n    def setUp(self):\n        # Generate synthetic data for testing\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 2)\n        self.y_train = (np.random.rand(100) &gt; 0.5).astype(int)\n\n        self.X_test = np.random.rand(20, 2)\n\n    def test_fit_predict(self):\n        model = LogisticRegression(learning_rate=0.01, n_iterations=1000)\n        model.fit(self.X_train, self.y_train)\n        predictions = model.predict(self.X_test)\n\n        self.assertEqual(predictions.shape, (20,))\n        self.assertTrue(np.all(predictions == 0) or np.all(predictions == 1))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Neural_Network_Regression/","title":"Neural Network Regression","text":"<p>This module contains an implementation of Neural Network Regression, a powerful algorithm for predicting continuous outcomes based on input features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Neural_Network_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>input_size</code>: Number of features in the input data.</li> <li><code>hidden_size</code>: Number of neurons in the hidden layer.</li> <li><code>output_size</code>: Number of output neurons.</li> <li><code>learning_rate</code>: Step size for updating weights during training.</li> <li><code>n_iterations</code>: Number of iterations for training the neural network.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Neural_Network_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>neural_network_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass NeuralNetworkRegression:\n    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, n_iterations=1000):\n        \"\"\"\n        Constructor for the NeuralNetworkRegression class.\n\n        Parameters:\n        - input_size: Number of input features.\n        - hidden_size: Number of neurons in the hidden layer.\n        - output_size: Number of output neurons.\n        - learning_rate: Step size for gradient descent.\n        - n_iterations: Number of iterations for gradient descent.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n        # Initialize weights and biases\n        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)\n        self.bias_hidden = np.zeros((1, self.hidden_size))\n        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)\n        self.bias_output = np.zeros((1, self.output_size))\n\n    def sigmoid(self, x):\n        \"\"\"Sigmoid activation function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        \"\"\"Derivative of the sigmoid function.\"\"\"\n        return x * (1 - x)\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Neural Network model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        for _ in range(self.n_iterations):\n            # Forward pass\n            hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n            hidden_layer_output = self.sigmoid(hidden_layer_input)\n\n            output_layer_input = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n            predicted_output = self.sigmoid(output_layer_input)\n\n            # Backpropagation\n            error = y - predicted_output\n            output_delta = error * self.sigmoid_derivative(predicted_output)\n\n            hidden_layer_error = output_delta.dot(self.weights_hidden_output.T)\n            hidden_layer_delta = hidden_layer_error * self.sigmoid_derivative(hidden_layer_output)\n\n            # Update weights and biases\n            self.weights_hidden_output += hidden_layer_output.T.dot(output_delta) * self.learning_rate\n            self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n\n            self.weights_input_hidden += X.T.dot(hidden_layer_delta) * self.learning_rate\n            self.bias_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * self.learning_rate\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n        hidden_layer_output = self.sigmoid(hidden_layer_input)\n\n        output_layer_input = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n        predicted_output = self.sigmoid(output_layer_input)\n\n        return predicted_output\n</code></pre> <ul> <li>neural_network_regression_test.py file </li> </ul> <pre><code>import numpy as np\nimport unittest\nfrom NeuralNetworkRegression import NeuralNetworkRegression\n\nclass TestNeuralNetworkRegression(unittest.TestCase):\n    def setUp(self):\n        # Generate synthetic data for testing\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 3)\n        self.y_train = np.random.rand(100, 1)\n\n        self.X_test = np.random.rand(10, 3)\n\n    def test_fit_predict(self):\n        # Initialize and fit the model\n        model = NeuralNetworkRegression(input_size=3, hidden_size=4, output_size=1, learning_rate=0.01, n_iterations=1000)\n        model.fit(self.X_train, self.y_train)\n\n        # Ensure predictions have the correct shape\n        predictions = model.predict(self.X_test)\n        self.assertEqual(predictions.shape, (10, 1))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Polynomial_Regression/","title":"Polynomial Regression","text":"<p>This module contains an implementation of Polynomial Regression, an extension of Linear Regression that models the relationship between the independent variable and the dependent variable as a polynomial.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Polynomial_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>degree</code>: Degree of the polynomial.</li> <li><code>learning_rate</code>: The step size for gradient descent.</li> <li><code>n_iterations</code>: The number of iterations for gradient descent.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Polynomial_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>polynomial_regression.py file </li> </ul> <pre><code>import numpy as np\n\n# Polynomial regression implementation\nclass PolynomialRegression:\n    def __init__(self, degree=2, learning_rate=0.01, n_iterations=1000):\n        \"\"\"\n        Constructor for the PolynomialRegression class.\n\n        Parameters:\n        - degree: Degree of the polynomial.\n        - learning_rate: The step size for gradient descent.\n        - n_iterations: The number of iterations for gradient descent.\n        \"\"\"\n        self.degree = degree\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def _polynomial_features(self, X):\n        \"\"\"\n        Create polynomial features up to the specified degree.\n\n        Parameters:\n        - X: Input features (numpy array).\n\n        Returns:\n        - Polynomial features (numpy array).\n        \"\"\"\n        return np.column_stack([X ** i for i in range(1, self.degree + 1)])\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the polynomial regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        X_poly = self._polynomial_features(X)\n        self.weights = np.zeros((X_poly.shape[1], 1))\n        self.bias = 0\n\n        for _ in range(self.n_iterations):\n            predictions = np.dot(X_poly, self.weights) + self.bias\n            errors = predictions - y\n\n            self.weights -= self.learning_rate * (1 / len(X_poly)) * np.dot(X_poly.T, errors)\n            self.bias -= self.learning_rate * (1 / len(X_poly)) * np.sum(errors)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        X_poly = self._polynomial_features(X)\n        return np.dot(X_poly, self.weights) + self.bias\n</code></pre> <ul> <li>polynomial_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom PolynomialRegression import PolynomialFeatures\n\nclass TestPolynomialRegression(unittest.TestCase):\n\n    def setUp(self):\n        # Create synthetic data for testing\n        np.random.seed(42)\n        self.X_train = 2 * np.random.rand(100, 1)\n        self.y_train = 4 + 3 * self.X_train + np.random.randn(100, 1)\n\n    def test_fit_predict(self):\n        # Test the fit and predict methods\n        poly_model = PolynomialFeatures(degree=2)\n        poly_model.fit(self.X_train, self.y_train)\n\n        # Create test data\n        X_test = np.array([[1.5], [2.0]])\n\n        # Make predictions\n        predictions = poly_model.predict(X_test)\n\n        # Assert that the predictions are NumPy arrays\n        self.assertTrue(isinstance(predictions, np.ndarray))\n\n        # Assert that the shape of predictions is as expected\n        self.assertEqual(predictions.shape, (X_test.shape[0], 1))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Random_Forest_Regression/","title":"Random Forest Regression","text":"<p>This module contains an implementation of Random Forest Regression, an ensemble learning method that combines multiple decision trees to create a more robust and accurate model for predicting continuous outcomes based on input features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Random_Forest_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>n_trees</code>: Number of trees in the random forest.</li> <li><code>max_depth</code>: Maximum depth of each decision tree.</li> <li><code>max_features</code>: Maximum number of features to consider for each split.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Random_Forest_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>random_forest_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass RandomForestRegression:\n\n    def __init__(self, n_trees=100, max_depth=None, max_features=None):\n        \"\"\"\n        Constructor for the RandomForestRegression class.\n\n        Parameters:\n        - n_trees: Number of trees in the random forest.\n        - max_depth: Maximum depth of each decision tree.\n        - max_features: Maximum number of features to consider for each split.\n        \"\"\"\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.max_features = max_features\n        self.trees = []\n\n    def _bootstrap_sample(self, X, y):\n        \"\"\"\n        Create a bootstrap sample of the dataset.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n\n        Returns:\n        - Bootstrap sample of X and y.\n        \"\"\"\n        indices = np.random.choice(len(X), len(X), replace=True)\n        return X[indices], y[indices]\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"\n        Recursively build a decision tree.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        - depth: Current depth of the tree.\n\n        Returns:\n        - Node of the decision tree.\n        \"\"\"\n        if depth == self.max_depth or np.all(y == y[0]):\n            return {'value': np.mean(y)}\n\n        n_features = X.shape[1]\n        if self.max_features is None:\n            subset_features = np.arange(n_features)\n        else:\n            subset_features = np.random.choice(n_features, self.max_features, replace=False)\n\n        # Create a random subset of features for this tree\n        X_subset = X[:, subset_features]\n\n        # Create a bootstrap sample\n        X_bootstrap, y_bootstrap = self._bootstrap_sample(X_subset, y)\n\n        # Find the best split using the selected subset of features\n        feature_index, threshold = self._find_best_split(X_bootstrap, y_bootstrap, subset_features)\n\n        if feature_index is not None:\n            # Split the dataset\n            X_left, X_right, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n\n            # Recursively build left and right subtrees\n            left_subtree = self._build_tree(X_left, y_left, depth + 1)\n            right_subtree = self._build_tree(X_right, y_right, depth + 1)\n\n            return {'feature_index': feature_index,\n                    'threshold': threshold,\n                    'left': left_subtree,\n                    'right': right_subtree}\n        else:\n            # If no split is found, return a leaf node\n            return {'value': np.mean(y)}\n\n    def _find_best_split(self, X, y, subset_features):\n        \"\"\"\n        Find the best split for a subset of features.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        - subset_features: Subset of features to consider.\n\n        Returns:\n        - Index of the best feature and the corresponding threshold.\n        \"\"\"\n        m, n = X.shape\n        best_feature_index = None\n        best_threshold = None\n        best_variance_reduction = 0\n\n        initial_variance = self._calculate_variance(y)\n\n        for feature_index in subset_features:\n            thresholds = np.unique(X[:, feature_index])\n\n            for threshold in thresholds:\n                # Split the dataset\n                _, _, y_left, y_right = self._split_dataset(X, y, feature_index, threshold)\n\n                # Calculate variance reduction\n                left_weight = len(y_left) / m\n                right_weight = len(y_right) / m\n                variance_reduction = initial_variance - (left_weight * self._calculate_variance(y_left) + right_weight * self._calculate_variance(y_right))\n\n                # Update the best split if variance reduction is greater\n                if variance_reduction &gt; best_variance_reduction:\n                    best_feature_index = feature_index\n                    best_threshold = threshold\n                    best_variance_reduction = variance_reduction\n\n        return best_feature_index, best_threshold\n\n    def _calculate_variance(self, y):\n        \"\"\"\n        Calculate the variance of a set of target values.\n\n        Parameters:\n        - y: Target values (numpy array).\n\n        Returns:\n        - Variance of the target values.\n        \"\"\"\n        return np.var(y)\n\n    def _split_dataset(self, X, y, feature_index, threshold):\n        \"\"\"\n        Split the dataset based on a feature and threshold.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        - feature_index: Index of the feature to split on.\n        - threshold: Threshold value for the split.\n\n        Returns:\n        - Left and right subsets of the dataset.\n        \"\"\"\n        left_mask = X[:, feature_index] &lt;= threshold\n        right_mask = ~left_mask\n        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Random Forest Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        self.trees = []\n        for _ in range(self.n_trees):\n            # Create a bootstrap sample for each tree\n            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)\n\n            # Build a decision tree and add it to the forest\n            tree = self._build_tree(X_bootstrap, y_bootstrap, depth=0)\n            self.trees.append(tree)\n\n    def _predict_single(self, tree, x):\n        \"\"\"\n        Recursively predict a single data point using a decision tree.\n\n        Parameters:\n        - tree: Decision tree node.\n        - x: Input features for prediction.\n\n        Returns:\n        - Predicted value.\n        \"\"\"\n        if 'value' in tree:\n            return tree['value']\n        else:\n            if x[tree['feature_index']] &lt;= tree['threshold']:\n                return self._predict_single(tree['left'], x)\n            else:\n                return self._predict_single(tree['right'], x)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data using the Random Forest.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        predictions = np.array([self._predict_single(tree, x) for x in X for tree in self.trees])\n        return np.mean(predictions.reshape(-1, len(self.trees)), axis=1)\n</code></pre> <ul> <li>random_forest_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom RandomForestRegressor import RandomForestRegression\n\nclass TestRandomForestRegressor(unittest.TestCase):\n    def setUp(self):\n        # Create sample data for testing\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 2)\n        self.y_train = 2 * self.X_train[:, 0] + 3 * self.X_train[:, 1] + np.random.normal(0, 0.1, 100)\n\n        self.X_test = np.random.rand(10, 2)\n\n    def test_fit_predict(self):\n        # Test if the model can be fitted and predictions are made\n        rfr_model = RandomForestRegression(n_trees=5, max_depth=3, max_features=2)\n        rfr_model.fit(self.X_train, self.y_train)\n\n        # Ensure predictions are made without errors\n        predictions = rfr_model.predict(self.X_test)\n\n        # Add your specific assertions based on the expected behavior of your model\n        self.assertIsInstance(predictions, np.ndarray)\n        self.assertEqual(predictions.shape, (10,))\n\n    # Add more test cases as needed\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Ridge_Regression/","title":"Ridge Regression","text":"<p>This module contains an implementation of Ridge Regression, a linear regression variant that includes regularization to prevent overfitting.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Ridge_Regression/#overview","title":"Overview","text":"<p>Ridge Regression is a linear regression technique with an added regularization term to handle multicollinearity and prevent the model from becoming too complex.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Ridge_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>alpha</code>: Regularization strength. A higher alpha increases the penalty for large coefficients.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Ridge_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>ridge_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass RidgeRegression:\n    def __init__(self, alpha=1.0):\n        \"\"\"\n        Constructor for the Ridge Regression class.\n\n        Parameters:\n        - alpha: Regularization strength. Higher values specify stronger regularization.\n        \"\"\"\n        self.alpha = alpha\n        self.weights = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Ridge Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        # Add a column of ones to the input features for the bias term\n        X_bias = np.c_[np.ones(X.shape[0]), X]\n\n        # Compute the closed-form solution for Ridge Regression\n        identity_matrix = np.identity(X_bias.shape[1])\n        self.weights = np.linalg.inv(X_bias.T @ X_bias + self.alpha * identity_matrix) @ X_bias.T @ y\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        # Add a column of ones to the input features for the bias term\n        X_bias = np.c_[np.ones(X.shape[0]), X]\n\n        # Make predictions using the learned weights\n        predictions = X_bias @ self.weights\n\n        return predictions\n</code></pre> <ul> <li>ridge_regression_test.py file </li> </ul> <pre><code>import numpy as np\nimport unittest\nfrom RidgeRegression import RidgeRegression  # Assuming your RidgeRegression class is in a separate file\n\nclass TestRidgeRegression(unittest.TestCase):\n    def test_fit_predict(self):\n        # Generate synthetic data for testing\n        np.random.seed(42)\n        X_train = np.random.rand(100, 2)\n        y_train = 3 * X_train[:, 0] + 5 * X_train[:, 1] + 2 + 0.1 * np.random.randn(100)\n        X_test = np.random.rand(20, 2)\n\n        # Create a Ridge Regression model\n        ridge_model = RidgeRegression(alpha=0.1)\n\n        # Fit the model to training data\n        ridge_model.fit(X_train, y_train)\n\n        # Make predictions on test data\n        predictions = ridge_model.predict(X_test)\n\n        # Ensure the predictions have the correct shape\n        self.assertEqual(predictions.shape, (20,))\n\n    def test_invalid_alpha(self):\n        # Check if an exception is raised for an invalid alpha value\n        with self.assertRaises(ValueError):\n            RidgeRegression(alpha=-1)\n\n    # Add more test cases as needed\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/Support_Vector_Regression/","title":"Support Vector Regression","text":"<p>This module contains an implementation of Support Vector Regression (SVR), a regression technique using Support Vector Machines (SVM) principles.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/Support_Vector_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>epsilon</code>: Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function.</li> <li><code>C</code>: Regularization parameter. The strength of the regularization is inversely proportional to C.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/Support_Vector_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>support_vector_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass SupportVectorRegression:\n\n    def __init__(self, epsilon=0.1, C=1.0):\n        \"\"\"\n        Constructor for the SupportVectorRegression class.\n\n        Parameters:\n        - epsilon: Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function.\n        - C: Regularization parameter. The strength of the regularization is inversely proportional to C.\n        \"\"\"\n        self.epsilon = epsilon\n        self.C = C\n        self.weights = None\n        self.bias = None\n\n    def _linear_kernel(self, X1, X2):\n        \"\"\"\n        Linear kernel function.\n\n        Parameters:\n        - X1, X2: Input data (numpy arrays).\n\n        Returns:\n        - Linear kernel result (numpy array).\n        \"\"\"\n        return np.dot(X1, X2.T)\n\n    def _compute_kernel_matrix(self, X):\n        \"\"\"\n        Compute the kernel matrix for the linear kernel.\n\n        Parameters:\n        - X: Input data (numpy array).\n\n        Returns:\n        - Kernel matrix (numpy array).\n        \"\"\"\n        m = X.shape[0]\n        kernel_matrix = np.zeros((m, m))\n\n        for i in range(m):\n            for j in range(m):\n                kernel_matrix[i, j] = self._linear_kernel(X[i, :], X[j, :])\n\n        return kernel_matrix\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Support Vector Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        m, n = X.shape\n\n        # Create the kernel matrix\n        kernel_matrix = self._compute_kernel_matrix(X)\n\n        # Quadratic programming problem coefficients\n        P = np.vstack([np.hstack([kernel_matrix, -kernel_matrix]),\n                       np.hstack([-kernel_matrix, kernel_matrix])])\n        q = np.vstack([self.epsilon * np.ones((m, 1)) - y, self.epsilon * np.ones((m, 1)) + y])\n\n        # Constraints matrix\n        G = np.vstack([np.eye(2 * m), -np.eye(2 * m)])\n        h = np.vstack([self.C * np.ones((2 * m, 1)), np.zeros((2 * m, 1))])\n\n        # Solve the quadratic programming problem\n        solution = np.linalg.solve(P, q)\n\n        # Extract weights and bias\n        self.weights = solution[:n]\n        self.bias = solution[n]\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        predictions = np.dot(X, self.weights) + self.bias\n        return predictions\n</code></pre> <ul> <li>support_vector_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom SVR import SupportVectorRegression\n\nclass TestSupportVectorRegression(unittest.TestCase):\n\n    def setUp(self):\n        # Create synthetic data for testing\n        np.random.seed(42)\n        self.X_train = 2 * np.random.rand(100, 1)\n        self.y_train = 4 + 3 * self.X_train + np.random.randn(100, 1)\n\n    def test_fit_predict(self):\n        # Test the fit and predict methods\n        svr_model = SupportVectorRegression(epsilon=0.1, C=1.0)\n        svr_model.fit(self.X_train, self.y_train)\n\n        # Create test data\n        X_test = np.array([[1.5], [2.0]])\n\n        # Make predictions\n        predictions = svr_model.predict(X_test)\n\n        # Assert that the predictions are NumPy arrays\n        self.assertTrue(isinstance(predictions, np.ndarray))\n\n        # Assert that the shape of predictions is as expected\n        self.assertEqual(predictions.shape, (X_test.shape[0], 1))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/XG_Boost_Regression/","title":"XG Boost Regression","text":"<p>This module contains an implementation of the XGBoost Regressor, a popular ensemble learning algorithm that combines the predictions from multiple decision trees to create a more robust and accurate model for regression tasks.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/XG_Boost_Regression/#parameters","title":"Parameters","text":"<ul> <li><code>n_estimators</code>: Number of boosting rounds (trees).</li> <li><code>learning_rate</code>: Step size shrinkage to prevent overfitting.</li> <li><code>max_depth</code>: Maximum depth of each tree.</li> <li><code>gamma</code>: Minimum loss reduction required to make a further partition.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/XG_Boost_Regression/#scratch-code","title":"Scratch Code","text":"<ul> <li>x_g_boost_regression.py file </li> </ul> <pre><code>import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nclass XGBoostRegressor:\n\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, gamma=0):\n        \"\"\"\n        Constructor for the XGBoostRegressor class.\n\n        Parameters:\n        - n_estimators: Number of boosting rounds (trees).\n        - learning_rate: Step size shrinkage to prevent overfitting.\n        - max_depth: Maximum depth of each tree.\n        - gamma: Minimum loss reduction required to make a further partition.\n        \"\"\"\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.gamma = gamma\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the XGBoost model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        # Initialize residuals\n        residuals = np.copy(y)\n\n        for _ in range(self.n_estimators):\n            # Fit a weak learner (decision tree) to the residuals\n            tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.gamma)\n            tree.fit(X, residuals)\n\n            # Compute predictions from the weak learner\n            predictions = tree.predict(X)\n\n            # Update residuals with the weighted sum of previous residuals and predictions\n            residuals -= self.learning_rate * predictions\n\n            # Store the tree in the list\n            self.trees.append(tree)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        # Initialize predictions with zeros\n        predictions = np.zeros(X.shape[0])\n\n        # Make predictions using each tree and update the overall prediction\n        for tree in self.trees:\n            predictions += self.learning_rate * tree.predict(X)\n\n        return predictions\n</code></pre> <ul> <li>x_g_boost_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom XGBoostRegressor import XGBoostRegressor\n\nclass TestXGBoostRegressor(unittest.TestCase):\n\n    def setUp(self):\n        # Generate synthetic data for testing\n        np.random.seed(42)\n        self.X_train = np.random.rand(100, 5)\n        self.y_train = np.random.rand(100)\n        self.X_test = np.random.rand(20, 5)\n\n    def test_fit_predict(self):\n        # Test the fit and predict methods\n        xgb_model = XGBoostRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, gamma=0.1)\n        xgb_model.fit(self.X_train, self.y_train)\n        predictions = xgb_model.predict(self.X_test)\n\n        # Ensure predictions have the correct shape\n        self.assertEqual(predictions.shape, (20,))\n\n    def test_invalid_parameters(self):\n        # Test invalid parameter values\n        with self.assertRaises(ValueError):\n            XGBoostRegressor(n_estimators=-1, learning_rate=0.1, max_depth=3, gamma=0.1)\n\n        with self.assertRaises(ValueError):\n            XGBoostRegressor(n_estimators=50, learning_rate=-0.1, max_depth=3, gamma=0.1)\n\n        with self.assertRaises(ValueError):\n            XGBoostRegressor(n_estimators=50, learning_rate=0.1, max_depth=-3, gamma=0.1)\n\n    def test_invalid_fit(self):\n        # Test fitting with mismatched X_train and y_train shapes\n        xgb_model = XGBoostRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, gamma=0.1)\n        with self.assertRaises(ValueError):\n            xgb_model.fit(self.X_train, np.random.rand(50))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/bayesian/","title":"Bayesian Regression","text":"<p>This module contains an implementation of Bayesian Regression, a probabilistic approach to linear regression that provides uncertainty estimates for predictions.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/bayesian/#overview","title":"Overview","text":"<p>Bayesian Regression is an extension of traditional linear regression that models the distribution of coefficients, allowing for uncertainty in the model parameters. It's particularly useful when dealing with limited data and provides a full probability distribution over the possible values of the regression coefficients.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/bayesian/#parameters","title":"Parameters","text":"<ul> <li><code>alpha</code>: Prior precision for the coefficients.</li> <li><code>beta</code>: Precision of the noise in the observations.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/bayesian/#scratch-code","title":"Scratch Code","text":"<ul> <li>bayesian_regression.py file </li> </ul> <pre><code>import numpy as np\n\nclass BayesianRegression:\n    def __init__(self, alpha=1, beta=1):\n        \"\"\"\n        Constructor for the BayesianRegression class.\n\n        Parameters:\n        - alpha: Prior precision.\n        - beta: Noise precision.\n        \"\"\"\n        self.alpha = alpha\n        self.beta = beta\n        self.w_mean = None\n        self.w_precision = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the Bayesian Regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        # Add a bias term to X\n        X = np.c_[np.ones(X.shape[0]), X]\n\n        # Compute posterior precision and mean\n        self.w_precision = self.alpha * np.eye(X.shape[1]) + self.beta * X.T @ X\n        self.w_mean = self.beta * np.linalg.solve(self.w_precision, X.T @ y)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        # Add a bias term to X\n        X = np.c_[np.ones(X.shape[0]), X]\n\n        # Compute predicted mean\n        y_pred = X @ self.w_mean\n\n        return y_pred\n</code></pre> <ul> <li>bayesian_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom BayesianRegression import BayesianRegression\n\nclass TestBayesianRegression(unittest.TestCase):\n    def setUp(self):\n        # Generate synthetic data for testing\n        np.random.seed(42)\n        self.X_train = 2 * np.random.rand(100, 1)\n        self.y_train = 4 + 3 * self.X_train + np.random.randn(100, 1)\n\n        self.X_test = 2 * np.random.rand(20, 1)\n\n    def test_fit_predict(self):\n        blr = BayesianRegression()\n        blr.fit(self.X_train, self.y_train)\n        y_pred = blr.predict(self.X_test)\n\n        self.assertTrue(y_pred.shape == (20, 1))\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/supervised/regressions/linear/","title":"Linear Regression","text":"<p>This module contains an implementation of the Linear Regression algorithm, a fundamental technique in machine learning for predicting a continuous outcome based on input features.</p>"},{"location":"algorithms/machine-learning/supervised/regressions/linear/#parameters","title":"Parameters","text":"<ul> <li><code>learning_rate</code>: The step size for gradient descent.</li> <li><code>n_iterations</code>: The number of iterations for gradient descent.</li> </ul>"},{"location":"algorithms/machine-learning/supervised/regressions/linear/#scratch-code","title":"Scratch Code","text":"<ul> <li>linear_regression.py file </li> </ul> <pre><code>import numpy as np\n\n# Linear regression implementation\nclass LinearRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        \"\"\"\n        Constructor for the LinearRegression class.\n\n        Parameters:\n        - learning_rate: The step size for gradient descent.\n        - n_iterations: The number of iterations for gradient descent.\n        - n_iterations: n_epochs.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the linear regression model to the input data.\n\n        Parameters:\n        - X: Input features (numpy array).\n        - y: Target values (numpy array).\n        \"\"\"\n        # Initialize weights and bias\n        self.weights = np.zeros((X.shape[1], 1))\n        self.bias = 0\n\n        # Gradient Descent\n        for _ in range(self.n_iterations):\n            # Compute predictions\n            predictions = np.dot(X, self.weights) + self.bias\n\n            # Calculate errors\n            errors = predictions - y\n\n            # Update weights and bias\n            self.weights -= self.learning_rate * (1 / len(X)) * np.dot(X.T, errors)\n            self.bias -= self.learning_rate * (1 / len(X)) * np.sum(errors)\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on new data.\n\n        Parameters:\n        - X: Input features for prediction (numpy array).\n\n        Returns:\n        - Predicted values (numpy array).\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n</code></pre> <ul> <li>linear_regression_test.py file </li> </ul> <pre><code>import unittest\nimport numpy as np\nfrom LinearRegression import LinearRegression\n\nclass TestLinearRegression(unittest.TestCase):\n\n    def setUp(self):\n        # Set up some common data for testing\n        np.random.seed(42)\n        self.X_train = 2 * np.random.rand(100, 1)\n        self.y_train = 4 + 3 * self.X_train + np.random.randn(100, 1)\n\n        self.X_test = 2 * np.random.rand(20, 1)\n        self.y_test = 4 + 3 * self.X_test + np.random.randn(20, 1)\n\n    def test_fit_predict(self):\n        # Test the fit and predict methods\n\n        # Create a LinearRegression model\n        lr_model = LinearRegression()\n\n        # Fit the model to the training data\n        lr_model.fit(self.X_train, self.y_train)\n\n        # Make predictions on the test data\n        predictions = lr_model.predict(self.X_test)\n\n        # Check that the predictions are of the correct shape\n        self.assertEqual(predictions.shape, self.y_test.shape)\n\n    def test_predict_with_unfitted_model(self):\n        # Test predicting with an unfitted model\n\n        # Create a LinearRegression model (not fitted)\n        lr_model = LinearRegression()\n\n        # Attempt to make predictions without fitting the model\n        with self.assertRaises(ValueError):\n            _ = lr_model.predict(self.X_test)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"algorithms/machine-learning/unsupervised/","title":"Unsupervised Machine Learning \ud83e\udd16","text":"Clustering <p>Grouping data to uncover hidden patterns.</p> Dimensionality Reduction <p>Simplifying data while preserving its essence.</p>"},{"location":"algorithms/machine-learning/unsupervised/clustering/","title":"Clustering Algorithms \ud83e\udd16","text":"KMeans Clustering <p>Unveiling hidden patterns by grouping data into cohesive clusters.</p> <p>\ud83d\udcc5 2025-01-19 | \u23f1\ufe0f 3 mins</p>"},{"location":"algorithms/machine-learning/unsupervised/clustering/kmeans-clustering/","title":"K Means Clustering","text":"<p>Overview: K-means clustering is an unsupervised machine learning algorithm for grouping similar data points together into clusters based on their features.</p>"},{"location":"algorithms/machine-learning/unsupervised/clustering/kmeans-clustering/#advantages-of-k-means","title":"Advantages of K-means:","text":"<ul> <li>Simple and Easy to implement</li> <li>Efficiency: K-means is computationally efficient and can handle large datasets with high dimensionality.</li> <li>Flexibility: K-means offers flexibility as it can be easily customized for different applications, allowing the use of various distance metrics and     initialization techniques. </li> <li>Scalability:  K-means can handle large datasets with many data points  </li> </ul> <p>How K-means Works (Scratch Implementation Guide): </p>"},{"location":"algorithms/machine-learning/unsupervised/clustering/kmeans-clustering/#algorithm-overview","title":"Algorithm Overview:","text":"<ol> <li>Initialization:</li> <li> <p>Choose <code>k</code> initial centroids randomly from the dataset.</p> </li> <li> <p>Iterative Process:</p> </li> <li>Assign Data Points: For each data point, calculate the Euclidean distance to all centroids and assign the data point to the nearest centroid.</li> <li>Update Centroids: Recalculate the centroids by averaging the data points assigned to each cluster.</li> <li> <p>Check for Convergence: If the centroids do not change significantly between iterations (i.e., they converge), stop. Otherwise, repeat the process.</p> </li> <li> <p>Termination:</p> </li> <li> <p>The algorithm terminates either when the centroids have converged or when the maximum number of iterations is reached.</p> </li> <li> <p>Output:</p> </li> <li>The final cluster assignments for each data point. </li> </ol>"},{"location":"algorithms/machine-learning/unsupervised/clustering/kmeans-clustering/#parameters","title":"Parameters","text":"<ul> <li><code>num_clusters</code>: Number of clusters to form.</li> <li><code>max_iterations</code>: Maximum number of iterations before stopping.</li> <li><code>show_steps</code>: Whether to visualize the clustering process step by step (Boolean).</li> </ul>"},{"location":"algorithms/machine-learning/unsupervised/clustering/kmeans-clustering/#scratch-code","title":"Scratch Code","text":"<ul> <li>kmeans_scratch.py file </li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef euclidean_distance(point1, point2):\n    \"\"\"\n    Calculate the Euclidean distance between two points in space.\n    \"\"\"\n    return np.sqrt(np.sum((point1 - point2) ** 2))\n\nclass KMeansClustering:\n    def __init__(self, num_clusters=5, max_iterations=100, show_steps=False):\n        \"\"\"\n        Initialize the KMeans clustering model with the following parameters:\n        - num_clusters: Number of clusters we want to form\n        - max_iterations: Maximum number of iterations for the algorithm\n        - show_steps: Boolean flag to visualize the clustering process step by step\n        \"\"\"\n        self.num_clusters = num_clusters\n        self.max_iterations = max_iterations\n        self.show_steps = show_steps\n        self.clusters = [[] for _ in range(self.num_clusters)]  # Initialize empty clusters\n        self.centroids = []  # List to store the centroids of clusters\n\n    def fit_predict(self, data):\n        \"\"\"\n        Fit the KMeans model on the data and predict the cluster labels for each data point.\n        \"\"\"\n        self.data = data\n        self.num_samples, self.num_features = data.shape  # Get number of samples and features\n        initial_sample_indices = np.random.choice(self.num_samples, self.num_clusters, replace=False)\n        self.centroids = [self.data[idx] for idx in initial_sample_indices]\n\n        for _ in range(self.max_iterations):\n            # Step 1: Assign each data point to the closest centroid to form clusters\n            self.clusters = self._assign_to_clusters(self.centroids)\n            if self.show_steps:\n                self._plot_clusters()\n\n            # Step 2: Calculate new centroids by averaging the data points in each cluster\n            old_centroids = self.centroids\n            self.centroids = self._calculate_new_centroids(self.clusters)\n\n            # Step 3: Check for convergence \n            if self._has_converged(old_centroids, self.centroids):\n                break\n            if self.show_steps:\n                self._plot_clusters()\n\n        return self._get_cluster_labels(self.clusters)\n\n    def _assign_to_clusters(self, centroids):\n        \"\"\"\n        Assign each data point to the closest centroid based on Euclidean distance.\n        \"\"\"\n        clusters = [[] for _ in range(self.num_clusters)]\n        for sample_idx, sample in enumerate(self.data):\n            closest_centroid_idx = self._find_closest_centroid(sample, centroids)\n            clusters[closest_centroid_idx].append(sample_idx)\n        return clusters\n\n    def _find_closest_centroid(self, sample, centroids):\n        \"\"\"\n        Find the index of the closest centroid to the given data point (sample).\n        \"\"\"\n        distances = [euclidean_distance(sample, centroid) for centroid in centroids]\n        closest_idx = np.argmin(distances)  # Index of the closest centroid\n        return closest_idx\n\n    def _calculate_new_centroids(self, clusters):\n        \"\"\"\n        Calculate new centroids by averaging the data points in each cluster.\n        \"\"\"\n        centroids = np.zeros((self.num_clusters, self.num_features))\n        for cluster_idx, cluster in enumerate(clusters):\n            cluster_mean = np.mean(self.data[cluster], axis=0)\n            centroids[cluster_idx] = cluster_mean\n        return centroids\n\n    def _has_converged(self, old_centroids, new_centroids):\n        \"\"\"\n        Check if the centroids have converged \n        \"\"\"\n        distances = [euclidean_distance(old_centroids[i], new_centroids[i]) for i in range(self.num_clusters)]\n        return sum(distances) == 0  # If centroids haven't moved, they are converged\n\n    def _get_cluster_labels(self, clusters):\n        \"\"\"\n        Get the cluster labels for each data point based on the final clusters.\n        \"\"\"\n        labels = np.empty(self.num_samples)\n        for cluster_idx, cluster in enumerate(clusters):\n            for sample_idx in cluster:\n                labels[sample_idx] = cluster_idx\n        return labels\n\n    def _plot_clusters(self):\n        \"\"\"\n        Visualize the clusters and centroids in a 2D plot using matplotlib.\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n        for i, cluster in enumerate(self.clusters):\n            cluster_points = self.data[cluster]\n            ax.scatter(cluster_points[:, 0], cluster_points[:, 1])\n\n        for centroid in self.centroids:\n            ax.scatter(centroid[0], centroid[1], marker=\"x\", color=\"black\", linewidth=2)\n\n        plt.show()\n</code></pre> <ul> <li>test_kmeans.py file </li> </ul> <p>```py import unittest import numpy as np from kmeans_scratch import KMeansClustering </p> <p>class TestKMeansClustering(unittest.TestCase):</p> <pre><code>def setUp(self):\n    np.random.seed(42)\n    self.X_train = np.vstack([\n        np.random.randn(100, 2) + np.array([5, 5]),\n        np.random.randn(100, 2) + np.array([-5, -5]),\n        np.random.randn(100, 2) + np.array([5, -5]),\n        np.random.randn(100, 2) + np.array([-5, 5])\n    ])\n\ndef test_kmeans(self):\n    \"\"\"Test the basic KMeans clustering functionality\"\"\"\n    kmeans = KMeansClustering(num_clusters=4, max_iterations=100, show_steps=False)\n\n    cluster_labels = kmeans.fit_predict(self.X_train)\n\n    unique_labels = np.unique(cluster_labels)\n    self.assertEqual(len(unique_labels), 4)  \n    self.assertEqual(cluster_labels.shape, (self.X_train.shape[0],))  \n    print(\"Cluster labels for the data points:\")\n    print(cluster_labels)\n</code></pre> <p>if name == 'main':     unittest.main()</p>"},{"location":"algorithms/machine-learning/unsupervised/dimensionality-reduction/","title":"Dimensionality Reduction \ud83e\udd16","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/natural-language-processing/","title":"Natural Language Processing \ud83d\udde3\ufe0f","text":"Bag Of Words <p>Representation of text that is based on an unordered collection.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 3 mins</p> Fast Text <p>From Facebook AI Research(FAIR) for learning word embeddings and word classifications.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 7 mins</p> Gloabl Vectors <p>Unsupervised learning algorithm for obtaining vector representations for words.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p> NLP Introduction <p>Enables computers to comprehend, generate, and manipulate human language.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 3 mins</p> NLTK Setup <p>Working with human language data.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 2 mins</p> Text Pre-Processing Techniques <p>Cleaning and preparing raw text data for further analysis or model training.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p> Term Frequency-Inverse Document Frequency <p>Measure of importance of a word to a document.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 3 mins</p> Transformers <p>Deep neural network architecture.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 4 mins</p> Word2Vec <p>Creates vector representations of words.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 3 mins</p> Word Embeddings <p>Numeric representations of words in a lower-dimensional space.</p> <p>\ud83d\udcc5 2025-01-15 | \u23f1\ufe0f 3 mins</p>"},{"location":"algorithms/natural-language-processing/Bag_Of_Words/","title":"Bag Of Words","text":"<pre><code>import re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\n#data collection\ndata = [\n    'Fashion is an art form and expression.',\n    'Style is a way to say who you are without having to speak.',\n    'Fashion is what you buy, style is what you do with it.',\n    'With fashion, you convey a message about yourself without uttering a single word'\n]\n\n#text processing\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-zs]',' ',text)\n    return text\n\npreprocessed_data = [preprocess_text(doc) for doc in data]\n\nfor i, doc in enumerate(preprocessed_data, 1):\n    print(f'Data-{i} {doc}')\n\n\n\n# removing words like the, is, are, and as they usually do not carry much useful information for the analysis.\nvectorizer = CountVectorizer(stop_words='english')\nX=vectorizer.fit_transform(preprocessed_data)\nWord=vectorizer.get_feature_names_out()\n\nbow_df = pd.DataFrame(X.toarray(),columns=Word)\nbow_df.index =[f'Data {i}' for i in range(1, len(data) + 1)]\n\ntfidf_transformer = TfidfTransformer()\nX_tfidf=tfidf_transformer.fit_transform(X)\ntfidf_df=pd.DataFrame(X_tfidf.toarray(), columns=Word)\ntfidf_df.index=[f'Data {i}' for i in range(1, len(data) + 1)]\n\n\nprint()\nprint(\"--------------------------------BoW Represention----------------------------\")\nprint(bow_df)\n\nprint()\nprint(\"--------------------------------TF-IDF Value----------------------------\")\nprint(tfidf_df)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Fast_Text/","title":"Fast Text","text":""},{"location":"algorithms/natural-language-processing/Fast_Text/#introduction","title":"Introduction","text":"<p>The <code>FastText</code> class implements a word representation and classification tool developed by Facebook's AI Research (FAIR) lab. FastText extends the Word2Vec model by representing each word as a bag of character n-grams. This approach helps capture subword information and improves the handling of rare words.</p>"},{"location":"algorithms/natural-language-processing/Fast_Text/#explanation","title":"Explanation","text":""},{"location":"algorithms/natural-language-processing/Fast_Text/#initialization","title":"Initialization","text":"<ul> <li><code>vocab_size</code>: Size of the vocabulary.</li> <li><code>embedding_dim</code>: Dimension of the word embeddings.</li> <li><code>n_gram_size</code>: Size of character n-grams.</li> <li><code>learning_rate</code>: Learning rate for updating embeddings.</li> <li><code>epochs</code>: Number of training epochs.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#building-vocabulary","title":"Building Vocabulary","text":"<ul> <li><code>build_vocab()</code>: Constructs the vocabulary from the input sentences and creates a reverse mapping of words to indices.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#generating-n-grams","title":"Generating N-grams","text":"<ul> <li><code>get_ngrams()</code>: Generates character n-grams for a given word. It pads the word with <code>&lt;</code> and <code>&gt;</code> symbols to handle edge cases effectively.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#training","title":"Training","text":"<ul> <li><code>train()</code>: Updates word and context embeddings using a simple Stochastic Gradient Descent (SGD) approach. The loss is computed as the squared error between the predicted and actual values.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#prediction","title":"Prediction","text":"<ul> <li><code>predict()</code>: Calculates the dot product between the target word and context embeddings to predict word vectors.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#getting-word-vectors","title":"Getting Word Vectors","text":"<ul> <li><code>get_word_vector()</code>: Retrieves the embedding for a specific word from the trained model.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#normalization","title":"Normalization","text":"<ul> <li><code>get_embedding_matrix()</code>: Returns the normalized embedding matrix for better performance and stability.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#advantages","title":"Advantages","text":"<ul> <li>Subword Information: FastText captures morphological details by using character n-grams, improving handling of rare and out-of-vocabulary words.</li> <li>Improved Representations: The use of subwords allows for better word representations, especially for languages with rich morphology.</li> <li>Efficiency: FastText is designed to handle large-scale datasets efficiently, with optimizations for both training and inference.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#applications","title":"Applications","text":"<ul> <li>Natural Language Processing (NLP): FastText embeddings are used in tasks like text classification, sentiment analysis, and named entity recognition.</li> <li>Information Retrieval: Enhances search engines by providing more nuanced semantic matching between queries and documents.</li> <li>Machine Translation: Improves translation models by leveraging subword information for better handling of rare words and phrases.</li> </ul>"},{"location":"algorithms/natural-language-processing/Fast_Text/#implementation","title":"Implementation","text":""},{"location":"algorithms/natural-language-processing/Fast_Text/#preprocessing","title":"Preprocessing","text":"<ol> <li>Initialization: Set up parameters such as vocabulary size, embedding dimension, n-gram size, learning rate, and number of epochs.</li> </ol>"},{"location":"algorithms/natural-language-processing/Fast_Text/#building-vocabulary_1","title":"Building Vocabulary","text":"<ol> <li>Build Vocabulary: Construct the vocabulary from the input sentences and create a mapping for words.</li> </ol>"},{"location":"algorithms/natural-language-processing/Fast_Text/#generating-n-grams_1","title":"Generating N-grams","text":"<ol> <li>Generate N-grams: Create character n-grams for each word in the vocabulary, handling edge cases with padding.</li> </ol>"},{"location":"algorithms/natural-language-processing/Fast_Text/#training_1","title":"Training","text":"<ol> <li>Train the Model: Use SGD to update word and context embeddings based on the training data.</li> </ol>"},{"location":"algorithms/natural-language-processing/Fast_Text/#prediction_1","title":"Prediction","text":"<ol> <li>Predict Word Vectors: Calculate the dot product between target and context embeddings to predict word vectors.</li> </ol>"},{"location":"algorithms/natural-language-processing/Fast_Text/#getting-word-vectors_1","title":"Getting Word Vectors","text":"<ol> <li>Retrieve Word Vectors: Extract the embedding for a specific word from the trained model.</li> </ol>"},{"location":"algorithms/natural-language-processing/Fast_Text/#normalization_1","title":"Normalization","text":"<ol> <li>Normalize Embeddings: Return the normalized embedding matrix for stability and improved performance.</li> </ol> <p>For more advanced implementations, consider using optimized libraries like the FastText library by Facebook or other frameworks that offer additional features and efficiency improvements.</p>"},{"location":"algorithms/natural-language-processing/Fast_Text/#code","title":"Code","text":"<ul> <li>main.py </li> </ul> <pre><code>from fasttext import FastText\n\n# Example sentences\nsentences = [\n    \"fast text is a library for efficient text classification\",\n    \"word embeddings are useful for NLP tasks\",\n    \"fasttext models can handle out-of-vocabulary words\"\n]\n\n# Initialize and train FastText model\nfasttext_model = FastText(vocab_size=100, embedding_dim=50)\nfasttext_model.build_vocab(sentences)\nfasttext_model.train(sentences)\n\n# Get the vector for a word\nvector = fasttext_model.get_word_vector(\"fast\")\nprint(f\"Vector for 'fast': {vector}\")\n</code></pre> <ul> <li>fast_test.py</li> </ul> <pre><code>import numpy as np\nfrom collections import defaultdict\nfrom sklearn.preprocessing import normalize\n\nclass FastText:\n    def __init__(self, vocab_size, embedding_dim, n_gram_size=3, learning_rate=0.01, epochs=10):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.n_gram_size = n_gram_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.word_embeddings = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n        self.context_embeddings = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n        self.vocab = {}\n        self.rev_vocab = {}\n\n    def build_vocab(self, sentences):\n        \"\"\"\n        Build vocabulary from sentences.\n\n        Args:\n        sentences (list): List of sentences (strings).\n        \"\"\"\n        word_count = defaultdict(int)\n        for sentence in sentences:\n            words = sentence.split()\n            for word in words:\n                word_count[word] += 1\n        self.vocab = {word: idx for idx, (word, _) in enumerate(word_count.items())}\n        self.rev_vocab = {idx: word for word, idx in self.vocab.items()}\n\n    def get_ngrams(self, word):\n        \"\"\"\n        Get n-grams for a given word.\n\n        Args:\n        word (str): Input word.\n\n        Returns:\n        set: Set of n-grams.\n        \"\"\"\n        ngrams = set()\n        word = '&lt;' * (self.n_gram_size - 1) + word + '&gt;' * (self.n_gram_size - 1)\n        for i in range(len(word) - self.n_gram_size + 1):\n            ngrams.add(word[i:i + self.n_gram_size])\n        return ngrams\n\n    def train(self, sentences):\n        \"\"\"\n        Train the FastText model using the given sentences.\n\n        Args:\n        sentences (list): List of sentences (strings).\n        \"\"\"\n        for epoch in range(self.epochs):\n            loss = 0\n            for sentence in sentences:\n                words = sentence.split()\n                for i, word in enumerate(words):\n                    if word not in self.vocab:\n                        continue\n                    word_idx = self.vocab[word]\n                    target_ngrams = self.get_ngrams(word)\n                    for j in range(max(0, i - 1), min(len(words), i + 2)):\n                        if i != j and words[j] in self.vocab:\n                            context_idx = self.vocab[words[j]]\n                            prediction = self.predict(word_idx, context_idx)\n                            error = prediction - 1 if j == i + 1 else prediction\n                            loss += error**2\n                            self.word_embeddings[word_idx] -= self.learning_rate * error * self.context_embeddings[context_idx]\n                            self.context_embeddings[context_idx] -= self.learning_rate * error * self.word_embeddings[word_idx]\n            print(f'Epoch {epoch + 1}/{self.epochs}, Loss: {loss}')\n\n    def predict(self, word_idx, context_idx):\n        \"\"\"\n        Predict the dot product of the word and context embeddings.\n\n        Args:\n        word_idx (int): Index of the word.\n        context_idx (int): Index of the context word.\n\n        Returns:\n        float: Dot product.\n        \"\"\"\n        return np.dot(self.word_embeddings[word_idx], self.context_embeddings[context_idx])\n\n    def get_word_vector(self, word):\n        \"\"\"\n        Get the word vector for the specified word.\n\n        Args:\n        word (str): Input word.\n\n        Returns:\n        np.ndarray: Word vector.\n        \"\"\"\n        if word in self.vocab:\n            return self.word_embeddings[self.vocab[word]]\n        else:\n            raise ValueError(f\"Word '{word}' not found in vocabulary\")\n\n    def get_embedding_matrix(self):\n        \"\"\"\n        Get the normalized embedding matrix.\n\n        Returns:\n        np.ndarray: Normalized word embeddings.\n        \"\"\"\n        return normalize(self.word_embeddings, axis=1)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Fast_Text/#references","title":"References","text":"<ol> <li>FastText - Facebook AI Research</li> <li>Understanding FastText</li> <li>FastText on GitHub</li> </ol>"},{"location":"algorithms/natural-language-processing/GloVe/","title":"GloVe","text":""},{"location":"algorithms/natural-language-processing/GloVe/#introduction","title":"Introduction","text":"<p>The <code>GloVe</code> class implements the Global Vectors for Word Representation algorithm, developed by Stanford researchers. GloVe generates dense vector representations of words, capturing semantic relationships between them. Unlike traditional one-hot encoding, GloVe produces low-dimensional, continuous vectors that convey meaningful information about words and their contexts.</p>"},{"location":"algorithms/natural-language-processing/GloVe/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>Co-occurrence Matrix: GloVe starts by creating a co-occurrence matrix from a large corpus of text. This matrix counts how often words appear together within a given context window, capturing the frequency of word pairs.</p> </li> <li> <p>Weighted Least Squares: The main idea behind GloVe is to factorize this co-occurrence matrix to find word vectors that capture the relationships between words. It aims to represent words that frequently appear together in similar contexts with similar vectors.</p> </li> <li> <p>Weighting Function: To ensure that the optimization process doesn't get overwhelmed by very frequent co-occurrences, GloVe uses a weighting function. This function reduces the influence of extremely common word pairs.</p> </li> <li> <p>Training Objective: The goal is to adjust the word vectors so that their dot products align with the observed co-occurrence counts. This helps in capturing the similarity between words based on their contexts.</p> </li> </ul>"},{"location":"algorithms/natural-language-processing/GloVe/#glove-training-objective","title":"GloVe Training Objective","text":"<p>GloVe\u2019s training involves adjusting word vectors so that their interactions match the observed co-occurrence data. It focuses on ensuring that words appearing together often have similar vector representations.</p>"},{"location":"algorithms/natural-language-processing/GloVe/#advantages","title":"Advantages","text":"<ul> <li>Efficient Training: By using a global co-occurrence matrix, GloVe captures semantic relationships effectively, including long-range dependencies.</li> <li>Meaningful Vectors: The resulting vectors can represent complex relationships between words, such as analogies (e.g., \"king\" - \"man\" + \"woman\" \u2248 \"queen\").</li> <li>Flexibility: GloVe vectors are versatile and can be used in various NLP tasks, including sentiment analysis and machine translation.</li> </ul>"},{"location":"algorithms/natural-language-processing/GloVe/#applications","title":"Applications","text":"<ul> <li>Natural Language Processing (NLP): GloVe vectors are used as features in NLP tasks like sentiment analysis, named entity recognition, and question answering.</li> <li>Information Retrieval: Enhance search engines by providing better semantic matching between queries and documents.</li> <li>Machine Translation: Improve translation models by capturing semantic similarities between words in different languages.</li> </ul>"},{"location":"algorithms/natural-language-processing/GloVe/#implementation","title":"Implementation","text":""},{"location":"algorithms/natural-language-processing/GloVe/#preprocessing","title":"Preprocessing","text":"<ol> <li>Clean and Tokenize: Prepare the text data by cleaning and tokenizing it into words.</li> </ol>"},{"location":"algorithms/natural-language-processing/GloVe/#building-vocabulary","title":"Building Vocabulary","text":"<ol> <li>Create Vocabulary: Construct a vocabulary and map words to unique indices.</li> </ol>"},{"location":"algorithms/natural-language-processing/GloVe/#co-occurrence-matrix","title":"Co-occurrence Matrix","text":"<ol> <li>Build Co-occurrence Matrix: Create a matrix that captures how often each word pair appears together within a specified context.</li> </ol>"},{"location":"algorithms/natural-language-processing/GloVe/#glove-model","title":"GloVe Model","text":"<ol> <li>Initialization: Set up the model parameters and hyperparameters.</li> <li>Weighting Function: Define how to balance the importance of different co-occurrence counts.</li> <li>Training: Use optimization techniques to adjust the word vectors based on the co-occurrence data.</li> <li>Get Word Vector: Extract the vector representation for each word from the trained model.</li> </ol> <p>For more advanced implementations, consider using libraries like TensorFlow or PyTorch, which offer enhanced functionalities and optimizations.</p>"},{"location":"algorithms/natural-language-processing/GloVe/#code","title":"Code","text":"<ul> <li>main.py file </li> </ul> <pre><code>import numpy as np\nfrom preprocess import preprocess\nfrom vocab_and_matrix import build_vocab, build_cooccurrence_matrix\nfrom glove_model import GloVe\n\n# Example text corpus\ncorpus = [\"I love NLP\", \"NLP is a fascinating field\", \"Natural language processing with GloVe\"]\n\n# Preprocess the corpus\ntokens = [token for sentence in corpus for token in preprocess(sentence)]\n\n# Build vocabulary and co-occurrence matrix\nword_to_index = build_vocab(tokens)\ncooccurrence_matrix = build_cooccurrence_matrix(tokens, word_to_index)\n\n# Initialize and train the GloVe model\nglove = GloVe(vocab_size=len(word_to_index), embedding_dim=50)\nglove.train(cooccurrence_matrix, epochs=100)\n\n# Get the word vector for 'nlp'\nword_vector = glove.get_word_vector('nlp', word_to_index)\nprint(word_vector)\n</code></pre> <ul> <li>glove_model.py file </li> </ul> <pre><code>import numpy as np\n\nclass GloVe:\n    def __init__(self, vocab_size, embedding_dim=50, x_max=100, alpha=0.75):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.x_max = x_max\n        self.alpha = alpha\n        self.W = np.random.rand(vocab_size, embedding_dim)\n        self.W_tilde = np.random.rand(vocab_size, embedding_dim)\n        self.b = np.random.rand(vocab_size)\n        self.b_tilde = np.random.rand(vocab_size)\n        self.gradsq_W = np.ones((vocab_size, embedding_dim))\n        self.gradsq_W_tilde = np.ones((vocab_size, embedding_dim))\n        self.gradsq_b = np.ones(vocab_size)\n        self.gradsq_b_tilde = np.ones(vocab_size)\n\n    def weighting_function(self, x):\n        if x &lt; self.x_max:\n            return (x / self.x_max) ** self.alpha\n        return 1.0\n\n    def train(self, cooccurrence_matrix, epochs=100, learning_rate=0.05):\n        for epoch in range(epochs):\n            total_cost = 0\n            for i in range(self.vocab_size):\n                for j in range(self.vocab_size):\n                    if cooccurrence_matrix[i, j] == 0:\n                        continue\n                    X_ij = cooccurrence_matrix[i, j]\n                    weight = self.weighting_function(X_ij)\n                    cost = weight * (np.dot(self.W[i], self.W_tilde[j]) + self.b[i] + self.b_tilde[j] - np.log(X_ij)) ** 2\n                    total_cost += cost\n\n                    grad_common = weight * (np.dot(self.W[i], self.W_tilde[j]) + self.b[i] + self.b_tilde[j] - np.log(X_ij))\n                    grad_W = grad_common * self.W_tilde[j]\n                    grad_W_tilde = grad_common * self.W[i]\n                    grad_b = grad_common\n                    grad_b_tilde = grad_common\n\n                    self.W[i] -= learning_rate * grad_W / np.sqrt(self.gradsq_W[i])\n                    self.W_tilde[j] -= learning_rate * grad_W_tilde / np.sqrt(self.gradsq_W_tilde[j])\n                    self.b[i] -= learning_rate * grad_b / np.sqrt(self.gradsq_b[i])\n                    self.b_tilde[j] -= learning_rate * grad_b_tilde / np.sqrt(self.gradsq_b_tilde[j])\n\n                    self.gradsq_W[i] += grad_W ** 2\n                    self.gradsq_W_tilde[j] += grad_W_tilde ** 2\n                    self.gradsq_b[i] += grad_b ** 2\n                    self.gradsq_b_tilde[j] += grad_b_tilde ** 2\n\n            if epoch % 10 == 0:\n                print(f'Epoch: {epoch}, Cost: {total_cost}')\n\n    def get_word_vector(self, word, word_to_index):\n        if word in word_to_index:\n            word_index = word_to_index[word]\n            return self.W[word_index]\n        return None\n</code></pre> <ul> <li>preprocess.py file </li> </ul> <pre><code>import string\n\ndef preprocess(text):\n    \"\"\"\n    Preprocess the text by removing punctuation, converting to lowercase, and splitting into words.\n\n    Args:\n    text (str): Input text string.\n\n    Returns:\n    list: List of words (tokens).\n    \"\"\"\n    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n    tokens = text.split()\n    return tokens\n</code></pre> <ul> <li>vocab_and_matrix.py file </li> </ul> <pre><code>import numpy as np\nfrom collections import Counter\n\ndef build_vocab(tokens):\n    \"\"\"\n    Build vocabulary from tokens and create a word-to-index mapping.\n\n    Args:\n    tokens (list): List of words (tokens).\n\n    Returns:\n    dict: Word-to-index mapping.\n    \"\"\"\n    vocab = Counter(tokens)\n    word_to_index = {word: i for i, word in enumerate(vocab)}\n    return word_to_index\n\ndef build_cooccurrence_matrix(tokens, word_to_index, window_size=2):\n    \"\"\"\n    Build the co-occurrence matrix from tokens using a specified window size.\n\n    Args:\n    tokens (list): List of words (tokens).\n    word_to_index (dict): Word-to-index mapping.\n    window_size (int): Context window size.\n\n    Returns:\n    np.ndarray: Co-occurrence matrix.\n    \"\"\"\n    vocab_size = len(word_to_index)\n    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n\n    for i, word in enumerate(tokens):\n        word_index = word_to_index[word]\n        context_start = max(0, i - window_size)\n        context_end = min(len(tokens), i + window_size + 1)\n\n        for j in range(context_start, context_end):\n            if i != j:\n                context_word = tokens[j]\n                context_word_index = word_to_index[context_word]\n                cooccurrence_matrix[word_index, context_word_index] += 1\n\n    return cooccurrence_matrix\n</code></pre>"},{"location":"algorithms/natural-language-processing/GloVe/#references","title":"References","text":"<ol> <li>GloVe - Stanford NLP</li> <li>Understanding GloVe</li> <li>GloVe: Global Vectors for Word Representation - Wikipedia</li> </ol>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/","title":"NLTK Setup","text":"<p>Hello there! \ud83c\udf1f Welcome to your first step into the fascinating world of Natural Language Processing (NLP) with the Natural Language Toolkit (NLTK). This guide is designed to be super beginner-friendly. We\u2019ll cover everything from installation to basic operations with lots of explanations along the way. Let's get started!</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#what-is-nltk","title":"What is NLTK?","text":"<p>The Natural Language Toolkit (NLTK) is a comprehensive Python library for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. It also includes wrappers for industrial-strength NLP libraries.</p> <p>Key Features of NLTK:</p> <p>1.Corpora and Lexical Resources: NLTK includes access to a variety of text corpora and lexical resources, such as WordNet, the Brown Corpus, the Gutenberg Corpus, and many more.</p> <p>2.Text Processing Libraries: It provides tools for a wide range of text processing tasks:</p> <p>Tokenization (splitting text into words, sentences, etc.)</p> <p>Part-of-Speech (POS) tagging</p> <p>Named Entity Recognition (NER)</p> <p>Stemming and Lemmatization</p> <p>Parsing (syntax analysis)</p> <p>Semantic reasoning</p> <p>3.Classification and Machine Learning: NLTK includes various classifiers and machine learning algorithms that can be used for text classification tasks.</p> <p>4.Visualization and Demonstrations: It offers visualization tools for trees, graphs, and other linguistic structures. It also includes a number of interactive demonstrations and sample data.</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#installation","title":"Installation","text":"<p>First, we need to install NLTK. Make sure you have Python installed on your system. If not, you can download it from python.org. Once you have Python, open your command prompt (or terminal) and type the following command: <pre><code>pip install nltk\n</code></pre> To verify that NLTK is installed correctly, open a Python shell and import the library:</p> <p>import nltk If no errors occur, NLTK is successfully installed. NLTK requires additional data packages for various functionalities. To download all the data packages, open a python shell and run : <pre><code>import nltk\nnltk.download ('all')\n</code></pre> Alternatively you can download specific data packages using :</p> <p>nltk.download ('punkt')  # Tokenizer for splitting sentences into words nltk.download ('averaged_perceptron_tagger')  # Part-of-speech tagger for tagging words with their parts of speech nltk.download ('maxent_ne_chunker')  # Named entity chunker for recognizing named entities in text nltk.download ('words')  # Corpus of English words required for many NLTK functions Now that we have everything set up, let\u2019s dive into some basic NLP operations with NLTK.</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#tokenization","title":"Tokenization","text":"<p>Tokenization is the process of breaking down text into smaller pieces, like words or sentences. It's like cutting a big cake into smaller slices. <pre><code>from nltk.tokenize import word_tokenize, sent_tokenize\n\n#Sample text to work with\ntext = \"Natural Language Processing with NLTK is fun and educational.\"\n\n#Tokenize into words\nwords = word_tokenize(text)\nprint(\"Word Tokenization:\", words)\n\n#Tokenize into sentences\nsentences = sent_tokenize(text)\nprint(\"Sentence Tokenization:\", sentences)\n</code></pre> Word Tokenization: ['Natural', 'Language', 'Processing', 'with', 'NLTK', 'is', 'fun', 'and', 'educational', '.']</p> <p>Sentence Tokenization: ['Natural Language Processing with NLTK is fun and educational.']</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#stopwords-removal","title":"Stopwords Removal","text":"<p>Stopwords are common words that don\u2019t carry much meaning on their own. In many NLP tasks, we remove these words to focus on the important ones. <pre><code>from nltk.corpus import stopwords\n\n# Get the list of stopwords in English\nstop_words = set(stopwords.words('english'))\n\n# Remove stopwords from our list of words\nfiltered_words = [word for word in words if word.lower() not in stop_words]\n\nprint(\"Filtered Words:\", filtered_words)\n</code></pre> Filtered Words: ['Natural', 'Language', 'Processing', 'NLTK', 'fun', 'educational', '.']</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#explanation","title":"Explanation:","text":"<p>stopwords.words('english'): This gives us a list of common English stopwords.</p> <p>[word for word in words if word.lower() not in stop_words]: This is a list comprehension that filters out the stopwords from our list of words.</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#stemming","title":"Stemming","text":"<p>Stemming is the process of reducing words to their root form. It\u2019s like finding the 'stem' of a word. <pre><code>from nltk.stem import PorterStemmer\n\n# Create a PorterStemmer object\nps = PorterStemmer()\n\n# Stem each word in our list of words\nstemmed_words = [ps.stem(word) for word in words]\n\nprint(\"Stemmed Words:\", stemmed_words)\n</code></pre> Stemmed Words: ['natur', 'languag', 'process', 'with', 'nltk', 'is', 'fun', 'and', 'educ', '.']</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#explanation_1","title":"Explanation:","text":"<p>PorterStemmer(): This creates a PorterStemmer object, which is a popular stemming algorithm.</p> <p>[ps.stem(word) for word in words]: This applies the stemming algorithm to each word in our list.</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#lemmatization","title":"Lemmatization","text":"<p>Lemmatization is similar to stemming but it uses a dictionary to find the base form of a word. It\u2019s more accurate than stemming. <pre><code>from nltk.stem import WordNetLemmatizer\n\n# Create a WordNetLemmatizer object\nlemmatizer = WordNetLemmatizer()\n\n# Lemmatize each word in our list of words\nlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n\nprint(\"Lemmatized Words:\", lemmatized_words)\n</code></pre> Lemmatized Words: ['Natural', 'Language', 'Processing', 'with', 'NLTK', 'is', 'fun', 'and', 'educational', '.']</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#explanation_2","title":"Explanation:","text":"<p>WordNetLemmatizer(): This creates a lemmatizer object.</p> <p>[lemmatizer.lemmatize(word) for word in words]: This applies the lemmatization process to each word in our list.</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#part-of-speech-tagging","title":"Part-of-speech tagging","text":"<p>Part-of-speech tagging is the process of labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc. NLTK provides functionality to perform POS tagging easily. <pre><code># Import the word_tokenize function from nltk.tokenize module\n# Import the pos_tag function from nltk module\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\n# Sample text to work with\ntext = \"NLTK is a powerful tool for natural language processing.\"\n\n# Tokenize the text into individual words\n# The word_tokenize function splits the text into a list of words\nwords = word_tokenize(text)\n\n# Perform Part-of-Speech (POS) tagging\n# The pos_tag function takes a list of words and assigns a part-of-speech tag to each word\npos_tags = pos_tag(words)\n\n# Print the part-of-speech tags\nprint(\"Part-of-speech tags:\")\nprint(pos_tags)\n</code></pre> Part-of-speech tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#explanation_3","title":"Explanation:","text":"<p>pos_tags = pos_tag(words): The pos_tag function takes the list of words and assigns a part-of-speech tag to each word. For example, it might tag 'NLTK' as a proper noun (NNP), 'is' as a verb (VBZ), and so on.</p> <p>Here is a list of common POS tags used in the Penn Treebank tag set, along with explanations and examples:</p>"},{"location":"algorithms/natural-language-processing/NLTK_Setup/#common-pos-tags","title":"Common POS Tags:","text":"<p>CC: Coordinating conjunction (e.g., and, but, or)</p> <p>CD: Cardinal number (e.g., one, two)</p> <p>DT: Determiner (e.g., the, a, an)</p> <p>EX: Existential there (e.g., there is)</p> <p>FW: Foreign word (e.g., en route)</p> <p>IN: Preposition or subordinating conjunction (e.g., in, of, like)</p> <p>JJ: Adjective (e.g., big, blue, fast)</p> <p>JJR: Adjective, comparative (e.g., bigger, faster)</p> <p>JJS: Adjective, superlative (e.g., biggest, fastest)</p> <p>LS: List item marker (e.g., 1, 2, One)</p> <p>MD: Modal (e.g., can, will, must)</p> <p>NN: Noun, singular or mass (e.g., dog, city, music)</p> <p>NNS: Noun, plural (e.g., dogs, cities)</p> <p>NNP: Proper noun, singular (e.g., John, London)</p> <p>NNPS: Proper noun, plural (e.g., Americans, Sundays)</p> <p>PDT: Predeterminer (e.g., all, both, half)</p> <p>POS: Possessive ending (e.g., 's, s')</p> <p>PRP: Personal pronoun (e.g., I, you, he)</p> <p>PRP$: Possessive pronoun (e.g., my, your, his)</p> <p>RB: Adverb (e.g., quickly, softly)</p> <p>RBR: Adverb, comparative (e.g., faster, harder)</p> <p>RBS: Adverb, superlative (e.g., fastest, hardest)</p> <p>RP: Particle (e.g., up, off)</p> <p>SYM: Symbol (e.g., $, %, &amp;)</p> <p>TO: to (e.g., to go, to read)</p> <p>UH: Interjection (e.g., uh, well, wow)</p> <p>VB: Verb, base form (e.g., run, eat)</p> <p>VBD: Verb, past tense (e.g., ran, ate)</p> <p>VBG: Verb, gerund or present participle (e.g., running, eating)</p> <p>VBN: Verb, past participle (e.g., run, eaten)</p> <p>VBP: Verb, non-3rd person singular present (e.g., run, eat)</p> <p>VBZ: Verb, 3rd person singular present (e.g., runs, eats)</p> <p>WDT: Wh-determiner (e.g., which, that)</p> <p>WP: Wh-pronoun (e.g., who, what)</p> <p>WP$: Possessive wh-pronoun (e.g., whose)</p> <p>WRB: Wh-adverb (e.g., where, when)</p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/","title":"NLP Introduction","text":""},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#what-is-nlp","title":"What is NLP? \ud83e\udd16","text":"<p>Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and respond to human language. It bridges the gap between human communication and machine understanding, making it possible for computers to interact with us in a natural and intuitive way.</p> <p></p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#importance-of-nlp","title":"Importance of NLP \ud83c\udf10","text":"<p>NLP is essential for various applications that we use daily, such as:</p> <ul> <li>Voice Assistants (e.g., Siri, Alexa) \ud83d\udde3\ufe0f</li> <li>Chatbots for customer service \ud83d\udcac</li> <li>Language Translation services (e.g., Google Translate) \ud83c\udf0d</li> <li>Sentiment Analysis in social media monitoring \ud83d\udcca</li> <li>Text Summarization for news articles \ud83d\udcf0</li> </ul>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#history-and-evolution-of-nlp","title":"History and Evolution of NLP \ud83d\udcdc","text":"<ol> <li>1950s: Alan Turing's seminal paper \"Computing Machinery and Intelligence\" proposes the Turing Test.</li> <li>1960s-1970s: Development of early NLP systems like ELIZA and SHRDLU.</li> <li>1980s: Introduction of machine learning algorithms and statistical models.</li> <li>1990s: Emergence of more sophisticated algorithms and large annotated datasets.</li> <li>2000s: Advent of deep learning, leading to significant breakthroughs in NLP.</li> <li>2010s-Present: Development of powerful models like BERT and GPT, revolutionizing the field.</li> </ol>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#key-concepts-and-terminology","title":"Key Concepts and Terminology \ud83d\udcda","text":"<ul> <li>Tokens: The smallest units of text, such as words or punctuation marks. Example: \"Hello, world!\" becomes [\"Hello\", \",\", \"world\", \"!\"].</li> <li>Corpus: A large collection of text used for training NLP models. Example: The Wikipedia Corpus.</li> <li>Stopwords: Commonly used words (e.g., \"the\", \"is\", \"in\") that are often removed from text during preprocessing.</li> <li>Stemming: Reducing words to their base or root form. Example: \"running\" becomes \"run\".</li> <li>Lemmatization: Similar to stemming, but it reduces words to their dictionary form. Example: \"better\" becomes \"good\".</li> </ul>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#real-world-use-cases","title":"Real-World Use Cases \ud83c\udf1f","text":""},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#voice-assistants","title":"Voice Assistants \ud83d\udde3\ufe0f","text":"<p>Voice assistants like Siri and Alexa use NLP to understand and respond to user commands. For example, when you ask, \"What's the weather today?\", NLP helps the assistant interpret your query and provide the relevant weather information.</p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#customer-service-chatbots","title":"Customer Service Chatbots \ud83d\udcac","text":"<p>Many companies use chatbots to handle customer inquiries. NLP enables these bots to understand customer questions and provide accurate responses, improving customer satisfaction and reducing response time.</p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#language-translation","title":"Language Translation \ud83c\udf0d","text":"<p>NLP powers translation services like Google Translate, which can translate text from one language to another. This helps break down language barriers and facilitates global communication.</p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#sentiment-analysis","title":"Sentiment Analysis \ud83d\udcca","text":"<p>Businesses use sentiment analysis to monitor social media and understand public opinion about their products or services. NLP analyzes text to determine whether the sentiment expressed is positive, negative, or neutral.</p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#text-summarization","title":"Text Summarization \ud83d\udcf0","text":"<p>NLP algorithms can summarize long articles into concise summaries, making it easier for readers to grasp the main points quickly. This is particularly useful for news articles and research papers.</p>"},{"location":"algorithms/natural-language-processing/N_L_P_Introduction/#conclusion","title":"Conclusion \ud83c\udf1f","text":"<p>NLP is a dynamic and rapidly evolving field that plays a crucial role in how we interact with technology. By understanding its basics and key concepts, you can start exploring the fascinating world of language and machines.</p> <p>This README provides a brief introduction to NLP. For a deeper dive, explore more resources and start building your own NLP projects!</p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/","title":"Text Preprocessing Techniques","text":""},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#welcome-to-this-comprehensive-guide-on-text-preprocessing-with-nltk-natural-language-toolkit-this-notebook-will-walk-you-through-various-essential-text-preprocessing-techniques-all-explained-in-simple-terms-with-easy-to-follow-code-examples-whether-youre-just-starting-out-in-nlp-natural-language-processing-or-looking-to-brush-up-on-your-skills-youre-in-the-right-place","title":"Welcome to this comprehensive guide on text preprocessing with NLTK (Natural Language Toolkit)! This notebook will walk you through various essential text preprocessing techniques, all explained in simple terms with easy-to-follow code examples. Whether you're just starting out in NLP (Natural Language Processing) or looking to brush up on your skills, you're in the right place! \ud83d\ude80","text":""},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#nltk-provides-a-comprehensive-suite-of-tools-for-processing-and-analyzing-unstructured-text-data","title":"NLTK provides a comprehensive suite of tools for processing and analyzing unstructured text data.","text":""},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#1-tokenization","title":"1. Tokenization","text":"<p>Tokenization is the process of splitting text into individual words or sentences.</p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#sentence-tokenization","title":"Sentence Tokenization","text":"<pre><code>import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\ntext = \"Hello World. This is NLTK. It is great for text processing.\"\nsentences = sent_tokenize(text)\nprint(sentences)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#word-tokenization","title":"Word Tokenization","text":"<pre><code>from nltk.tokenize import word_tokenize\n\nwords = word_tokenize(text)\nprint(words)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#2-removing-stop-words","title":"2. Removing Stop Words","text":"<p>Stop words are common words that may not be useful for text analysis (e.g., \"is\", \"the\", \"and\").</p> <pre><code>from nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in words if word.lower() not in stop_words]\nprint(filtered_words)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#3-stemming","title":"3. Stemming","text":"<p>Stemming reduces words to their root form by chopping off the ends. <pre><code>from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nstemmed_words = [stemmer.stem(word) for word in filtered_words]\nprint(stemmed_words)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#4-lemmatization","title":"4. Lemmatization","text":"<p>Lemmatization reduces words to their base form (lemma), taking into account the meaning of the word. <pre><code>from nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nlemmatizer = WordNetLemmatizer()\nlemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\nprint(lemmatized_words)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#5-part-of-speech-tagging","title":"5. Part-of-Speech Tagging","text":"<p>Tagging words with their parts of speech (POS) helps understand the grammatical structure.</p> <p>The complete POS tag list can be accessed from the Installation and set-up notebook. <pre><code>nltk.download('averaged_perceptron_tagger')\n\npos_tags = nltk.pos_tag(lemmatized_words)\nprint(pos_tags)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#6-named-entity-recognition","title":"6. Named Entity Recognition","text":"<p>Identify named entities such as names of people, organizations, locations, etc.</p> <pre><code># Numpy is required to run this\n%pip install numpy\n\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nfrom nltk.chunk import ne_chunk\n\nnamed_entities = ne_chunk(pos_tags)\nprint(named_entities)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#7-word-frequency-distribution","title":"7. Word Frequency Distribution","text":"<p>Count the frequency of each word in the text.</p> <pre><code>from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(lemmatized_words)\nprint(freq_dist.most_common(5))\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#8-removing-punctuation","title":"8. Removing Punctuation","text":"<p>Remove punctuation from the text.</p> <pre><code>import string\n\nno_punct = [word for word in lemmatized_words if word not in string.punctuation]\nprint(no_punct)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#9-lowercasing","title":"9. Lowercasing","text":"<p>Convert all words to lowercase.</p> <pre><code>lowercased = [word.lower() for word in no_punct]\nprint(lowercased)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#10-spelling-correction","title":"10. Spelling Correction","text":"<p>Correct the spelling of words.</p> <pre><code>%pip install pyspellchecker\n\nfrom nltk.corpus import wordnet\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\n\ndef correct_spelling(word):\n    if not wordnet.synsets(word):\n        return spell.correction(word)\n    return word\n\nlemmatized_words = ['hello', 'world', '.', 'klown', 'taxt', 'procass', '.']\nwords_with_corrected_spelling = [correct_spelling(word) for word in lemmatized_words]\nprint(words_with_corrected_spelling)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#11-removing-numbers","title":"11. Removing Numbers","text":"<p>Remove numerical values from the text.</p> <pre><code>lemmatized_words = ['hello', 'world', '88', 'text', 'process', '.']\n\nno_numbers = [word for word in lemmatized_words if not word.isdigit()]\nprint(no_numbers)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#12-word-replacement","title":"12. Word Replacement","text":"<p>Replace specific words with other words (e.g., replacing slang with formal words).</p> <pre><code>lemmatized_words = ['hello', 'world', 'gr8', 'text', 'NLTK', '.']\nreplacements = {'NLTK': 'Natural Language Toolkit', 'gr8' : 'great'}\n\nreplaced_words = [replacements.get(word, word) for word in lemmatized_words]\nprint(replaced_words)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#13-synonym-replacement","title":"13. Synonym Replacement","text":"<p>Replace words with their synonyms.</p> <pre><code>from nltk.corpus import wordnet\nlemmatized_words = ['hello', 'world', 'awesome', 'text', 'great', '.']\n\ndef get_synonym(word):\n    synonyms = wordnet.synsets(word)\n    if synonyms:\n        return synonyms[0].lemmas()[0].name()\n    return word\n\nsynonym_replaced = [get_synonym(word) for word in lemmatized_words]\nprint(synonym_replaced)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#14-extracting-bigrams-and-trigrams","title":"14. Extracting Bigrams and Trigrams","text":"<p>Extract bigrams (pairs of consecutive words) and trigrams (triplets of consecutive words).</p> <pre><code>from nltk import bigrams\n\nbigrams_list = list(bigrams(lemmatized_words))\nprint(bigrams_list)\n\nfrom nltk import trigrams\n\ntrigrams_list = list(trigrams(lemmatized_words))\nprint(trigrams_list)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#15-sentence-segmentation","title":"15. Sentence Segmentation","text":"<p>Split text into sentences while considering abbreviations and other punctuation complexities.</p> <pre><code>import nltk.data\n\ntext = 'Hello World. This is NLTK. It is great for text preprocessing.'\n\n# Load the sentence tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Tokenize the text into sentences\nsentences = tokenizer.tokenize(text)\n\n# Print the tokenized sentences\nprint(sentences)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#16-identifying-word-frequencies","title":"16. Identifying Word Frequencies","text":"<p>Identify and display the frequency of words in a text.</p> <pre><code>from nltk.probability import FreqDist\n\nlemmatized_words = ['hello', 'hello', 'awesome', 'text', 'great', '.', '.', '.']\n\n\nword_freq = FreqDist(lemmatized_words)\nfor word, freq in word_freq.items():\n    print(f\"{word}: {freq}\")\n</code></pre>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#17-removing-html-tags","title":"17. Removing HTML tags","text":"<p>Remove HTML tags from the text. <pre><code>%pip install bs4\n\nfrom bs4 import BeautifulSoup\n\nhtml_text = \"&lt;p&gt;Hello World. This is NLTK.&lt;/p&gt;\"\nsoup = BeautifulSoup(html_text, \"html.parser\")\ncleaned_text = soup.get_text()\nprint(cleaned_text)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#18-detecting-language","title":"18. Detecting Language","text":"<p>Detect the language of the text. <pre><code>%pip install langdetect\n\nfrom langdetect import detect\n\nlanguage = detect(text)\nprint(language) #`en` (for English)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#19-tokenizing-by-regular-expressions","title":"19. Tokenizing by Regular Expressions","text":"<p>Use Regular Expressions to tokenize text. <pre><code>text = 'Hello World. This is NLTK. It is great for text preprocessing.'\n\nfrom nltk.tokenize import regexp_tokenize\n\npattern = r'\\w+'\nregex_tokens = regexp_tokenize(text, pattern)\nprint(regex_tokens)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#20-remove-frequent-words","title":"20. Remove Frequent Words","text":"<p>Removes frequent words (also known as \u201chigh-frequency words\u201d) from a list of tokens using NLTK, you can use the nltk.FreqDist() function to calculate the frequency of each word and filter out the most common ones. <pre><code>import nltk\n\n# input text\ntext = \"Natural language processing is a field of AI. I love AI.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# calculate the frequency of each word\nfdist = nltk.FreqDist(tokens)\n\n# remove the most common words (e.g., the top 10% of words by frequency)\nfiltered_tokens = [token for token in tokens if fdist[token] &lt; fdist.N() * 0.1]\n\nprint(\"Tokens without frequent words:\", filtered_tokens)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#21-remove-extra-whitespace","title":"21. Remove extra whitespace","text":"<p>Tokenizes the input string into individual sentences and remove any leading or trailing whitespace from each sentence. <pre><code>import nltk.data\n\n# Text data\ntext = 'Hello World. This is NLTK. It is great for text preprocessing.'\n\n# Load the sentence tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Tokenize the text into sentences\nsentences = tokenizer.tokenize(text)\n\n# Remove extra whitespace from each sentence\nsentences = [sentence.strip() for sentence in sentences]\n\n# Print the tokenized sentences\nprint(sentences)\n</code></pre></p>"},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#conclusion","title":"Conclusion \ud83c\udf89","text":""},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#text-preprocessing-is-a-crucial-step-in-natural-language-processing-nlp-and-can-significantly-impact-the-performance-of-your-models-and-applications-with-nltk-we-have-a-powerful-toolset-that-simplifies-and-streamlines-these-tasks","title":"Text preprocessing is a crucial step in natural language processing (NLP) and can significantly impact the performance of your models and applications. With NLTK, we have a powerful toolset that simplifies and streamlines these tasks.","text":""},{"location":"algorithms/natural-language-processing/Text_PreProcessing_Techniques/#i-hope-this-guide-has-provided-you-with-a-solid-foundation-for-text-preprocessing-with-nltk-as-you-continue-your-journey-in-nlp-remember-that-preprocessing-is-just-the-beginning-there-are-many-more-exciting-and-advanced-techniques-to-explore-and-apply-in-your-projects","title":"I hope this guide has provided you with a solid foundation for text preprocessing with NLTK. As you continue your journey in NLP, remember that preprocessing is just the beginning. There are many more exciting and advanced techniques to explore and apply in your projects.","text":""},{"location":"algorithms/natural-language-processing/Tf_Idf/","title":"Tf-Idf","text":""},{"location":"algorithms/natural-language-processing/Tf_Idf/#introduction","title":"Introduction","text":"<p>The <code>TFIDF</code> class converts a collection of documents into their respective TF-IDF (Term Frequency-Inverse Document Frequency) representations. TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus).</p>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#attributes","title":"Attributes","text":"<p>The <code>TFIDF</code> class is initialized with two main attributes:</p> <ul> <li><code>self.vocabulary</code>: A dictionary that maps words to their indices in the TF-IDF matrix.</li> <li><code>self.idf_values</code>: A dictionary that stores the IDF (Inverse Document Frequency) values for each word.</li> </ul>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#methods","title":"Methods","text":""},{"location":"algorithms/natural-language-processing/Tf_Idf/#fit-method","title":"fit Method","text":""},{"location":"algorithms/natural-language-processing/Tf_Idf/#input","title":"Input","text":"<ul> <li><code>documents</code> (list of str): List of documents where each document is a string.</li> </ul>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#purpose","title":"Purpose","text":"<p>Calculate the IDF values for all unique words in the corpus.</p>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#steps","title":"Steps","text":"<ol> <li>Count Document Occurrences: Determine how many documents contain each word.</li> <li>Compute IDF: Calculate the importance of each word across all documents. Higher values indicate the word is more unique to fewer documents.</li> <li>Build Vocabulary: Create a mapping of words to unique indexes.</li> </ol>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#transform-method","title":"transform Method","text":""},{"location":"algorithms/natural-language-processing/Tf_Idf/#input_1","title":"Input","text":"<ul> <li><code>documents</code> (list of str): A list where each entry is a document in the form of a string.</li> </ul>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#purpose_1","title":"Purpose","text":"<p>Convert each document into a numerical representation that shows the importance of each word.</p>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#steps_1","title":"Steps","text":"<ol> <li>Compute Term Frequency (TF): Determine how often each word appears in a document relative to the total number of words in that document.</li> <li>Compute TF-IDF: Multiply the term frequency of each word by its IDF to get a measure of its relevance in each document.</li> <li>Store Values: Save these numerical values in a matrix where each row represents a document.</li> </ol>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#fit_transform-method","title":"fit_transform Method","text":""},{"location":"algorithms/natural-language-processing/Tf_Idf/#purpose_2","title":"Purpose","text":"<p>Perform both fitting (computing IDF values) and transforming (converting documents to TF-IDF representation) in one step.</p>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#explanation-of-the-code","title":"Explanation of the Code","text":"<p>The <code>TFIDF</code> class includes methods for fitting the model to the data, transforming new data into the TF-IDF representation, and combining these steps. Here's a breakdown of the primary methods:</p> <ol> <li> <p><code>fit</code> Method: Calculates IDF values for all unique words in the corpus. It counts the number of documents containing each word and computes the IDF. The vocabulary is built with a word-to-index mapping.</p> </li> <li> <p><code>transform</code> Method: Converts each document into a TF-IDF representation. It computes Term Frequency (TF) for each word in the document, calculates TF-IDF by multiplying TF with IDF, and stores these values in a matrix where each row corresponds to a document.</p> </li> <li> <p><code>fit_transform</code> Method: Combines the fitting and transforming steps into a single method for efficient processing of documents.</p> </li> </ol>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#code","title":"Code","text":"<ul> <li>main.py file </li> </ul> <pre><code>import math\nfrom collections import Counter\n\nclass TFIDF:\n    def __init__(self):\n        self.vocabulary = {}  # Vocabulary to store word indices\n        self.idf_values = {}  # IDF values for words\n\n    def fit(self, documents):\n        \"\"\"\n        Compute IDF values based on the provided documents.\n\n        Args:\n            documents (list of str): List of documents where each document is a string.\n        \"\"\"\n        doc_count = len(documents)\n        term_doc_count = Counter()  # To count the number of documents containing each word\n\n        # Count occurrences of words in documents\n        for doc in documents:\n            words = set(doc.split())  # Unique words in the current document\n            for word in words:\n                term_doc_count[word] += 1\n\n        # Compute IDF values\n        self.idf_values = {\n            word: math.log(doc_count / (count + 1))  # +1 to avoid division by zero\n            for word, count in term_doc_count.items()\n        }\n\n        # Build vocabulary\n        self.vocabulary = {word: idx for idx, word in enumerate(self.idf_values.keys())}\n\n    def transform(self, documents):\n        \"\"\"\n        Transform documents into TF-IDF representation.\n\n        Args:\n            documents (list of str): List of documents where each document is a string.\n\n        Returns:\n            list of list of float: TF-IDF matrix where each row corresponds to a document.\n        \"\"\"\n        rows = []\n        for doc in documents:\n            words = doc.split()\n            word_count = Counter(words)\n            doc_length = len(words)\n            row = [0] * len(self.vocabulary)\n\n            for word, count in word_count.items():\n                if word in self.vocabulary:\n                    tf = count / doc_length\n                    idf = self.idf_values[word]\n                    index = self.vocabulary[word]\n                    row[index] = tf * idf\n            rows.append(row)\n        return rows\n\n    def fit_transform(self, documents):\n        \"\"\"\n        Compute IDF values and transform documents into TF-IDF representation.\n\n        Args:\n            documents (list of str): List of documents where each document is a string.\n\n        Returns:\n            list of list of float: TF-IDF matrix where each row corresponds to a document.\n        \"\"\"\n        self.fit(documents)\n        return self.transform(documents)\n# Example usage\nif __name__ == \"__main__\":\n    documents = [\n        \"the cat sat on the mat\",\n        \"the dog ate my homework\",\n        \"the cat ate the dog food\",\n        \"I love programming in Python\",\n        \"Machine learning is fun\",\n        \"Python is a versatile language\",\n        \"Learning new skills is always beneficial\"\n    ]\n\n    # Initialize the TF-IDF model\n    tfidf = TFIDF()\n\n    # Fit the model and transform the documents\n    tfidf_matrix = tfidf.fit_transform(documents)\n\n    # Print the vocabulary\n    print(\"Vocabulary:\", tfidf.vocabulary)\n\n    # Print the TF-IDF representation\n    print(\"TF-IDF Representation:\")\n    for i, vector in enumerate(tfidf_matrix):\n        print(f\"Document {i + 1}: {vector}\")\n\n    # More example documents with mixed content\n    more_documents = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"a journey of a thousand miles begins with a single step\",\n        \"to be or not to be that is the question\",\n        \"the rain in Spain stays mainly in the plain\",\n        \"all human beings are born free and equal in dignity and rights\"\n    ]\n\n    # Fit the model and transform the new set of documents\n    tfidf_more = TFIDF()\n    tfidf_matrix_more = tfidf_more.fit_transform(more_documents)\n\n    # Print the vocabulary for the new documents\n    print(\"\\nVocabulary for new documents:\", tfidf_more.vocabulary)\n\n    # Print the TF-IDF representation for the new documents\n    print(\"TF-IDF Representation for new documents:\")\n    for i, vector in enumerate(tfidf_matrix_more):\n        print(f\"Document {i + 1}: {vector}\")\n</code></pre>"},{"location":"algorithms/natural-language-processing/Tf_Idf/#references","title":"References","text":"<ol> <li>TF-IDF - Wikipedia</li> <li>Understanding TF-IDF</li> <li>Scikit-learn: TF-IDF</li> </ol>"},{"location":"algorithms/natural-language-processing/Transformers/","title":"Transformers","text":"<p>Welcome to the official documentation for the Transformers library! \ud83d\ude80 This library, developed by Hugging Face, is designed to provide state-of-the-art natural language processing (NLP) models and tools. It's widely used for a variety of NLP tasks, including text classification, translation, summarization, and more.</p>"},{"location":"algorithms/natural-language-processing/Transformers/#overview","title":"\ud83d\udd0d Overview","text":"<p>Transformers are a type of deep learning model that excel in handling sequential data, like text. They rely on mechanisms such as attention to process and generate text in a way that captures long-range dependencies and contextual information.</p>"},{"location":"algorithms/natural-language-processing/Transformers/#key-features","title":"Key Features","text":"<ul> <li>State-of-the-art Models: Access pre-trained models like BERT, GPT, T5, and many more. \ud83c\udfc6</li> <li>Easy-to-use Interface: Simplify the process of using and fine-tuning models with a user-friendly API. \ud83c\udfaf</li> <li>Tokenization Tools: Tokenize and preprocess text efficiently for model input. \ud83e\udde9</li> <li>Multi-Framework Support: Compatible with PyTorch and TensorFlow, giving you flexibility in your deep learning environment. \u2699\ufe0f</li> <li>Extensive Documentation: Detailed guides and tutorials to help you get started and master the library. \ud83d\udcd6</li> </ul>"},{"location":"algorithms/natural-language-processing/Transformers/#installation","title":"\ud83d\udd27 Installation","text":"<p>To get started with the Transformers library, you need to install it via pip:</p> <pre><code>pip install transformers\n</code></pre>"},{"location":"algorithms/natural-language-processing/Transformers/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: Version 3.6 or later.</li> <li>PyTorch or TensorFlow: Depending on your preferred framework. Visit the official documentation for compatibility details.</li> </ul>"},{"location":"algorithms/natural-language-processing/Transformers/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Here's a basic example to demonstrate how to use the library for sentiment classification:</p> <pre><code>from transformers import pipeline\n\n# Initialize the pipeline for sentiment analysis\nclassifier = pipeline('sentiment-analysis')\n\n# Analyze sentiment of a sample text\nresult = classifier(\"Transformers are amazing for NLP tasks! \ud83c\udf1f\")\n\nprint(result)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Transformers/#common-pipelines","title":"Common Pipelines","text":"<ul> <li>Text Classification: Classify text into predefined categories.</li> <li>Named Entity Recognition (NER): Identify entities like names, dates, and locations.</li> <li>Text Generation: Generate text based on a prompt.</li> <li>Question Answering: Answer questions based on a given context.</li> <li>Translation: Translate text between different languages.</li> </ul>"},{"location":"algorithms/natural-language-processing/Transformers/#documentation","title":"\ud83d\udcda Documentation","text":"<p>For comprehensive guides, tutorials, and API references, check out the following resources:</p> <ul> <li>Transformers Documentation: The official site with detailed information on using and customizing the library.</li> <li>Model Hub: Explore a wide range of pre-trained models available for different NLP tasks.</li> <li>API Reference: Detailed descriptions of classes and functions in the library.</li> </ul>"},{"location":"algorithms/natural-language-processing/Transformers/#community-and-support","title":"\ud83d\udee0\ufe0f Community and Support","text":"<p>Join the vibrant community of Transformers users and contributors to get support, share your work, and stay updated:</p> <ul> <li>Hugging Face Forums: Engage with other users and experts. Ask questions, share your projects, and participate in discussions.</li> <li>GitHub Repository: Browse the source code, report issues, and contribute to the project. Check out the issues for ongoing conversations.</li> </ul>"},{"location":"algorithms/natural-language-processing/Transformers/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Research Papers: Read the research papers behind the models and techniques used in the library.</li> <li>Blog Posts: Discover insights, tutorials, and updates from the Hugging Face team.</li> <li>Webinars and Talks: Watch recorded talks and webinars on the latest developments and applications of Transformers.</li> </ul>"},{"location":"algorithms/natural-language-processing/Transformers/#faq","title":"\u2753 FAQ","text":"<p>Q: What are the main differences between BERT and GPT?</p> <p>A: BERT (Bidirectional Encoder Representations from Transformers) is designed for understanding the context of words in both directions (left and right). GPT (Generative Pre-trained Transformer), on the other hand, is designed for generating text and understanding context in a left-to-right manner.</p> <p>Q: Can I fine-tune a model on my own data?</p> <p>A: Yes, the Transformers library provides tools for fine-tuning pre-trained models on your custom datasets. Check out the fine-tuning guide for more details.</p> <p>Happy Transforming! \ud83c\udf1f</p>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/","title":"Word 2 Vec","text":""},{"location":"algorithms/natural-language-processing/Word_2_Vec/#introduction","title":"Introduction","text":"<p>Word2Vec is a technique to learn word embeddings using neural networks. The primary goal is to represent words in a continuous vector space where semantically similar words are mapped to nearby points. Word2Vec can be implemented using two main architectures:</p> <ol> <li>Continuous Bag of Words (CBOW): Predicts the target word based on the context words (surrounding words).</li> <li>Skip-gram: Predicts the context words based on a given target word.</li> </ol> <p>In this example, we focus on the Skip-gram approach, which is more commonly used in practice. The Skip-gram model tries to maximize the probability of context words given a target word.</p>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#installation","title":"Installation","text":"<p>Ensure you have Python installed. You can install the necessary dependencies using pip:</p> <pre><code>pip install numpy\n</code></pre>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#usage","title":"Usage","text":""},{"location":"algorithms/natural-language-processing/Word_2_Vec/#initialization","title":"Initialization","text":"<p>Define the parameters for the Word2Vec model:</p> <ul> <li><code>window_size</code>: Defines the size of the context window around the target word.</li> <li><code>embedding_dim</code>: Dimension of the word vectors (embedding space).</li> <li><code>learning_rate</code>: Rate at which weights are updated.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#tokenization","title":"Tokenization","text":"<p>The <code>tokenize</code> method creates a vocabulary from the documents and builds mappings between words and their indices.</p>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#generate-training-data","title":"Generate Training Data","text":"<p>The <code>generate_training_data</code> method creates pairs of target words and context words based on the window size.</p>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#training","title":"Training","text":"<p>The <code>train</code> method initializes the weight matrices and updates them using gradient descent.</p> <p>For each word-context pair, it computes the hidden layer representation, predicts context probabilities, calculates the error, and updates the weights.</p>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#retrieve-word-vector","title":"Retrieve Word Vector","text":"<p>The <code>get_word_vector</code> method retrieves the embedding of a specific word.</p>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#explanation-of-the-code","title":"Explanation of the Code","text":""},{"location":"algorithms/natural-language-processing/Word_2_Vec/#initialization_1","title":"Initialization","text":"<ul> <li>Parameters:</li> <li><code>window_size</code>: Size of the context window around the target word.</li> <li><code>embedding_dim</code>: Dimension of the word vectors (embedding space).</li> <li><code>learning_rate</code>: Rate at which weights are updated.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#tokenization_1","title":"Tokenization","text":"<ul> <li>The <code>tokenize</code> method creates a vocabulary from the documents.</li> <li>Builds mappings between words and their indices.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#generate-training-data_1","title":"Generate Training Data","text":"<ul> <li>The <code>generate_training_data</code> method creates pairs of target words and context words based on the window size.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#training_1","title":"Training","text":"<ul> <li>The <code>train</code> method initializes the weight matrices.</li> <li>Updates the weights using gradient descent.</li> <li>For each word-context pair:</li> <li>Computes the hidden layer representation.</li> <li>Predicts context probabilities.</li> <li>Calculates the error.</li> <li>Updates the weights.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#softmax-function","title":"Softmax Function","text":"<ul> <li>The <code>softmax</code> function converts the output layer scores into probabilities.</li> <li>Used to compute the error and update the weights.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#retrieve-word-vector_1","title":"Retrieve Word Vector","text":"<ul> <li>The <code>get_word_vector</code> method retrieves the embedding of a specific word.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#code","title":"Code","text":"<ul> <li>word2vec.py file </li> </ul> <pre><code>import numpy as np\n\nclass Word2Vec:\n    def __init__(self, window_size=2, embedding_dim=10, learning_rate=0.01):\n        # Initialize parameters\n        self.window_size = window_size\n        self.embedding_dim = embedding_dim\n        self.learning_rate = learning_rate\n        self.vocabulary = {}\n        self.word_index = {}\n        self.index_word = {}\n        self.W1 = None\n        self.W2 = None\n\n    def tokenize(self, documents):\n        # Tokenize documents and build vocabulary\n        vocabulary = set()\n        for doc in documents:\n            words = doc.split()\n            vocabulary.update(words)\n\n        self.vocabulary = list(vocabulary)\n        self.word_index = {word: idx for idx, word in enumerate(self.vocabulary)}\n        self.index_word = {idx: word for idx, word in enumerate(self.vocabulary)}\n\n    def generate_training_data(self, documents):\n        # Generate training data for the Skip-gram model\n        training_data = []\n        for doc in documents:\n            words = doc.split()\n            for idx, word in enumerate(words):\n                target_word = self.word_index[word]\n                context = [self.word_index[words[i]] for i in range(max(0, idx - self.window_size), min(len(words), idx + self.window_size + 1)) if i != idx]\n                for context_word in context:\n                    training_data.append((target_word, context_word))\n        return training_data\n\n    def train(self, documents, epochs=1000):\n        # Tokenize the documents and generate training data\n        self.tokenize(documents)\n        training_data = self.generate_training_data(documents)\n\n        # Initialize weight matrices with random values\n        vocab_size = len(self.vocabulary)\n        self.W1 = np.random.uniform(-1, 1, (vocab_size, self.embedding_dim))\n        self.W2 = np.random.uniform(-1, 1, (self.embedding_dim, vocab_size))\n\n        for epoch in range(epochs):\n            loss = 0\n            for target_word, context_word in training_data:\n                # Forward pass\n                h = self.W1[target_word]  # Hidden layer representation of the target word\n                u = np.dot(h, self.W2)    # Output layer scores\n                y_pred = self.softmax(u) # Predicted probabilities\n\n                # Calculate error\n                e = np.zeros(vocab_size)\n                e[context_word] = 1\n                error = y_pred - e\n\n                # Backpropagation\n                self.W1[target_word] -= self.learning_rate * np.dot(self.W2, error)\n                self.W2 -= self.learning_rate * np.outer(h, error)\n\n                # Calculate loss (cross-entropy)\n                loss -= np.log(y_pred[context_word])\n\n            if (epoch + 1) % 100 == 0:\n                print(f'Epoch {epoch + 1}, Loss: {loss}')\n\n    def softmax(self, x):\n        # Softmax function to convert scores into probabilities\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n\n    def get_word_vector(self, word):\n        # Retrieve the vector representation of a word\n        return self.W1[self.word_index[word]]\n\n    def get_vocabulary(self):\n        # Retrieve the vocabulary\n        return self.vocabulary\n# Example usage\nif __name__ == \"__main__\":\n    # Basic example usage    \n    documents = [\n        \"the cat sat on the mat\",\n        \"the dog ate my homework\",\n        \"the cat ate the dog food\",\n        \"I love programming in Python\",\n        \"Machine learning is fun\",\n        \"Python is a versatile language\",\n        \"Learning new skills is always beneficial\"\n    ]\n\n    # Initialize and train the Word2Vec model\n    word2vec = Word2Vec()\n    word2vec.train(documents)\n\n    # Print the vocabulary\n    print(\"Vocabulary:\", word2vec.get_vocabulary())\n\n    # Print the word vectors for each word in the vocabulary\n    print(\"Word Vectors:\")\n    for word in word2vec.get_vocabulary():\n        vector = word2vec.get_word_vector(word)\n        print(f\"Vector for '{word}':\", vector)\n\n    # More example documents with mixed content\n    more_documents = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"a journey of a thousand miles begins with a single step\",\n        \"to be or not to be that is the question\",\n        \"the rain in Spain stays mainly in the plain\",\n        \"all human beings are born free and equal in dignity and rights\"\n    ]\n\n    # Initialize and train the Word2Vec model on new documents\n    word2vec_more = Word2Vec()\n    word2vec_more.train(more_documents)\n\n    # Print the word vectors for selected words\n    print(\"\\nWord Vectors for new documents:\")\n    for word in ['quick', 'journey', 'be', 'rain', 'human']:\n        vector = word2vec_more.get_word_vector(word)\n        print(f\"Vector for '{word}':\", vector)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Word_2_Vec/#references","title":"References","text":"<ol> <li>Word2Vec - Google</li> <li>Efficient Estimation of Word Representations in Vector Space</li> <li>Distributed Representations of Words and Phrases and their Compositionality</li> </ol>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/","title":"Word Embeddings","text":""},{"location":"algorithms/natural-language-processing/Word_Embeddings/#introduction","title":"Introduction","text":"<p>Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. These embeddings capture semantic meanings of words based on their context and usage in a given corpus. Word embeddings have become a fundamental concept in Natural Language Processing (NLP) and are widely used for various NLP tasks such as sentiment analysis, machine translation, and more.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#why-use-word-embeddings","title":"Why Use Word Embeddings?","text":"<p>Traditional word representations, such as one-hot encoding, have limitations: - High Dimensionality: One-hot encoding results in very high-dimensional vectors, which are sparse (mostly zeros). - Lack of Semantics: One-hot vectors do not capture any semantic relationships between words.</p> <p>Word embeddings address these issues by: - Dimensionality Reduction: Representing words in a lower-dimensional space. - Capturing Semantics: Encoding words such that similar words are closer together in the vector space.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#types-of-word-embeddings","title":"Types of Word Embeddings","text":""},{"location":"algorithms/natural-language-processing/Word_Embeddings/#1-word2vec","title":"1. Word2Vec","text":"<p>Developed by Mikolov et al., Word2Vec generates word embeddings using neural networks. There are two main models:</p> <ul> <li>Continuous Bag of Words (CBOW): Predicts a target word based on its surrounding context words.</li> <li>Skip-gram: Predicts surrounding context words given a target word.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#advantages","title":"Advantages","text":"<ul> <li>Efficient and scalable.</li> <li>Captures semantic similarity.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#limitations","title":"Limitations","text":"<ul> <li>Context window size and other parameters need tuning.</li> <li>Does not consider word order beyond the fixed context window.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#2-glove-global-vectors-for-word-representation","title":"2. GloVe (Global Vectors for Word Representation)","text":"<p>Developed by Pennington et al., GloVe generates embeddings by factorizing the word co-occurrence matrix. The key idea is to use the global statistical information of the corpus.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#advantages_1","title":"Advantages","text":"<ul> <li>Captures global statistical information.</li> <li>Produces high-quality embeddings.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#limitations_1","title":"Limitations","text":"<ul> <li>Computationally intensive.</li> <li>Fixed window size and parameters.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#3-fasttext","title":"3. FastText","text":"<p>Developed by Facebook's AI Research (FAIR) lab, FastText is an extension of Word2Vec that represents words as bags of character n-grams. This allows FastText to generate embeddings for out-of-vocabulary words.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#advantages_2","title":"Advantages","text":"<ul> <li>Handles morphologically rich languages better.</li> <li>Generates embeddings for out-of-vocabulary words.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#limitations_2","title":"Limitations","text":"<ul> <li>Slightly more complex to train than Word2Vec.</li> <li>Increased training time.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#4-elmo-embeddings-from-language-models","title":"4. ELMo (Embeddings from Language Models)","text":"<p>Developed by Peters et al., ELMo generates embeddings using deep contextualized word representations based on a bidirectional LSTM.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#advantages_3","title":"Advantages","text":"<ul> <li>Contextual embeddings.</li> <li>Captures polysemy (different meanings of a word).</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#limitations_3","title":"Limitations","text":"<ul> <li>Computationally expensive.</li> <li>Not as interpretable as static embeddings.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#5-bert-bidirectional-encoder-representations-from-transformers","title":"5. BERT (Bidirectional Encoder Representations from Transformers)","text":"<p>Developed by Devlin et al., BERT uses transformers to generate embeddings. BERT's embeddings are contextual and capture bidirectional context.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#advantages_4","title":"Advantages","text":"<ul> <li>State-of-the-art performance in many NLP tasks.</li> <li>Contextual embeddings capture richer semantics.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#limitations_4","title":"Limitations","text":"<ul> <li>Requires significant computational resources.</li> <li>Complexity in implementation.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#applications-of-word-embeddings","title":"Applications of Word Embeddings","text":"<ul> <li>Sentiment Analysis: Understanding the sentiment of a text by analyzing word embeddings.</li> <li>Machine Translation: Translating text from one language to another using embeddings.</li> <li>Text Classification: Categorizing text into predefined categories.</li> <li>Named Entity Recognition: Identifying and classifying entities in text.</li> </ul>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#example-code","title":"Example Code","text":"<p>Here's an example of using Word2Vec with the <code>gensim</code> library in Python:</p> <pre><code>from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\nnltk.download('punkt')\n\n# Sample corpus\ncorpus = [\n    \"This is a sample sentence\",\n    \"Word embeddings are useful in NLP\",\n    \"Natural Language Processing with embeddings\"\n]\n\n# Tokenize the corpus\ntokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, sg=1)\n\n# Get the embedding for the word 'word'\nword_embedding = model.wv['word']\nprint(\"Embedding for 'word':\", word_embedding)\n</code></pre>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#conclusion","title":"Conclusion","text":"<p>Word embeddings are a powerful technique in NLP that provide a way to represent words in a dense, continuous vector space. By capturing semantic relationships and reducing dimensionality, embeddings improve the performance of various NLP tasks and models.</p>"},{"location":"algorithms/natural-language-processing/Word_Embeddings/#references","title":"References","text":"<ul> <li>Word2Vec Explained</li> <li>GloVe: Global Vectors for Word Representation</li> <li>FastText</li> <li>ELMo: Deep Contextualized Word Representations</li> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li> </ul>"},{"location":"algorithms/statistics/","title":"Statistics \ud83d\udcc3","text":"Descriptive <p>Describe our data in some meaningful manner</p> Inferential <p> Uses sample data to draw conclusions about a larger population.</p> Metrics and Losses <p>Quantify how well your model matches the actual data</p> Probabilty <p>Possibility of the outcome of any random event.</p>"},{"location":"algorithms/statistics/descriptive/","title":"Descriptive Statstics \ud83d\udcc3","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/statistics/inferential/","title":"Inferential Statstics \ud83d\udcc3","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"algorithms/statistics/metrics-and-losses/","title":"Metrics and Losses \ud83d\udcc3","text":""},{"location":"algorithms/statistics/metrics-and-losses/errors/Mean_Absolute_Error/","title":"Mean Absolute Error","text":"<pre><code>import numpy as np\n\ndef mean_absolute_error(y_true, y_pred):\n    \"\"\"\n    Calculate the mean absolute error between true and predicted values.\n\n    Parameters:\n    - y_true: True target values (numpy array).\n    - y_pred: Predicted values (numpy array).\n\n    Returns:\n    - Mean absolute error (float).\n    \"\"\"    \n    return (np.absolute(y_true - y_pred)).mean()\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/errors/Mean_Squared_Error/","title":"Mean Squared Error","text":"<pre><code>import numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"\n    Calculate the mean squared error between true and predicted values.\n\n    Parameters:\n    - y_true: True target values (numpy array).\n    - y_pred: Predicted values (numpy array).\n\n    Returns:\n    - Mean squared error (float).\n    \"\"\"\n    return np.mean((y_true - y_pred) ** 2)\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/errors/R2_Squared_Error/","title":"R2 Squared Error","text":"<pre><code>import numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value between true and predicted values.\n\n    Parameters:\n    - y_true: True target values (numpy array).\n    - y_pred: Predicted values (numpy array).\n\n    Returns:\n    - R-squared value (float).\n    \"\"\"\n    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n    explained_variance = np.sum((y_pred - np.mean(y_true)) ** 2)\n    r2 = 1 - (explained_variance / total_variance)\n    return r2\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/errors/Root_Mean_Squared_Error/","title":"Root Mean Squared Error","text":"<pre><code>import numpy as np\nimport math as mt\n\ndef root_mean_squared_error(y_true,y_pred):\n    \"\"\"\n    Calculate the root mean squared error between true and predicted values.\n\n    Parameters:\n    - y_true: True target values (numpy array).\n    - y_pred: Predicted values (numpy array).\n\n    Returns:\n    - Root Mean squared error (float).\n    \"\"\"\n    return mt.sqrt(np.mean((y_true - y_pred) ** 2))\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/loss-functions/Cross_Entropy_Loss/","title":"Cross Entropy Loss","text":"<pre><code>import numpy as np\n\ndef binary_cross_entropy_loss(y_true: np.ndarray | list, y_pred: np.ndarray | list) -&gt; float:\n    \"\"\"\n    Calculate the binary cross entropy loss between true and predicted values.\n    It measures the difference between the predicted probability distribution and the actual binary label distribution.\n    The formula for binary cross-entropy loss is as follows:\n\n    L(y, \u0177) = -[y * log(\u0177) + (1 \u2014 y) * log(1 \u2014 \u0177)]\n\n    where y is the true binary label (0 or 1), \u0177 is the predicted probability (ranging from 0 to 1), and log is the natural logarithm.\n\n    Parameters:\n    - y_true: True target values (numpy array).\n    - y_pred: Predicted values (numpy array).\n\n    Returns:\n    - Binary cross entropy loss (float).\n    \"\"\"\n    if (y_true is not None) and (y_pred is not None):\n        if type(y_true) == list:\n            y_true = np.asarray(y_true)\n        if type(y_pred) == list:\n            y_pred = np.asarray(y_pred)\n        assert y_true.shape == y_pred.shape, f\"Shape of y_true ({y_true.shape}) does not match y_pred ({y_pred.shape})\"\n        # calculate the binary cross-entropy loss\n        loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n        return loss\n    else:\n        return None\n\ndef weighted_binary_cross_entropy_loss(y_true: np.ndarray | list, y_pred: np.ndarray | list, w_pos: float, w_neg: float) -&gt; float:\n    \"\"\"\n    Calculates the weighted binary cross entropy loss between true and predicted values.\n    Weighted Binary Cross-Entropy loss is a variation of the binary cross-entropy loss that allows for assigning different weights to positive and negative examples. This can be useful when dealing with imbalanced datasets, where one class is significantly underrepresented compared to the other.\n    The formula for weighted binary cross-entropy loss is as follows:\n\n    L(y, \u0177) = -[w_pos * y * log(\u0177) + w_neg * (1 \u2014 y) * log(1 \u2014 \u0177)]\n\n    where y is the true binary label (0 or 1), \u0177 is the predicted probability (ranging from 0 to 1), log is the natural logarithm, and w_pos and w_neg are the positive and negative weights, respectively.\n\n    Parameters:\n    - y_true: True target values (numpy array).\n    - y_pred: Predicted values (numpy array).\n\n    Returns:\n    - Weighted binary cross entropy loss (float).\n    \"\"\"\n    if (y_true is not None) and (y_pred is not None):\n        assert w_pos != 0.0, f\"Weight w_pos = {w_pos}\"\n        assert w_neg != 0.0, f\"Weight w_neg = {w_neg}\"\n        if type(y_true) == list:\n            y_true = np.asarray(y_true)\n        if type(y_pred) == list:\n            y_pred = np.asarray(y_pred)\n        assert y_true.shape == y_pred.shape, f\"Shape of y_true ({y_true.shape}) does not match y_pred ({y_pred.shape})\"\n        # calculate the binary cross-entropy loss\n        loss = -(w_pos * y_true * np.log(y_pred) + w_neg * (1 - y_true) * np.log(1 - y_pred)).mean()\n        return loss\n    else:\n        return None\n\n\ndef categorical_cross_entropy_loss(y_true: np.ndarray | list, y_pred: np.ndarray | list) -&gt; float:\n    \"\"\"\n    Calculate the categorical cross entropy loss between true and predicted values.\n    It measures the difference between the predicted probability distribution and the actual one-hot encoded label distribution.\n    The formula for categorical cross-entropy loss is as follows:\n\n    L(y, \u0177) = -1/N * \u03a3[\u03a3{y * log(\u0177)}]\n\n    where y is the true one-hot encoded label vector, \u0177 is the predicted probability distribution, and log is the natural logarithm.\n\n    Parameters:\n    - y_true: True target values (numpy array) (one-hot encoded).\n    - y_pred: Predicted values (numpy array) (probabilities).\n\n    Returns:\n    - Categorical cross entropy loss (float).\n    \"\"\"\n    if (y_true is not None) and (y_pred is not None):\n        if type(y_true) == list:\n            y_true = np.asarray(y_true)\n        if type(y_pred) == list:\n            y_pred = np.asarray(y_pred)\n        assert y_pred.ndim == 2, f\"Shape of y_pred should be (N, C), got {y_pred.shape}\"\n        assert y_true.shape == y_pred.shape, f\"Shape of y_true ({y_true.shape}) does not match y_pred ({y_pred.shape})\"\n\n        # Ensure numerical stability\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n\n        # calculate the categorical cross-entropy loss\n        loss = -1/len(y_true) * np.sum(np.sum(y_true * np.log(y_pred)))\n        return loss.mean()\n    else:\n        return None\n\ndef sparse_categorical_cross_entropy_loss(y_true: np.ndarray | list, y_pred: np.ndarray | list) -&gt; float:\n    \"\"\"\n    Calculate the sparse categorical cross entropy loss between true and predicted values.\n    It measures the difference between the predicted probability distribution and the actual class indices.\n    The formula for sparse categorical cross-entropy loss is as follows:\n\n    L(y, \u0177) = -\u03a3[log(\u0177[range(N), y])]\n\n    where y is the true class indices, \u0177 is the predicted probability distribution, and log is the natural logarithm.\n\n    Parameters:\n    - y_true: True target values (numpy array) (class indices).\n    - y_pred: Predicted values (numpy array) (probabilities).\n\n    Returns:\n    - Sparse categorical cross entropy loss (float).\n    \"\"\"\n    if (y_true is not None) and (y_pred is not None):\n        if type(y_true) == list:\n            y_true = np.asarray(y_true)\n        if type(y_pred) == list:\n            y_pred = np.asarray(y_pred)\n        assert y_true.shape[0] == y_pred.shape[0], f\"Batch size of y_true ({y_true.shape[0]}) does not match y_pred ({y_pred.shape[0]})\"\n\n        # convert true labels to one-hot encoding\n        y_true_onehot = np.zeros_like(y_pred)\n        y_true_onehot[np.arange(len(y_true)), y_true] = 1\n\n        # Ensure numerical stability\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n\n        # calculate loss\n        loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred), axis=-1))\n        return loss\n    else:\n        return None\n\n\nif __name__ == \"__main__\":\n    # define true labels and predicted probabilities\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([0.1, 0.9, 0.8, 0.3])\n\n    print(\"\\nTesting Binary Cross Entropy Loss\")\n    print(\"Y_True: \", y_true)\n    print(\"Y_Pred:\", y_pred)\n    print(\"Binary Cross Entropy Loss: \", binary_cross_entropy_loss(y_true, y_pred))\n\n    positive_weight = 0.7\n    negative_weight = 0.3\n\n    print(\"\\nTesting Weighted Binary Cross Entropy Loss\")\n    print(\"Y_True: \", y_true)\n    print(\"Y_Pred:\", y_pred)\n    print(\"Weighted Binary Cross Entropy Loss: \", weighted_binary_cross_entropy_loss(y_true, y_pred, positive_weight, negative_weight))\n\n    y_true = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.3, 0.5], [0.1, 0.6, 0.3]])\n    print(\"\\nTesting Categorical Cross Entropy Loss\")\n    print(\"Y_True: \", y_true)\n    print(\"Y_Pred:\", y_pred)\n    print(\"Categorical Cross Entropy Loss: \", categorical_cross_entropy_loss(y_true, y_pred))\n\n    y_true = np.array([1, 2, 0])\n    y_pred = np.array([[0.1, 0.8, 0.1], [0.3, 0.2, 0.5], [0.4, 0.3, 0.3]])\n    print(\"\\nTesting Sparse Categorical Cross Entropy Loss\")\n    print(\"Y_True: \", y_true)\n    print(\"Y_Pred:\", y_pred)\n    print(\"Sparse Categorical Cross Entropy Loss: \", sparse_categorical_cross_entropy_loss(y_true, y_pred))\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/loss-functions/Hinge_Loss/","title":"Hinge Loss","text":"<pre><code>import numpy as np\n\ndef hinge_loss(y_true: np.ndarray | list, y_pred: np.ndarray | list)-&gt; float:\n    \"\"\"\n    Calculates the hinge loss between true and predicted values.\n\n    The formula for hinge loss is as follows:\n\n    L(y, \u0177) = max(0, 1 - y * \u0177)\n\n    \"\"\"\n    if (y_true is not None) and (y_pred is not None):\n        if type(y_true) == list:\n            y_true = np.asarray(y_true)\n        if type(y_pred) == list:\n            y_pred = np.asarray(y_pred)\n        assert y_true.shape[0] == y_pred.shape[0], f\"Batch size of y_true ({y_true.shape[0]}) does not match y_pred ({y_pred.shape[0]})\"\n\n    # replacing 0 values to -1\n    y_pred = np.where(y_pred == 0, -1, 1)\n    y_true = np.where(y_true == 0, -1, 1)\n\n     # Calculate loss\n    loss = np.maximum(0, 1 - y_true * y_pred).mean()\n    return loss\n\nif __name__ == \"__main__\":\n    # define true labels and predicted probabilities\n    actual = np.array([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1])\n    predicted = np.array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])\n\n    print(\"\\nTesting Hinge Loss\")\n    print(\"Y_True: \", actual)\n    print(\"Y_Pred:\", predicted)\n    print(\"Hinge Loss: \", hinge_loss(actual, predicted))\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/loss-functions/Kullback_Leibler_Divergence_Loss/","title":"KL Divergence Loss","text":"<pre><code>Kullback Leibler Divergence Loss\n</code></pre> <pre><code>import numpy as np\n\ndef kl_divergence_loss(y_true: np.ndarray | list, y_pred: np.ndarray | list) -&gt; float:\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two probability distributions.\n    KL divergence measures how one probability distribution diverges from another reference probability distribution.\n\n    The formula for KL divergence is:\n    D_KL(P || Q) = \u03a3 P(x) * log(P(x) / Q(x))\n\n    where P is the true probability distribution and Q is the predicted probability distribution.\n\n    Parameters:\n    - y_true: True probability distribution (numpy array or list).\n    - y_pred: Predicted probability distribution (numpy array or list).\n\n    Returns:\n    - KL divergence loss (float).\n    \"\"\"\n    if (y_true is not None) and (y_pred is not None):\n        if type(y_true) == list:\n            y_true = np.asarray(y_true)\n        if type(y_pred) == list:\n            y_pred = np.asarray(y_pred)\n        assert y_true.shape == y_pred.shape, f\"Shape of p_true ({y_true.shape}) does not match q_pred ({y_pred.shape})\"\n\n        # Ensure numerical stability by clipping the probabilities\n        y_true = np.clip(y_true, 1e-15, 1)\n        y_pred = np.clip(y_pred, 1e-15, 1)\n\n        # Normalize the distributions\n        y_true /= y_true.sum(axis=-1, keepdims=True)\n        y_pred /= y_pred.sum(axis=-1, keepdims=True)\n\n        # Calculate KL divergence\n        kl_div = np.sum(y_true * np.log(y_true / y_pred), axis=-1)\n        return kl_div.mean()\n    else:\n        return None\n\nif __name__ == \"__main__\":\n    y_true = np.array([[0.2, 0.5, 0.3], [0.1, 0.7, 0.2]]) # True probability distributions\n    y_pred = np.array([[0.1, 0.6, 0.3], [0.2, 0.5, 0.3]]) # Predicted probability distributions\n\n    print(\"\\nTesting Kullback Leibler Divergence Loss\")\n    print(\"Y_True: \", y_true)\n    print(\"Y_Pred:\", y_pred)\n    print(\"Kullback Leibler Divergence Loss: \", kl_divergence_loss(y_true, y_pred))\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/loss-functions/Ranking_Losses/","title":"Ranking Losses","text":""},{"location":"algorithms/statistics/metrics-and-losses/loss-functions/Ranking_Losses/#pair-wise-ranking-loss","title":"Pair Wise Ranking Loss","text":"<pre><code>import tensorflow as tf\nfrom typing import Tuple\n\ndef pairwise_ranking_loss(y_true: tf.Tensor, y_pred: tf.Tensor, margin: float = 1.0) -&gt; tf.Tensor:\n    \"\"\"\n    Computes the pairwise ranking loss for a batch of pairs.\n\n    Args:\n        y_true: Tensor of true labels (0 for negative pairs, 1 for positive pairs).\n        y_pred: Tensor of predicted similarities/distances, expected to be a tensor of shape (batch_size, 2, embedding_dim) where \n                y_pred[:, 0] is the anchor and y_pred[:, 1] is the positive/negative.\n        margin: Margin parameter for the pairwise ranking loss.\n\n    Returns:\n        loss: Computed pairwise ranking loss as a scalar tensor.\n    \"\"\"\n    anchor, positive_or_negative = y_pred[:, 0], y_pred[:, 1]\n\n    distances = tf.reduce_sum(tf.square(anchor - positive_or_negative), axis=-1)\n    positive_loss = y_true * distances\n    negative_loss = (1 - y_true) * tf.maximum(margin - distances, 0.0)\n\n    loss = positive_loss + negative_loss\n    return tf.reduce_mean(loss)\n\n# Example usage:\n# model.compile(optimizer='adam', loss=pairwise_ranking_loss)\n</code></pre>"},{"location":"algorithms/statistics/metrics-and-losses/loss-functions/Ranking_Losses/#triplet-loss","title":"Triplet Loss","text":"<pre><code>import tensorflow as tf\nfrom typing import Tuple\n\ndef triplet_loss_func(y_true: tf.Tensor, y_pred: tf.Tensor, alpha: float = 0.3) -&gt; tf.Tensor:\n    \"\"\"\n    Computes the triplet loss for a batch of triplets.\n\n    Args:\n        y_true: True values of classification (unused in this implementation, typically required for compatibility with Keras).\n        y_pred: Predicted values, expected to be a tensor of shape (batch_size, 3, embedding_dim) where \n                y_pred[:, 0] is the anchor, y_pred[:, 1] is the positive, and y_pred[:, 2] is the negative.\n        alpha: Margin parameter for the triplet loss.\n\n    Returns:\n        loss: Computed triplet loss as a scalar tensor.\n    \"\"\"\n    anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n\n    positive_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n    negative_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n\n    loss = tf.maximum(positive_dist - negative_dist + alpha, 0.0)\n    return tf.reduce_mean(loss)\n\n# Example usage:\n# model.compile(optimizer='adam', loss=triplet_loss_func)\n</code></pre>"},{"location":"algorithms/statistics/probability/","title":"Probability \ud83d\udcc3","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"projects/","title":"Projects \ud83c\udf89","text":"Statistics <p>Understanding data through statistical analysis and inference methods.</p> Machine Learning <p>Dive into the world of algorithms and models in Machine Learning.</p> Deep Learning <p>Explore the fascinating world of deep learning.</p> Computer Vision <p>Learn computer vision with OpenCV for real-time image processing applications.</p> Natural Language Processing <p>Dive into how machines understand and generate human language.</p> Generative Adversarial Networks <p>Learn about the power of Generative Adversarial Networks for creative AI solutions.</p> Large Language Models <p>Explore the cutting-edge techniques behind large language models like GPT and BERT.</p> Artificial Intelligence <p>Explore the fundamentals and advanced concepts of Artificial Intelligence.</p>"},{"location":"projects/artificial-intelligence/","title":"Artificial Intelligence \ud83d\udca1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"projects/computer-vision/","title":"Computer Vision \ud83c\udfa5","text":"Face Detection Model <p>Detecting faces in images using OpenCV's powerful Haar cascades.</p> <p>\ud83d\udcc5 2025-01-16 | \u23f1\ufe0f 10 mins</p>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/","title":"Black and White Image Colorizer","text":""},{"location":"projects/computer-vision/black_and_white_image_colorizer/#aim","title":"AIM","text":"<p>Colorization of Black and White Images using OpenCV and pre-trained caffe models.</p>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#pre-trained-models","title":"PRE-TRAINED MODELS","text":"<p>colorization_deploy_v2.prototxt -  colorization_release_v2.caffemodel -  pts_in_hull.npy</p>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#notebook-link","title":"NOTEBOOK LINK","text":"<p>Colab Notebook</p>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>numpy</li> <li>cv2</li> </ul>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project aims to perform colorization of black and white images.</li> <li>It involves in showcase the capabilities of OpenCV's DNN module and caffe models.</li> <li>It is done by processing given image using openCV and use Lab Color space model to hallucinate an approximation of how colorized version of the image \"might look like\".</li> </ul> Why is it necessary? <ul> <li>It helps preserving historical black-and-white photos. </li> <li>It can be used adding color to grayscale images for creative industries.  </li> <li>It acts an advancing computer vision applications in artistic and research fields.</li> </ul> How is it beneficial and used? <ul> <li>Personal use : It helps in restoring old family photographs.  </li> <li>Cultural and Political : it also enhances grayscale photographs of important historic events for modern displays. </li> <li>Creativity and Art  : it improves AI-based creative tools for artists and designers.  </li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Initial approach : reading various research papers and analyze different approaches on how to deal with this project.</li> <li>Identified Richzhang research paper on the title : Colorful Image colorization.</li> <li>Did some research on pre-trained models for image colorization.  </li> <li>Understood OpenCV's DNN module and its implementation.  </li> <li>Experimented with sample images to test model outputs. </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Richzhang's Colorful Image Colorization</li> <li>Lab Color space</li> <li>openCV Documentation </li> </ul>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#explanation","title":"EXPLANATION","text":""},{"location":"projects/computer-vision/black_and_white_image_colorizer/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4 <p>Initial data exploration and understanding:</p> <ul> <li>Load the grayscale input image.</li> <li>Load pre-trained caffe models using openCV dnn module.</li> </ul> <p>Data cleaning and preprocessing:</p> <ul> <li>Preprocess image to normalize and convert to LAB color space.</li> <li>Resize image for the network.</li> <li>Split L channel and perform mean subtraction.</li> <li>Predict ab channel from the input of L channel.</li> </ul> <p>Feature engineering and selection:</p> <ul> <li>Resize predicted ab channel's volume to same dimension as our image.</li> <li>Join L and predicted ab channel.</li> <li>Convert image from Lab back to RGB.</li> </ul> <p>Result : </p> <ul> <li>Resize and Show the Original and Colorized image.</li> </ul>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Computational efficiency vs. color accuracy.  </li> <li>Solution : Used optimized preprocessing pipelines to reduce runtime. </li> </ul> <ul> <li>Pre-trained model generalization vs. custom training.  </li> <li>Solution : Choose the pre-trained model for faster implementation and reliable results.  </li> </ul>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#screenshots","title":"SCREENSHOTS","text":"<p>Project structure or tree diagram</p> <pre><code>  graph LR  \n    A[Load Grayscale Image] --&gt; B[Preprocess Image];  \n    B --&gt; C[Load Pre-trained Model];  \n    C --&gt; D[Predict A and B Channels];  \n    D --&gt; E[Combine with L Channel];  \n    E --&gt; F[Convert to RGB];  \n    F --&gt; G[Display/Save Colorized Image];</code></pre> Visualizations of results Original ImageColorized ImageResult <p></p> <p></p> <p></p>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/computer-vision/black_and_white_image_colorizer/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Color Space : LAB color space facilitates colorization tasks.  </li> <li>Pre-trained Models : Pre-trained models can generalize across various grayscale images.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>OpenCV : Enhanced knowledge of OpenCV's DNN module.  </li> <li>Caffe Models : Usage of pre-trained models.</li> <li>Image Dimensionality : Understanding how Image can be manipulated.</li> </ul> Challenges faced and how they were overcome <ul> <li>Color Space Conversion : Initial difficulties with LAB to RGB conversion; resolved using OpenCV documentation. </li> </ul>"},{"location":"projects/computer-vision/black_and_white_image_colorizer/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Image Restoration</p> <ul> <li>Restoring old family photographs to vivid colors.</li> </ul> <p>Creative Industries</p> <ul> <li>Colorizing artistic grayscale sketches for concept designs.</li> </ul>"},{"location":"projects/computer-vision/face-detection/","title":"Face Detection","text":""},{"location":"projects/computer-vision/face-detection/#aim","title":"AIM","text":"<p>The goal of this project is to build a face detection system using OpenCV, which identifies faces in static images using Haar Cascades.</p>"},{"location":"projects/computer-vision/face-detection/#dataset-link","title":"DATASET LINK","text":"<p>For this project we are going to use the pretrained Haar Cascade XML file for face detection from OpenCV's Github repository. </p> <p>https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml</p>"},{"location":"projects/computer-vision/face-detection/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1upcl9sa5cL5fUuVLBG5IVuU0xPYs3Nwf#scrollTo=94ggAdg5AnUk</p>"},{"location":"projects/computer-vision/face-detection/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>OpenCV</li> <li>Random</li> <li>Matplotlib</li> </ul>"},{"location":"projects/computer-vision/face-detection/#description","title":"DESCRIPTION","text":"<p>This project involves building a face detection model using OpenCV's pre-trained Haar Cascade Classifiers to detect faces in images.</p> <p>What is the requirement of the project?</p> <ul> <li>A face detection system is needed for various applications such as security, attendance tracking, and facial recognition systems.</li> <li>This project demonstrates a basic use of computer vision techniques for detecting faces in static images.</li> </ul> Why is it necessary? <ul> <li>Face detection is the first crucial step in many computer vision applications such as face recognition and emotion analysis.</li> <li>It is an essential component in systems that require human identification or verification.</li> </ul> How is it beneficial and used? <ul> <li>Face detection can be used in automation systems, for example, in attendance tracking, photo tagging, and security surveillance.</li> <li>It enables various applications in user experience enhancement and biometric systems.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>I began by exploring OpenCV documentation, focusing on how to implement Haar Cascade for face detection.</li> <li>Initially, I focused on static image detection, planning to extend the project to video-based detection in the future.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>OpenCV documentation</li> <li>Book: \"Learning OpenCV 3\" by Adrian Kaehler and Gary Bradski</li> </ul>"},{"location":"projects/computer-vision/face-detection/#explanation","title":"EXPLANATION","text":""},{"location":"projects/computer-vision/face-detection/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<pre><code>- Haar Cascade Classifier: A machine learning-based approach for detecting objects in images or video. It works by training on a large set of positive and negative images of faces.\n- Cascade Classifier*: The classifier works through a series of stages, each aimed at increasing detection accuracy.\n- Face Detection: The primary feature of this project is detecting human faces in static images, which is the first step in many facial recognition systems.\n</code></pre>"},{"location":"projects/computer-vision/face-detection/#project-workflow","title":"PROJECT WORKFLOW","text":"Step 1 <p>Initial data exploration and understanding:</p> <ul> <li>Research the Haar Cascade method for face detection in OpenCV.</li> <li>Collect sample images for testing the model's performance.</li> </ul> Step 2 <p>Data cleaning and preprocessing:</p> <ul> <li>Ensure all input images are properly formatted (e.g., grayscale images for face detection).</li> <li>Resize or crop images to ensure optimal processing speed.</li> </ul> Step 3Step 4Step 5 <p>Feature engineering and selection: - Use pre-trained Haar Cascade classifiers for detecting faces. - Select the appropriate classifier based on face orientation and conditions (e.g., frontal face, profile).</p> <p>Model training and evaluation: - Use OpenCV's pre-trained Haar Cascade models. - Test the detection accuracy on various sample images.</p> <p>Model optimization and fine-tuning:</p> <ul> <li>Adjust parameters such as scale factor and minNeighbors to enhance accuracy.</li> <li>Experiment with different input image sizes to balance speed and accuracy.</li> </ul> Step 6 <p>Validation and testing:</p> <ul> <li>Validate the model's effectiveness on different test images, ensuring robust detection.</li> <li>Evaluate the face detection accuracy based on diverse lighting and image conditions.</li> </ul>"},{"location":"projects/computer-vision/face-detection/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. computational efficiency.<ul> <li>Solution: Fine-tuned classifier parameters to ensure a balance between accuracy and speed.</li> </ul> </li> </ul> <ul> <li>Detection performance vs. image resolution. <ul> <li>Solution: Optimized input image resolution and processing flow to ensure both fast processing and accurate detection.</li> </ul> </li> </ul>"},{"location":"projects/computer-vision/face-detection/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\nA[Start] --&gt; B{Face Detected?}\nB --&gt;|Yes| C[Mark Face]\nC --&gt; D[Display Result]\nB --&gt;|No| F[Idle/Do Nothing]</code></pre>"},{"location":"projects/computer-vision/face-detection/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/computer-vision/face-detection/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Gained an understanding of face detection using Haar Cascades. </li> <li>Improved ability to optimize computer vision models for accuracy and speed.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned how to handle trade-offs between accuracy and speed in real-time applications. </li> <li>Gained hands-on experience with the implementation of object detection algorithms.</li> </ul> Challenges faced and how they were overcome <ul> <li>Challenge: Low detection accuracy in poor lighting conditions. </li> <li>Solution: Adjusted classifier parameters and added preprocessing steps to improve accuracy.</li> </ul>"},{"location":"projects/computer-vision/face-detection/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Security Surveillance Systems</p> <ul> <li>Used for identifying individuals or monitoring for intruders in secure areas.</li> </ul> <p>Attendance Systems</p> <ul> <li>Used to automate attendance tracking by detecting the faces of students or employees.</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/","title":"Music Genre Classification Model","text":""},{"location":"projects/computer-vision/music_genre_classification_model/#aim","title":"AIM","text":"<p>To develop a precise and effective music genre classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), Random Forest and XGBoost Classifier algorithms for the Kaggle GTZAN Dataset Music Genre Classification. </p>"},{"location":"projects/computer-vision/music_genre_classification_model/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/data</p>"},{"location":"projects/computer-vision/music_genre_classification_model/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1j8RZccP2ee5XlWEFSkTyJ98lFyNrezHS?usp=sharing</p>"},{"location":"projects/computer-vision/music_genre_classification_model/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>librosa</li> <li>matplotlib</li> <li>pandas</li> <li>sklearn</li> <li>seaborn</li> <li>numpy</li> <li>scipy</li> <li>xgboost</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The objective of this research is to develop a precise and effective music genre classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), Random Forest and XGBoost algorithms for the Kaggle GTZAN Dataset Music Genre Classification.</li> </ul> Why is it necessary? <ul> <li>Music genre classification has several real-world applications, including music recommendation, content-based music retrieval, and personalized music services. However, the task of music genre classification is challenging due to the subjective nature of music and the complexity of audio signals.</li> </ul> How is it beneficial and used? <ul> <li>For User: Provides more personalised music</li> <li>For Developers: A recommendation system for songs that are of interest to the user </li> <li>For Business: Able to charge premium for the more personalised and recommendation services provided</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Initially how the different sounds are structured. </li> <li>Learned how to represent sound signal in 2D format on graphs using the librosa library.</li> <li>Came to know about the various features of sound like <ul> <li>Mel-frequency cepstral coefficients (MFCC)</li> <li>Chromagram</li> <li>Spectral Centroid</li> <li>Zero-crossing rate</li> <li>BPM - Beats Per Minute</li> </ul> </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>https://scholarworks.calstate.edu/downloads/73666b68n</li> <li>https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/data</li> <li>https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#explanation","title":"EXPLANATION","text":""},{"location":"projects/computer-vision/music_genre_classification_model/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<pre><code>There are 3 different types of the datasets.\n\n- genres_original\n- images_original\n- features_3_sec.csv\n- feature_30_sec.csv\n</code></pre> <ul> <li> <p>The features in <code>genres_original</code></p> <p>['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] Each and every genre has 100 WAV files</p> </li> <li> <p>The features in <code>genres_original</code></p> <p>['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] Each and every genre has 100 PNG files</p> </li> <li> <p>There are 60 features in <code>features_3_sec.csv</code></p> </li> <li> <p>There are 60 features in <code>features_30_sec.csv</code></p> </li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5Step 6Step 7 <ul> <li>Created data visual reprsentation of the data to help understand the data</li> </ul> <ul> <li>Found strong relationships between independent features and dependent feature using correlation.</li> </ul> <ul> <li>Performed Exploratory Data Analysis on data.</li> </ul> <ul> <li>Used different Classification techniques like SVM, Random Forest, </li> </ul> <ul> <li>Compared various models and used best performance model to make predictions.</li> </ul> <ul> <li>Used Mean Squared Error and R2 Score for evaluating model's performance.</li> </ul> <ul> <li>Visualized best model's performance using matplotlib and seaborn library.</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2Trade Off 3Trade Off 4 <p>How do you visualize audio signal</p> <ul> <li> <p>Solution: </p> </li> <li> <p>librosa: It is the mother of all audio file libraries</p> </li> <li>Plotting Graphs: As I have the necessary libraries to visualize the data. I started plotting the audio signals</li> <li>Spectogram:A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams. Here we convert the frequency axis to a logarithmic one.</li> </ul> <p>Features that help classify the data</p> <ul> <li> <p>Solution:</p> </li> <li> <p>Feature Engineering: What are the features present in audio signals</p> </li> <li>Spectral Centroid: Indicates where the \u201dcentre of mass\u201d for a sound is located and is calculated as the weighted mean of the frequencies present in the sound.</li> <li>Mel-Frequency Cepstral Coefficients: The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.</li> <li>Chroma Frequencies: Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.</li> </ul> <p>Performing EDA on the CSV files</p> <ul> <li> <p>Solution:</p> </li> <li> <p>Tool Selection: Used the correlation matrix on the features_30_sec.csv dataset to extract most related datasets</p> </li> <li>Visualization Best Practices: Followed best practices such as using appropriate chart types (e.g., box plots for BPM data, PCA plots for correlations), adding labels and titles, and ensuring readability.</li> <li>Iterative Refinement: Iteratively refined visualizations based on feedback and self-review to enhance clarity and informativeness.</li> </ul> <p>Implementing Machine Learning Models</p> <ul> <li> <p>Solution:</p> </li> <li> <p>Cross-validation: Used cross-validation techniques to ensure the reliability and accuracy of the analysis results.</p> </li> <li>Collaboration with Experts: Engaged with Music experts and enthusiasts to validate the findings and gain additional perspectives.</li> <li>Contextual Understanding: Interpreted results within the context of the music, considering factors such as mood of the users, surrounding, and specific events to provide meaningful and actionable insights.</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Visualizations and EDA of different features Harm PercSound WaveSTFTPop Mel-SpecBlues Mel-SpecSpec CentSpec RolloffMFCCChromogramCorr HeatmapBPM BoxplotPCA Scatter PlotConfusion Matrix <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"projects/computer-vision/music_genre_classification_model/#models-used-and-their-accuracies","title":"MODELS USED AND THEIR ACCURACIES","text":"Model Accuracy KNN 0.80581 Random Forest 0.81415 Cross Gradient Booster 0.90123 SVM 0.75409"},{"location":"projects/computer-vision/music_genre_classification_model/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> ACC Plot <p></p>"},{"location":"projects/computer-vision/music_genre_classification_model/#conclusion","title":"CONCLUSION","text":"<pre><code>We can see that Accuracy plots of the different models.\nXGB Classifier can predict most accurate results for predicting the Genre of the music.\n</code></pre>"},{"location":"projects/computer-vision/music_genre_classification_model/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>Discovered a new library that help visualize audio signal</li> <li>Discovered new features related to audio like STFT, MFCC, Spectral Centroid, Spectral Rolloff</li> <li>Gained a deeper understanding of the features of different genres of music</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Enhanced knowledge of data cleaning and preprocessing techniques to handle real-world datasets.</li> <li>Improved skills in exploratory data analysis (EDA) to extract meaningful insights from raw data.</li> <li>Learned how to use visualization tools to effectively communicate data-driven findings.</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <p>User Personalisation</p> <ul> <li>It can be used to provide more personalised music recommendation for users based on their taste in music or the various genres they listen to. This personalisation experience can be used to develop 'Premium' based business models.</li> </ul> <p>Compatability Between Users</p> <ul> <li>Based on the musical taste and the genres they listen we can identify the user behaviour and pattern come with similar users who can be friends with. This increases social interaction within the app.</li> </ul>"},{"location":"projects/computer-vision/music_genre_classification_model/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1Feature 1 <ul> <li> <p>Real-time Compatability Tracking</p> </li> <li> <p>Implementing a real-time tracking system to view compatability between users.</p> </li> </ul> <ul> <li> <p>Predictive Analytics</p> </li> <li> <p>Using advanced machine learning algorithms to predict the next song the users is likely to listen to.</p> </li> </ul>"},{"location":"projects/deep-learning/","title":"Deep Learning \u2728","text":"Brain Tumor Detection Model <p>Deep learning algorithm for image and video recognition.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 10 mins</p>"},{"location":"projects/deep-learning/brain-tumor-detection-model/","title":"Brain Tumor Detectioon","text":""},{"location":"projects/deep-learning/brain-tumor-detection-model/#aim","title":"AIM","text":"<p>To predict the Brain Tumor using Convolutional Neural Network</p>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/primus11/brain-tumor-mri</p>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://colab.research.google.com/github/11PRIMUS/ALOK/blob/main/Tumor3.ipynb</p>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 for TunedThresholdClassifierCV)</li> <li>matplotlib</li> <li>seaborn</li> <li>streamlit</li> </ul>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>This project aims to predict early stage brain tumor it uses Convolutional Neural Network to classify wheter tumor is present or not.</li> </ul> Why is it necessary? <ul> <li>Brain Tumor is leading case of deaths on world and most of the cases can be solved by detecting the cancer in its initial stages so one can take medication according to that without having further risks.</li> </ul> How is it beneficial and used? <ul> <li>Doctors can use it to detect cancer and the region affected by that using MRI scans an help patient to overcom ethat with right and proper guidance. It also acts as a fallback mechanism in rare cases where the diagnosis is not obvious.</li> <li>People (patients in particular) can check simply by using MRI scans to detect Tumor and take necessary medication and precautions</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Going through previous research and articles related to the problem.</li> <li>Data exploration to understand the features. </li> <li>Identifying key metrics for the problem based on ratio of target classes.</li> <li>Feature engineering and selection based on EDA.</li> <li>Setting up a framework for easier testing of multiple models even for peoples.</li> <li>Analysing results of models simply using MRI scans</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Research paper: Review of MRI-based Brain Tumor Image Segmentation Using Deep Learning Methods</li> <li>Public notebook: Brain Tumor Classification</li> </ul>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#model-architecture","title":"Model Architecture","text":"<pre><code>- The CNN architecture is designed to perform binary classification. The key layers used in the architecture are:\n- Convolutional Layers: For feature extraction from images. MaxPooling Layers: To downsample the image features. Dense Layers: To perform the classification. Dropout: For regularization to prevent overfitting.\n</code></pre>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#model-structure","title":"Model Structure","text":"<pre><code>- Input Layer 224*224 pixels.\n- Convolutionla layer followed by MaxPooling layers.\n- Flattern layer to convert feature into 1D vector\n- Fully connected layer for Classification.\n- Output Layer: Sigmoid activation for binary classification (tumor/no tumor)\n</code></pre>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Splitting of data (70% for training, 15% for validation and 15% for testing)</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Data preperation using Image Generator</li> <li>Categorical feature encoding</li> <li>Image resized to 224*224 pixels</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Combining original features based on domain knowledge</li> <li>Using MobileNet to process input</li> </ul> <p>Modeling</p> <ul> <li>Convolutional layer followed by MaxPooling layer</li> <li>Flattering the layer to convert features into 1D vector</li> <li>Sigmoid function to actiavte binary classification</li> <li>Holdout dataset created or model testing</li> <li>Using VGG16 and ResNet for future improvement</li> </ul> <p>Result analysis</p> <ul> <li>Hosted on Streamlit to ensure one can easily upload MRI scans and detect wether canncer is present or not.</li> <li>Early stopping to ensure better accuracy is achieved.</li> <li>Experiment with differnt agumentation techniques to improve model's robustness.</li> </ul>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Accuracy vs Validation_accuracy: The training accuracy is much higher than the validation accuracy after epoch 2, suggesting that the model may be overfitting the training data.</p> <ul> <li>Solution: It might be better to stop training around epoch 2 or 3 to avoid overfitting and ensure better generalization.</li> </ul>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/deep-learning/brain-tumor-detection-model/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>Early detection of cancer can lead to easily reduce the leading death of person and take proper medication on that basis.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned and implemented the concept of predicting probability and tuning the prediction threshold for more accurate results, compared to directly predicting with the default thresold for models.</li> </ul> Challenges faced and how they were overcome <ul> <li>Resigning the RGB image to grayscale and reducing the pixel by 225 * 225 was big challenge. </li> <li>Lesser dataset so we reached out to some hospitals which helped us collecting the MRI scans.</li> </ul>"},{"location":"projects/deep-learning/brain-tumor-detection-model/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <ul> <li>Doctors can identify the cancer stage and type accurately, allowing for tailored treatment approaches.</li> </ul> <ul> <li>Treatments at early stages are often less invasive and have fewer side effects compared to late-stage therapies</li> </ul>"},{"location":"projects/generative-adversarial-networks/","title":"Generative Adversarial Networks \ud83d\udcb1","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"projects/large-language-models/","title":"Large Language Models \ud83e\udd2a","text":"No Items Found <p>     There are no items available at this time. Check back again later.   </p>"},{"location":"projects/machine-learning/","title":"Machine Learning \ud83e\udd16","text":""},{"location":"projects/machine-learning/air-quality-prediction/","title":"Air Quality Prediction Using Machine Learning","text":""},{"location":"projects/machine-learning/air-quality-prediction/#aim","title":"AIM","text":"<p>The aim of this project is to predict air quality levels based on various features such as CO (Carbon Monoxide), NO (Nitrogen Oxides), NO2 (Nitrogen Dioxide), O3 (Ozone), and other environmental factors. By applying machine learning models, this project explores how different algorithms perform in predicting air quality and understanding the key factors that influence it.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#dataset-link","title":"DATASET LINK","text":"<p>Air Quality Dataset    Date       Time          CO(GT)     PT08.S1(CO)  NMHC(GT)    C6H6(GT)    PT08.S2(NMHC)   NOx(GT) PT08.S3(NOx)    NO2(GT) PT08.S4(NO2)    PT08.S5(O3) T   RH  AH  Unnamed: 15 Unnamed: 16 0   10/03/2004  18.00.00    2,6 1360.0  150.0         11,9     1046.0   166.0            1056.0 113.0            1692.0 1268.0      13,6    48,9    0,7578  NaN NaN 1   10/03/2004  19.00.00    2      1292.0   112.0         9,4      955.0       103.0             1174.0 92.0             1559.0 972.0         13,3  47,7    0,7255  NaN NaN 2   10/03/2004  20.00.00    2,2 1402.0  88.0          9,0      939.0       131.0             1140.0 114.0            1555.0 1074.0      11,9    54,0    0,7502  NaN NaN 3   10/03/2004  21.00.00    2,2 1376.0  80.0          9,2      948.0       172.0             1092.0 122.0            1584.0 1203.0      11,0    60,0    0,7867  NaN NaN 4   10/03/2004  22.00.00    1,6 1272.0  51.0          6,5      836.0       131.0             1205.0 116.0            1490.0 1110.0      11,2    59,6    0,7888  NaN NaN</p>"},{"location":"projects/machine-learning/air-quality-prediction/#notebook-link","title":"NOTEBOOK LINK","text":"<p>Air Quality Prediction Notebook</p>"},{"location":"projects/machine-learning/air-quality-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"<ul> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>seaborn</li> <li>scikit-learn</li> </ul>"},{"location":"projects/machine-learning/air-quality-prediction/#description","title":"DESCRIPTION","text":"<p>The project focuses on predicting air quality levels based on the features of air pollutants and environmental parameters. Air quality is a critical issue for human health, and accurate forecasting models can provide insights to policymakers and the public. The objective is to test various regression models to see which one gives the best predictions for CO (Carbon Monoxide) levels.</p> <p>What is the requirement of the project? - To accurately predict the CO levels based on environmental data.</p> <p>Why is it necessary? - Predicting air quality can help in early detection of air pollution and assist in controlling environmental factors effectively.</p> <p>How is it beneficial and used? - This model can be used by environmental agencies, city planners, and policymakers to predict and manage air pollution in urban areas, contributing to better public health outcomes.</p> <p>How did you start approaching this project? - Began by cleaning the dataset, handling missing data, and converting categorical features into numerical data. - After preparing the data, various machine learning models were trained and evaluated to identify the best-performing model.</p> <p>Mention any additional resources used: - Kaggle kernels and documentation for additional dataset understanding. - Tutorials on machine learning regression techniques, particularly for Random Forest, SVR, and Decision Trees.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"projects/machine-learning/air-quality-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<ul> <li>CO(GT): Carbon monoxide in the air (target feature).</li> <li>Date and Time: Record of data collection time.</li> <li>PT08.S1(CO), PT08.S2(NMHC), PT08.S3(NOX), PT08.S4(NO2), PT08.S5(O3): These are sensor readings for different gas pollutants.</li> <li>T, RH, AH: Temperature, Humidity, and Absolute Humidity respectively, recorded as environmental factors.</li> </ul>"},{"location":"projects/machine-learning/air-quality-prediction/#project-workflow","title":"PROJECT WORKFLOW","text":""},{"location":"projects/machine-learning/air-quality-prediction/#1-import-necessary-libraries","title":"1. Import Necessary Libraries","text":"<p>First, we import all the essential libraries needed for handling, analyzing, and modeling the dataset. This includes libraries like Pandas for data manipulation, Numpy for numerical computations, Matplotlib and Seaborn for data visualization, and Scikit-learn for machine learning models, evaluation, and data preprocessing. These libraries will enable us to perform all required tasks efficiently.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#2-load-dataset","title":"2. Load Dataset","text":"<p>We load the dataset using Pandas\u2019 <code>read_csv()</code> function. The dataset contains air quality data, which is loaded with a semicolon delimiter. After loading, we inspect the first few rows to understand the structure of the data and ensure that the dataset is correctly loaded. This helps in getting an initial insight into the dataset.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#3-data-cleaning-process","title":"3. Data Cleaning Process","text":"<p>Data cleaning is a crucial step in any project. In this step: - We remove unnamed columns that aren't useful for analysis (such as 'Unnamed: 15', 'Unnamed: 16'). - We correct data consistency issues, specifically replacing commas with periods in numeric columns to ensure the correct parsing of values. - Missing values in numeric columns are replaced with the mean of that respective column. - We eliminate rows that consist entirely of missing values (NaN). - A new datetime feature is created by combining the 'Date' and 'Time' columns. - Additional temporal features such as month, day, weekday, and hour are derived from the new datetime feature. - The original Date and Time columns are dropped as they are no longer needed.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#4-visualizing-correlations-between-features","title":"4. Visualizing Correlations Between Features","text":"<p>To understand relationships among the features, a heatmap is used to visualize correlations between all numeric columns. The heatmap highlights how features are correlated with each other, helping to identify possible redundancies or important predictors for the target variable.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#5-data-preparation-features-x-and-target-y","title":"5. Data Preparation - Features (X) and Target (y)","text":"<p>After cleaning the data, we separate the dataset into features (X) and the target variable (y): - Features (X): These are the columns used to predict the target value. We exclude the target variable column \u2018CO(GT)\u2019 and include all other columns as features. - Target (y): This is the variable we want to predict. We extract the 'CO(GT)' column and ensure all values are numeric. To prepare the data for machine learning, any non-numeric columns in the features (X) are encoded using <code>LabelEncoder</code>.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#6-split-the-data-into-training-and-test-sets","title":"6. Split the Data into Training and Test Sets","text":"<p>We split the dataset into training and testing sets, allocating 80% of the data for training and the remaining 20% for testing. This split allows us to evaluate model performance on unseen data and validate the effectiveness of the model.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#7-define-models","title":"7. Define Models","text":"<p>We define multiple regression models to train and evaluate on the dataset: - RandomForestRegressor: A robust ensemble method that performs well on non-linear datasets. - LinearRegression: A fundamental regression model, useful for establishing linear relationships. - SVR (Support Vector Regression): A regression model based on Support Vector Machines, useful for complex, non-linear relationships. - DecisionTreeRegressor: A decision tree-based model, capturing non-linear patterns and interactions.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#8-train-and-evaluate-each-model","title":"8. Train and Evaluate Each Model","text":"<p>Each model is trained on the training data and used to make predictions on the testing set. The performance is evaluated using two metrics: - Mean Absolute Error (MAE): Measures the average error between predicted and actual values. - R2 Score: Represents the proportion of the variance in the target variable that is predictable from the features.</p> <p>The evaluation metrics for each model are stored for comparison.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#9-visualizing-model-evaluation-metrics","title":"9. Visualizing Model Evaluation Metrics","text":"<p>We visualize the evaluation results for all models to get a comparative view of their performances. Two plots are generated: - Mean Absolute Error (MAE) for each model, showing how much deviation there is between predicted and actual values. - R2 Score, depicting the models' ability to explain the variability in the target variable. Higher R2 values indicate a better fit.</p> <p>These visualizations make it easy to compare model performances and understand which model is performing the best.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#10-conclusion-and-observations","title":"10. Conclusion and Observations","text":"<p>In this final step, we summarize the results and draw conclusions based on the evaluation metrics. We discuss which model achieved the best performance in terms of both MAE and R2 Score, along with insights from the data cleaning and feature engineering steps. Key observations include the importance of feature selection, the efficacy of different models for regression tasks, and which model has the most accurate predictions based on the dataset at hand.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#this-write-up-outlines-the-key-steps-in-the-process-according-to-the-code-provided","title":"This write-up outlines the key steps in the process according to the code provided.","text":""},{"location":"projects/machine-learning/air-quality-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <ul> <li>Trade-off: Choosing between model accuracy and training time.<ul> <li>Solution: Random Forest was chosen due to its balance between accuracy and efficiency, with SVR considered for its powerful predictive power despite longer training time.</li> </ul> </li> </ul> Trade Off 2 <ul> <li>Trade-off: Model interpretability vs complexity.<ul> <li>Solution: Decision trees were avoided in favor of Random Forest, which tends to be more robust in dealing with complex data and prevents overfitting.</li> </ul> </li> </ul>"},{"location":"projects/machine-learning/air-quality-prediction/#screenshots","title":"SCREENSHOTS","text":""},{"location":"projects/machine-learning/air-quality-prediction/#project-workflow_1","title":"PROJECT WORKFLOW","text":"<pre><code>  graph LR\n    A[Start] --&gt; B{Is data clean?};\n    B --&gt;|Yes| C[Explore Data];\n    C --&gt; D[Data Preprocessing];\n    D --&gt; E[Feature Selection &amp; Engineering];\n    E --&gt; F[Split Data into Training &amp; Test Sets];\n    F --&gt; G[Define Models];\n    G --&gt; H[Train and Evaluate Models];\n    H --&gt; I[Visualize Evaluation Metrics];\n    I --&gt; J[Model Testing];\n    J --&gt; K[Conclusion and Observations];\n    B ----&gt;|No| L[Clean Data];\n\n</code></pre>"},{"location":"projects/machine-learning/air-quality-prediction/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Mean Absolute Error (MAE) R2 Score Random Forest Regressor 1.2391 0.885 Linear Regression 1.4592 0.82 SVR 1.3210 0.843 Decision Tree Regressor 1.5138 0.755"},{"location":"projects/machine-learning/air-quality-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/machine-learning/air-quality-prediction/#key-learnings","title":"KEY LEARNINGS","text":"<ul> <li>Learned how different machine learning models perform on real-world data and gained insights into their strengths and weaknesses.</li> <li>Understood the significance of feature engineering and preprocessing to achieve better model performance.</li> </ul> <p>Insights gained from the data: - Data had missing values that required filling. - Feature creation from datetime led to better prediction accuracy.</p> <p>Improvements in understanding machine learning concepts: - Learned how to effectively implement and optimize machine learning models using libraries like scikit-learn.</p> <p>Challenges faced and how they were overcome: - Handling missing data effectively and ensuring the data preprocessing did not lose valuable information.</p>"},{"location":"projects/machine-learning/air-quality-prediction/#use-cases","title":"USE CASES","text":"Application 1 <p>Predicting Air Quality in Urban Areas    - Local governments can use this model to predict air pollution levels and take early actions to reduce pollution in cities.</p> Application 2 <p>Predicting Seasonal Air Pollution Levels    - The model can help forecast air quality during different times of the year, assisting in long-term policy planning.</p>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/","title":"Cardiovascular Disease Prediction","text":""},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#aim","title":"AIM","text":"<p>To predict the risk of cardiovascular disease based on lifestyle factors.</p>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/alphiree/cardiovascular-diseases-risk-prediction-dataset</p>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/sid4ds/cardiovascular-disease-risk-prediction</p>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 for TunedThresholdClassifierCV)</li> <li>matplotlib</li> <li>seaborn</li> <li>joblib</li> </ul>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>This project aims to predict the risk of cardivascular diseases (CVD) based on data provided by people about their lifestyle factors. Predicting the risk in advance can minimize cases which reach a terminal stage.</li> </ul> Why is it necessary? <ul> <li>CVD is one of the leading causes of death globally. Using machine learning models to predict risk of CVD can be an important tool in helping the people affected by it.</li> </ul> How is it beneficial and used? <ul> <li>Doctors can use it as a second opinion to support their diagnosis. It also acts as a fallback mechanism in rare cases where the diagnosis is not obvious.</li> <li>People (patients in particular) can track their risk of CVD based on their own lifestyle and schedule an appointment with a doctor in advance to mitigate the risk.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Going through previous research and articles related to the problem.</li> <li>Data exploration to understand the features. Using data visualization to check their distributions.</li> <li>Identifying key metrics for the problem based on ratio of target classes.</li> <li>Feature engineering and selection based on EDA.</li> <li>Setting up a framework for easier testing of multiple models.</li> <li>Analysing results of models using confusion matrix.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Research paper: Integrated Machine Learning Model for Comprehensive Heart Disease Risk Assessment Based on Multi-Dimensional Health Factors</li> <li>Public notebook: Cardiovascular-Diseases-Risk-Prediction</li> </ul>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range General_Health \"Would you say that in general your health is\u2014\" Categorical [Poor, Fair, Good, Very Good, Excellent] Checkup \"About how long has it been since you last visited a doctor for a routine checkup?\" Categorical [Never, 5 or more years ago, Within last 5 years, Within last 2 years, Within the last year] Exercise \"Did you participate in any physical activities like running, walking, or gardening?\" Categorical [Yes, No] Skin_Cancer Respondents that reported having skin cancer Categorical [Yes, No] Other_Cancer Respondents that reported having any other types of cancer Categorical [Yes, No] Depression Respondents that reported having a depressive disorder Categorical [Yes, No] Diabetes Respondents that reported having diabetes. If yes, specify the type. Categorical [Yes, No, No pre-diabetes or borderline diabetes, Yes but female told only during pregnancy] Arthritis Respondents that reported having arthritis Categorical [Yes, No] Sex Respondent's gender Categorical [Yes, No] Age_Category Respondent's age range Categorical ['18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-80', '80+'] Height_(cm) Respondent's height in cm Numerical Measured in cm Weight_(kg) Respondent's weight in kg Numerical Measured in kg BMI Respondent's Body Mass Index in kg/cm\u00b2 Numerical Measured in kg/cm\u00b2 Smoking_History Respondent's smoking history Categorical [Yes, No] Alcohol_Consumption Number of days of alcohol consumption in a month Numerical Integer values Fruit_Consumption Number of servings of fruit consumed in a month Numerical Integer values Green_Vegetables_Consumption Number of servings of green vegetables consumed in a month Numerical Integer values FriedPotato_Consumption Number of servings of fried potato consumed in a month Numerical Integer values"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Regrouping rare categories</li> <li>Categorical feature encoding</li> <li>Outlier clipping for numerical features</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Combining original features based on domain knowledge</li> <li>Discretizing numerical features</li> </ul> <p>Modeling</p> <ul> <li>Holdout dataset created or model testing</li> <li>Models trained: Logistic Regression, Decision Tree, Random Forest, AdaBoost, HistGradient Boosting, Multi-Layer Perceptron</li> <li>Class imbalance handled through:</li> <li>Class weights, when supported by model architecture</li> <li>Threshold tuning using TunedThresholdClassifierCV</li> <li>Metric for model-tuning: F2-score (harmonic weighted mean of precision and recall, with twice the weightage for recall)</li> </ul> <p>Result analysis</p> <ul> <li>Confusion matrix using predictions made on holdout test set</li> </ul>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Accuracy vs Recall: Data is extremely imbalanced, with only ~8% representing the positive class. This makes accuracy unsuitable as a metric for our problem. It is critical to correctly predict all the positive samples, due to which we must focus on recall. However, this lowers the overall accuracy since some negative samples may be predicted as positive.</p> <ul> <li>Solution: Prediction threshold for models is tuned using F2-score to create a balance between precision and recall, with more importance given to recall. This maintains overall accuracy at an acceptable level while boosting recall.</li> </ul>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Numerical feature distributions Height_(cm)Weight_(kg)BMIAlcoholFruitVegetableFried Patato <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> Correlations PearsonSpearman's RankKendall-Tau <p></p> <p></p> <p></p>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#models-used-and-their-accuracies","title":"MODELS USED AND THEIR ACCURACIES","text":"Model + Feature set Accuracy (%) Recall (%) Logistic Regression + Original 76.29 74.21 Logistic Regression + Extended 76.27 74.41 Logistic Regression + Selected 72.66 78.09 Decision Tree + Original 72.76 78.61 Decision Tree + Extended 74.09 76.69 Decision Tree + Selected 75.52 73.61 Random Forest + Original 73.97 77.33 Random Forest + Extended 74.10 76.61 Random Forest + Selected 74.80 74.05 AdaBoost + Original 76.03 74.49 AdaBoost + Extended 74.99 76.25 AdaBoost + Selected 74.76 75.33 Multi-Layer Perceptron + Original 76.91 72.81 Multi-Layer Perceptron + Extended 73.26 79.01 Multi-Layer Perceptron + Selected 74.86 75.05 Hist-Gradient Boosting + Original 75.98 73.49 Hist-Gradient Boosting + Extended 75.63 74.73 Hist-Gradient Boosting + Selected 74.40 75.85"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Logistic Regression</p> LR OriginalLR ExtendedLR Selected <p></p> <p></p> <p></p> Decision Tree DT OriginalDT ExtendedDT Selected <p></p> <p></p> <p></p> Random Forest RF OriginalRF ExtendedRF Selected <p></p> <p></p> <p></p> Ada Boost AB OriginalAB ExtendedAB Selected <p></p> <p></p> <p></p> Multi-Layer Perceptron MLP OriginalMLP ExtendedMLP Selected <p></p> <p></p> <p></p> Hist-Gradient Boosting HGB OriginalHGB ExtendedHGB Selected <p></p> <p></p> <p></p>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>General Health, Age and Co-morbities (such as Diabetes &amp; Arthritis) are the most indicative features for CVD risk.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned and implemented the concept of predicting probability and tuning the prediction threshold for more accurate results, compared to directly predicting with the default thresold for models.</li> </ul> Challenges faced and how they were overcome <ul> <li>Deciding the correct metric for evaluation of models due to imbalanced nature of the dataset. Since positive class is more important, Recall was used as the final metric for ranking models. </li> <li>F2-score was used to tune the threshold for models to maintain a balance between precision and recall, thereby maintaining overall accuracy.</li> </ul>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <ul> <li>Doctors can use it as a second opinion when assessing a new patient. Model trained on cases from previous patients can be used to predict the risk.</li> </ul> <ul> <li>People (patients in particular) can use this tool to track the risk of CVD based on their own lifestyle factors and take preventive measures when the risk is high.</li> </ul>"},{"location":"projects/machine-learning/cardiovascular-disease-prediction/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Different implementations of gradient-boosting models such as XGBoost, CatBoost, LightGBM, etc. were not implemented since none of the tree ensemble models such as Random Forest, AdaBoost or Hist-Gradient Boosting were among the best performers. Hence, avoid additional dependencies based on such models.</li> </ul>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/","title":"Health Insurance Cross-Sell Prediction","text":""},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#aim","title":"AIM","text":"<p>To predict whether a Health Insurance customer would be interested in buying Vehicle Insurance.</p>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction</p>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/sid4ds/insurance-cross-sell-prediction-eda-modeling</p>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 for TunedThresholdClassifierCV)</li> <li>xgboost</li> <li>catboost</li> <li>lightgbm</li> <li>matplotlib</li> <li>seaborn</li> <li>joblib</li> </ul>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#description","title":"DESCRIPTION","text":"<p>Why is it necessary?</p> <ul> <li>This project aims to predict the chances of cross-selling Vehicle insurance to existing Health insurance customers. This would be extremely helpful for companies because they can then accordingly plan communication strategy to reach out to those customers and optimise their business model and revenue.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Going through previous research and articles related to the problem.</li> <li>Data exploration to understand the features. Using data visualization to check their distributions.</li> <li>Identifying key metrics for the problem based on ratio of target classes - ROC-AUC &amp; Matthew's Correlation Coefficient (MCC) instead of Accuracy.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Feature Engineering: Tutorial notebook</li> <li>Public notebook: Vehicle Insurance EDA and boosting models</li> </ul>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range id Unique ID for the customer Numerical Unique numerical values Gender Binary gender of the customer Binary [0: Male, 1: Female] (or other binary representations as applicable) Age Numerical age of the customer Numerical Measured in years Driving_License Indicates if the customer has a Driving License Binary [0: No, 1: Yes] Region_Code Unique code for the customer's region Numerical Unique numerical values Previously_Insured Indicates if the customer already has Vehicle Insurance Binary [0: No, 1: Yes] Vehicle_Age Age of the vehicle categorized as ordinal values Categorical [&lt; 1 year, 1-2 years, &gt; 2 years] Vehicle_Damage Indicates if the vehicle was damaged in the past Binary [0: No, 1: Yes] Annual_Premium Amount to be paid as premium over the year Numerical Measured in currency Policy_Sales_Channel Anonymized code for the channel of customer outreach Numerical Unique numerical codes representing various channels Vintage Number of days the customer has been associated with the company Numerical Measured in days"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Removing duplicates</li> <li>Categorical feature encoding</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Discretizing numerical features</li> <li>Feature selection based on model-based feature importances and statistical tests.</li> </ul> <p>Modeling</p> <ul> <li>Holdout dataset created or model testing</li> <li>Setting up a framework for easier testing of multiple models.</li> <li>Models trained: Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive-Bayes, Decision Tree, Random Forest, AdaBoost, Multi-Layer Perceptron, XGBoost, CatBoost, LightGBM</li> <li>Class imbalance handled through:</li> <li>Class weights, when supported by model architecture</li> <li>Threshold tuning using TunedThresholdClassifierCV</li> <li>Metric for model-tuning: F1-score (harmonic weighted mean of precision and recall)</li> </ul> <p>Result analysis</p> <ul> <li>Predictions made on holdout test set</li> <li>Models compared based on classification report and chosen metrics: ROC-AUC and MCC.</li> </ul>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Accuracy vs Recall &amp; Precision</p> <p>Data is heavily imbalanced, with only ~12% representing the positive class. This makes accuracy unsuitable as a metric for our problem. Our goal is to correctly predict all the positive samples, due to which we must focus on recall. However, this lowers the overall accuracy since some negative samples may be predicted as positive.</p> <ul> <li>Solution: Prediction threshold for models is tuned using F1-score to create a balance between precision and recall. This maintains overall accuracy at an acceptable level while boosting recall.</li> </ul>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Feature distributions (Univariate Analysis) AgeLicenseRegion CodePreviously InsuredVehical AgeVehical DamageAnnual PremiumPolicy ChannelVintage <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> Engineered Features Age GroupPolicy Group <p></p> <p></p> Feature Distributions (Bivariate Analysis) Pair PlotsSpearman-Rank CorrelationPoint-Biserial CorrelationTetrachoric Correlation <p></p> <p></p> <p></p> <p></p> Feature Selection Point-Biserial CorrelationANOVA F-TestTetrachoric CorrelationChi-Squared Test of IndependenceMutual InformationXGBoost Feature ImportancesExtra Trees Feature Importances <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#models-used-and-their-performance","title":"MODELS USED AND THEIR PERFORMANCE","text":"<pre><code>Best threshold after threshold tuning is also mentioned.\n</code></pre> Model + Feature set ROC-AUC MCC Best threshold Logistic Regression + Original 0.8336 0.3671 0.65 Logistic Regression + Extended 0.8456 0.3821 0.66 Logistic Regression + Reduced 0.8455 0.3792 0.67 Logistic Regression + Minimal 0.8177 0.3507 0.60 Linear DA + Original 0.8326 0.3584 0.19 Linear DA + Extended 0.8423 0.3785 0.18 Linear DA + Reduced 0.8421 0.3768 0.18 Linear DA + Minimal 0.8185 0.3473 0.15 Quadratic DA + Original 0.8353 0.3779 0.45 Quadratic DA + Extended 0.8418 0.3793 0.54 Quadratic DA + Reduced 0.8422 0.3807 0.44 Quadratic DA + Minimal 0.8212 0.3587 0.28 Gaussian Naive Bayes + Original 0.8230 0.3879 0.78 Gaussian Naive Bayes + Extended 0.8242 0.3914 0.13 Gaussian Naive Bayes + Reduced 0.8240 0.3908 0.08 Gaussian Naive Bayes + Minimal 0.8055 0.3605 0.15 K-Nearest Neighbors + Original 0.7819 0.3461 0.09 K-Nearest Neighbors + Extended 0.7825 0.3469 0.09 K-Nearest Neighbors + Reduced 0.7710 0.3405 0.01 K-Nearest Neighbors + Minimal 0.7561 0.3201 0.01 Decision Tree + Original 0.8420 0.3925 0.67 Decision Tree + Extended 0.8420 0.3925 0.67 Decision Tree + Reduced 0.8419 0.3925 0.67 Decision Tree + Minimal 0.8262 0.3683 0.63 Random Forest + Original 0.8505 0.3824 0.70 Random Forest + Extended 0.8508 0.3832 0.70 Random Forest + Reduced 0.8508 0.3820 0.71 Random Forest + Minimal 0.8375 0.3721 0.66 Extra-Trees + Original 0.8459 0.3770 0.70 Extra-Trees + Extended 0.8504 0.3847 0.71 Extra-Trees + Reduced 0.8515 0.3836 0.72 Extra-Trees + Minimal 0.8337 0.3682 0.67 AdaBoost + Original 0.8394 0.3894 0.83 AdaBoost + Extended 0.8394 0.3894 0.83 AdaBoost + Reduced 0.8404 0.3839 0.84 AdaBoost + Minimal 0.8269 0.3643 0.86 Multi-Layer Perceptron + Original 0.8512 0.3899 0.22 Multi-Layer Perceptron + Extended 0.8528 0.3865 0.23 Multi-Layer Perceptron + Reduced 0.8517 0.3892 0.23 Multi-Layer Perceptron + Minimal 0.8365 0.3663 0.21 XGBoost + Original 0.8585 0.3980 0.68 XGBoost + Extended 0.8585 0.3980 0.68 XGBoost + Reduced 0.8584 0.3967 0.68 XGBoost + Minimal 0.8459 0.3765 0.66 CatBoost + Original 0.8579 0.3951 0.46 CatBoost + Extended 0.8578 0.3981 0.45 CatBoost + Reduced 0.8577 0.3975 0.45 CatBoost + Minimal 0.8449 0.3781 0.42 LightGBM + Original 0.8587 0.3978 0.67 LightGBM + Extended 0.8587 0.3976 0.67 LightGBM + Reduced 0.8587 0.3983 0.67 LightGBM + Minimal 0.8462 0.3753 0.66"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ol> <li>Previously_Insured, Vehicle_Damage, Policy_Sales_Channel and Age are the most informative features for predicting cross-sell probability.</li> <li>Vintage and Driving_License have no predictive power. They are not included in the best model.</li> </ol> Improvements in understanding machine learning concepts <ol> <li>Implemented threshold-tuning for more accurate results.</li> <li>Researched and utilized statistical tests for feature selection.</li> </ol> Challenges faced and how they were overcome <ol> <li>Shortlisting the apropriate statistical test for bivariate analysis and feature selection.</li> <li>Deciding the correct metric for evaluation of models due to imbalanced nature of the dataset.</li> <li>F1-score was used for threshold-tuning. ROC-AUC score and MCC were used for model comparison.</li> </ol>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1 <ul> <li>Companies can use customer data to predict which customers to target for cross-sell marketing. This saves cost and effort for the company, and protects uninterested customers from unnecessary marketing calls.</li> </ul>"},{"location":"projects/machine-learning/health-insurance-cross-sell-prediction/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Complex model-ensembling through stacking or hill-climbing was not implemented due to significantly longer training time.</li> </ul>"},{"location":"projects/machine-learning/heart-disease-detection-model/","title":"Heart Disease Detection Model","text":""},{"location":"projects/machine-learning/heart-disease-detection-model/#aim","title":"AIM","text":"<p>The aim of this project is to develop a reliable and efficient machine learning-based system for the early detection and diagnosis of heart disease. By leveraging advanced algorithms, the system seeks to analyze patient data, identify significant patterns, and predict the likelihood of heart disease, thereby assisting healthcare professionals in making informed decisions.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#dataset-link","title":"DATASET LINK","text":"<p>This project uses a publicly available heart disease dataset from UCI Machine Learning Repository</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#notebook-link","title":"NOTEBOOK LINK","text":"<p>This is notebook of the following project Kaggle</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#libraries-needed","title":"LIBRARIES NEEDED","text":"<pre><code>- pandas\n- numpy\n- scikit-learn\n- matplotlib\n- seaborn\n</code></pre>"},{"location":"projects/machine-learning/heart-disease-detection-model/#description","title":"DESCRIPTION","text":"<p>what is the requirement of the project?,  The project requires a dataset containing patient health records, including attributes like age, cholesterol levels, blood pressure, and medical history. Additionally, it needs machine learning tools and frameworks (e.g., Python, scikit-learn) for building and evaluating predictive models.</p> <p>why is it necessary?,  Early detection of heart disease is crucial to prevent severe complications and reduce mortality rates. A machine learning-based system provides accurate, fast, and cost-effective predictions, aiding timely medical intervention and improved patient outcomes.</p> <p>how is it beneficial and used?,  This system benefits healthcare by improving diagnostic accuracy and reducing reliance on invasive procedures. It can be used by doctors for decision support, by patients for risk assessment, and in hospitals for proactive healthcare management.</p> <p>how did you start approaching this project?,  The project begins by collecting and preprocessing a heart disease dataset, ensuring it is clean and ready for analysis. Next, machine learning models are selected, trained, and evaluated to identify the most accurate algorithm for predicting heart disease.</p> <p>Any additional resources used like blogs reading, books reading (mention the name of book along with the pages you have read)? Kaggle kernels and documentation for additional dataset understanding. Tutorials on machine learning regression techniques, particularly for Random Forest, SVR, and Decision Trees.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#explanation","title":"EXPLANATION","text":""},{"location":"projects/machine-learning/heart-disease-detection-model/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<p>Age: Patient's age in years.</p> <p>Sex: Gender of the patient (1 = male; 0 = female).</p> <p>Chest Pain Type (cp): Categorized as:</p> <p>0: Typical angina 1: Atypical angina 2: Non-anginal pain 3: Asymptomatic Resting Blood Pressure (trestbps): Measured in mm Hg upon hospital admission.</p> <p>Serum Cholesterol (chol): Measured in mg/dL.</p> <p>Fasting Blood Sugar (fbs): Indicates if fasting blood sugar &gt; 120 mg/dL (1 = true; 0 = false).</p> <p>Resting Electrocardiographic Results (restecg):</p> <p>0: Normal 1: Having ST-T wave abnormality (e.g., T wave inversions and/or ST elevation or depression &gt; 0.05 mV) 2: Showing probable or definite left ventricular hypertrophy by Estes' criteria Maximum Heart Rate Achieved (thalach): Peak heart rate during exercise.</p> <p>Exercise-Induced Angina (exang): Presence of angina induced by exercise (1 = yes; 0 = no).</p> <p>Oldpeak: ST depression induced by exercise relative to rest.</p> <p>Slope of the Peak Exercise ST Segment (slope):</p> <p>0: Upsloping 1: Flat 2: Downsloping Number of Major Vessels Colored by Fluoroscopy (ca): Ranges from 0 to 3.</p> <p>Thalassemia (thal):</p> <p>1: Normal 2: Fixed defect 3: Reversible defect Target: Diagnosis of heart disease (0 = no disease; 1 = disease).</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#project-workflow","title":"PROJECT WORKFLOW","text":""},{"location":"projects/machine-learning/heart-disease-detection-model/#1problem-definition","title":"1.Problem Definition","text":"<p>Identify the objective: To predict the presence or absence of heart disease based on patient data. Define the outcome variable (target) and input features.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#2data-collection","title":"2.Data Collection","text":"<p>Gather a reliable dataset, such as the Cleveland Heart Disease dataset, which includes features relevant to heart disease prediction.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#3data-preprocessing","title":"3.Data Preprocessing","text":"<p>Handle missing values: Fill or remove records with missing data. Normalize/standardize data to ensure all features have comparable scales. Encode categorical variables like sex, cp, and thal using techniques like one-hot encoding or label encoding.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#4exploratory-data-analysis-eda","title":"4.Exploratory Data Analysis (EDA)","text":"<p>Visualize data distributions using histograms, boxplots, or density plots. Identify relationships between features using correlation matrices and scatterplots. Detect and handle outliers to improve model performance.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#5feature-selection","title":"5.Feature Selection","text":"<p>Use statistical methods or feature importance metrics to identify the most relevant features for prediction. Remove redundant or less significant features.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#6data-splitting","title":"6.Data Splitting","text":"<p>Divide the dataset into training, validation, and testing sets (e.g., 70%-15%-15%). Ensure a balanced distribution of the target variable in all splits.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#7model-selection","title":"7.Model Selection","text":"<p>Experiment with multiple machine learning algorithms such as Logistic Regression, Random Forest, Decision Trees, Support Vector Machines (SVM), and Neural Networks. Select models based on the complexity and nature of the dataset.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#8model-training","title":"8.Model Training","text":"<p>Train the chosen models using the training dataset. Tune hyperparameters using grid search or random search techniques.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#9model-evaluation","title":"9.Model Evaluation","text":"<p>Assess models on validation and testing datasets using metrics such as: Accuracy Precision, Recall, and F1-score Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC). Compare models to identify the best-performing one.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#10deployment-and-prediction","title":"10.##Deployment and Prediction","text":"<p>Save the trained model using frameworks like joblib or pickle. Develop a user interface (UI) or API for end-users to input data and receive predictions.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#11iterative-improvement","title":"11.Iterative Improvement","text":"<p>Continuously refine the model using new data or advanced algorithms. Address feedback and optimize the system based on real-world performance.</p>"},{"location":"projects/machine-learning/heart-disease-detection-model/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. Interpretability</li> <li>Complex models like Random Forests or Neural Networks offer higher accuracy but are less interpretable compared to simpler models like Logistic Regression.</li> </ul> <ul> <li>Overfitting vs. Generalization</li> <li>Models with high complexity may overfit the training data, leading to poor generalization on unseen data.</li> </ul>"},{"location":"projects/machine-learning/heart-disease-detection-model/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Score Logistic regression 88% K-Nearest Classifier 68% Random Forest Classifier 86%"},{"location":"projects/machine-learning/heart-disease-detection-model/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/machine-learning/heart-disease-detection-model/#key-learnings","title":"KEY LEARNINGS","text":"<ol> <li> <p>Data Insights Understanding Healthcare Data: Learned how medical attributes (e.g., age, cholesterol, chest pain type) influence heart disease risk. Data Imbalance: Recognized the challenges posed by imbalanced datasets and explored techniques like SMOTE and class weighting to address them. Importance of Preprocessing: Gained expertise in handling missing values, scaling data, and encoding categorical variables, which are crucial for model performance.</p> </li> <li> <p>Techniques Mastered Exploratory Data Analysis (EDA): Applied visualization tools (e.g., histograms, boxplots, heatmaps) to uncover patterns and correlations in data. Feature Engineering: Identified and prioritized key features using statistical methods and feature importance metrics. Modeling: Implemented various machine learning algorithms, including Logistic Regression, Random Forest, Gradient Boosting, and Support Vector Machines. Evaluation Metrics: Learned to evaluate models using metrics like Precision, Recall, F1-score, and ROC-AUC to optimize for healthcare-specific goals. Hyperparameter Tuning: Used grid search and random search to optimize model parameters and improve performance. Interpretability Tools: Utilized SHAP and feature importance analysis to explain model predictions.</p> </li> <li> <p>Skills Developed Problem-Solving: Addressed trade-offs such as accuracy vs. interpretability, and overfitting vs. generalization. Critical Thinking: Improved decision-making on model selection, preprocessing methods, and evaluation strategies. Programming: Strengthened Python programming skills, including the use of libraries like scikit-learn, pandas, matplotlib, and TensorFlow. Collaboration: Enhanced communication and teamwork when discussing medical insights and technical challenges with domain experts. Time Management: Balanced experimentation with computational efficiency, focusing on techniques that maximized impact. Ethical Considerations: Gained awareness of ethical issues like ensuring fairness in predictions and minimizing false negatives, which are critical in healthcare applications.</p> </li> <li> <p>Broader Understanding Interdisciplinary Knowledge: Combined expertise from data science, healthcare, and statistics to create a meaningful application. Real-World Challenges: Understood the complexities of translating machine learning models into practical tools for healthcare. Continuous Learning: Learned that model development is iterative, requiring continuous refinement based on feedback and new data. </p> </li> </ol>"},{"location":"projects/machine-learning/heart-disease-detection-model/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Clinical Decision Support Systems (CDSS)</p> <ul> <li>ML models can be integrated into Electronic Health Record (EHR) systems to assist doctors in diagnosing heart disease. The model can provide predictions based on patient data, helping clinicians make faster and more accurate decisions.</li> </ul> <p>Early Screening and Risk Assessment</p> <ul> <li>Patients can undergo routine screening using a heart disease detection system to assess their risk level. The system can predict whether a patient is at high, moderate, or low risk, prompting early interventions or lifestyle changes.</li> </ul>"},{"location":"projects/machine-learning/poker-hand-prediction/","title":"Poker Hand Prediction","text":""},{"location":"projects/machine-learning/poker-hand-prediction/#aim","title":"AIM","text":"<p>The aim of this project is to develop a machine learning model using a Multi-Layer Perceptron (MLP) classifier to accurately classify different types of poker hands based on the suit and rank of five cards.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/dysphoria/poker-hand-classification</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/supratikbhowal/poker-hand-prediction-model</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"<ul> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>seaborn</li> <li>scikit-learn</li> </ul>"},{"location":"projects/machine-learning/poker-hand-prediction/#description","title":"DESCRIPTION","text":"<p>This project involves building a classification model to predict poker hands using a Multi-Layer Perceptron (MLP) classifier. The dataset consists of features representing the suit and rank of five cards, with the target variable being the type of poker hand (e.g., one pair, two pair, royal flush). The model is trained on a standardized dataset, with class weights computed to address class imbalance. Performance is evaluated using metrics such as accuracy, classification report, confusion matrix, prediction error, ROC curve, and AUC, providing a comprehensive analysis of the model's effectiveness in classifying poker hands.</p> <p>What is the requirement of the project?  - To accurately predict the Poker Hand type.</p> <p>Why is it necessary?  - The project demonstrates how machine learning can solve structured data problems, bridging the gap between theoretical knowledge and practical implementation.</p> <p>How is it beneficial and used?  - The project automates the classification of poker hands, enabling players to quickly and accurately identify the type of hand they have, such as a straight, flush, or royal flush, without manual effort.  - By understanding the probabilities and patterns of certain hands appearing, players can make informed decisions on whether to bet, raise, or fold, improving their gameplay strategy.</p> <p>How did you start approaching this project?  - Analyzed the poker hand classification problem, reviewed the dataset structure (suits, ranks, and hand types), and identified the multi-class nature of the target variable. Studied the class imbalance issue and planned data preprocessing steps, including scaling and class weight computation.  - Chose the Multi-Layer Perceptron (MLP) Classifier for its capability to handle complex patterns in data. Defined model hyperparameters, trained the model using standardized features, and evaluated its performance using metrics like accuracy, ROC-AUC, and confusion matrix</p> <p>Mention any additional resources used:  - Kaggle kernels and documentation for additional dataset understanding.  - Tutorials on machine learning regression techniques, particularly for MLP</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"projects/machine-learning/poker-hand-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range S1 Suit of card #1 Ordinal (1-4) representing C1 Rank of card #1 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S2 Suit of card #2 Ordinal (1-4) representing C2 Rank of card #2 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S3 Suit of card #3 Ordinal (1-4) representing C3 Rank of card #3 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S4 Suit of card #4 Ordinal (1-4) representing C4 Rank of card #4 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S5 Suit of card #5 Ordinal (1-4) representing C5 Rank of card #5 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) Poker Hand Type of Card in Hand Ordinal (0-9) types* <p>Poker Hands  0: Nothing in hand, not a recognized poker hand  1: One pair, one pair of equal ranks within five cards  2: Two pairs, two pairs of equal ranks within five cards  3: Three of a kind, three equal ranks within five cards  4: Straight, five cards, sequentially ranked with no gaps  5: Flush, five cards with the same suit  6: Full house, pair + different rank three of a kind  7: Four of a kind, four equal ranks within five cards  8: Straight flush, straight + flush  9: Royal flush, {Ace, King, Queen, Jack, Ten} + flush </p>"},{"location":"projects/machine-learning/poker-hand-prediction/#project-workflow","title":"PROJECT WORKFLOW","text":""},{"location":"projects/machine-learning/poker-hand-prediction/#1-dataset-loading-and-inspection","title":"1. Dataset Loading and Inspection","text":"<p>The dataset is loaded into the environment, and its structure, features, and target classes are analyzed. Column names are assigned to enhance clarity and understanding.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#2-data-preprocessing","title":"2. Data Preprocessing","text":"<p>Target variables are mapped to descriptive hand labels, features and target variables are separated, and data is standardized using 'StandardScaler' for uniform scaling.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#3-handling-class-imbalance","title":"3. Handling Class Imbalance","text":"<p>Class weights are computed using 'compute_class_weight' to address the imbalance in target classes, ensuring fair training for all hand types.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#4-model-selection-and-training","title":"4. Model Selection and Training","text":"<p>The MLPClassifier is selected for its capability to handle multi-class problems, configured with suitable hyperparameters, and trained on the preprocessed training data.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#5-model-evaluation","title":"5. Model Evaluation","text":"<p>The model's performance is assessed using metrics such as accuracy, classification reports, and confusion matrices. Predictions are generated for the test dataset.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#6-visualization-and-error-analysis","title":"6. Visualization and Error Analysis","text":"<p>Class prediction errors are visualized using bar charts, and ROC curves with AUC values are generated for each hand type to evaluate classification effectiveness.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#7-insights-and-interpretation","title":"7. Insights and Interpretation","text":"<p>Strengths and weaknesses of the model are identified through error analysis, and the findings are presented using visual tools like heatmaps for better understanding.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#8-conclusion","title":"8. Conclusion","text":"<p>The project outcomes are summarized, practical applications are discussed, and suggestions for further research or improvements are proposed.</p>"},{"location":"projects/machine-learning/poker-hand-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <ul> <li>Trade-off: Accuracy vs. Complexity.<ul> <li>Solution: Using a Multi-Layer Perceptron (MLP) introduces complexity due to its hidden layers and numerous parameters. While this improves accuracy, it requires more computational resources and training time compared to simpler models like decision trees or logistic regression.</li> </ul> </li> </ul> Trade Off 2 <ul> <li>Trade-off: Bias vs. Variance<ul> <li>Solution: The project's hyperparameter tuning (e.g., learning rate, number of hidden layers) aims to reduce bias and variance. However, achieving a perfect balance is a trade-off, as overly complex models may increase variance (overfitting), while overly simplified models may increase bias (underfitting).</li> </ul> </li> </ul> Trade Off 3 <ul> <li>Trade-off: Generalization vs. Overfitting<ul> <li>Solution:The model's flexibility with complex hyperparameters (e.g., hidden layers, activation functions) risks overfitting, especially on small or imbalanced datasets. Regularization techniques like adjusting 'alpha' help mitigate this but may compromise some accuracy.</li> </ul> </li> </ul>"},{"location":"projects/machine-learning/poker-hand-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Is data clean?};\n    B --&gt;|Yes| C[Explore Data];\n    C --&gt; D[Data Preprocessing];\n    D --&gt; E[Define Models];\n    E --&gt; F[Train the Model];\n    F --&gt; G[Evaluate Performance];\n    G --&gt; H[Visualize Evaluation Metrics];\n    H --&gt; I[Model Testing];\n    I --&gt; J[Conclusion and Observations];\n    B ----&gt;|No| K[Clean Data];\n    K --&gt; C;</code></pre> Model Evaluation Metrics Model vs AccuracyROC Curve &amp; AUC ScoreClassification Report Heatmap <p></p> <p></p> <p></p>"},{"location":"projects/machine-learning/poker-hand-prediction/#models-evaluation-metrics","title":"MODELS EVALUATION METRICS","text":"Poker Hand precision recall f1-score support flush 0.52 0.07 0.12 1996 four_of_a_kind 0.00 0.00 0.00 230 full_house 0.77 0.34 0.47 1424 one_pair 1.00 1.00 1.00 422498 royal_flush 0.00 0.00 0.00 3 straight 0.96 0.44 0.60 3885 straight_flush 0.00 0.00 0.00 12 three_of_a_kind 0.95 0.00 0.00 21121 two_pair 1.00 1.00 1.00 47622 zilch 0.99 1.00 1.00 501209 Model Accuracy Random Forest Regression 0.6190 Gradient Boosting Regression 0.3204 MLP Classifier 0.9924 StackingClassifier 0.9382"},{"location":"projects/machine-learning/poker-hand-prediction/#-","title":"---","text":""},{"location":"projects/machine-learning/poker-hand-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/machine-learning/poker-hand-prediction/#key-learnings","title":"KEY LEARNINGS","text":"<ul> <li>Learned how different machine learning models perform on real-world data and gained insights into their strengths and weaknesses.</li> <li>Realized how important feature engineering and data preprocessing are for enhancing model performance.</li> </ul> <p>Insights gained from the data:</p> <ul> <li>Class imbalance can bias the model, requiring class weighting for fair performance.</li> <li>Suits and ranks of cards are crucial features, necessitating preprocessing for better model input.</li> </ul> <p>Improvements in understanding machine learning concepts:</p> <ul> <li>Learned how to effectively implement and optimize machine learning models using libraries like scikit-learn.</li> <li>Learned how to effectively use visualizations (e.g., ROC curves, confusion matrices, and heatmaps) to interpret and communicate model performance.</li> </ul> <p>Challenges faced and how they were overcome:</p> <ul> <li>Some features had different scales, which could affect model performance. This was overcome by applying standardization to the features, ensuring they were on a similar scale for better convergence during training.</li> <li>Some rare hands, such as \"royal_flush\" and \"straight_flush,\" had very low prediction accuracy. This was mitigated by analyzing misclassification patterns and considering potential improvements like generating synthetic samples or using other models.</li> </ul>"},{"location":"projects/machine-learning/poker-hand-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1 <p>Poker Strategy Development and Simulation      -&gt; Developers or researchers studying poker strategies can use this model to simulate various hand combinations and evaluate strategic decisions. The model's classification can help assess the strength of different hands and optimize strategies.</p> Application 2 <p>Real-time Poker Hand Evaluation for Mobile Apps      -&gt; Mobile apps that allow users to practice or play poker could incorporate this model to provide real-time hand evaluation, helping users understand the strength of their hands during gameplay.</p>"},{"location":"projects/machine-learning/sleep-quality-prediction/","title":"Sleep quality prediction","text":"Sleep Quality Prediction AIM <p>To predict sleep quality based on lifestyle and health factors.</p> DATASET LINK <p>          Sleep Health and Lifestyle Dataset      </p> DESCRIPTION What is the requirement of the project? <ul> <li>This project aims to predict the quality of sleep using various health and lifestyle metrics. Predicting sleep quality helps individuals and healthcare professionals address potential sleep-related health issues early.</li> </ul> Why is it necessary? <ul> <li>Sleep quality significantly impacts physical and mental health. Early predictions can prevent chronic conditions linked to poor sleep, such as obesity, heart disease, and cognitive impairment.</li> </ul> How is it beneficial and used? <ul> <li>Individuals: Assess their sleep health and make lifestyle changes to improve sleep quality.</li> <li>Healthcare Professionals: Use the model as an auxiliary diagnostic tool to recommend personalized interventions.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Researching sleep health factors and existing literature.</li> <li>Exploring and analyzing the dataset to understand feature distributions.</li> <li>Preprocessing data for effective feature representation.</li> <li>Iterating over machine learning models to find the optimal balance between accuracy and interpretability.</li> </ul> Mention any additional resources used <ul> <li>Research Paper: Analyzing Sleep Patterns Using AI</li> <li>Public Notebook: Sleep Quality Prediction with 96% Accuracy</li> </ul> LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>seaborn</li> <li>joblib</li> <li>flask</li> </ul> EXPLANATION DETAILS OF THE DIFFERENT FEATURES Feature Name Description Type Values/Range Gender Respondent's gender Categorical [Male, Female] Age Respondent's age Numerical Measured in years Sleep Duration (hours) Hours of sleep per day Numerical Measured in hours Physical Activity Level Daily physical activity in minutes Numerical Measured in minutes Stress Level Stress level on a scale Numerical 1 to 5 (low to high) BMI Category Body Mass Index category Categorical [Underweight, Normal, Overweight, Obese] Systolic Blood Pressure Systolic blood pressure Numerical Measured in mmHg Diastolic Blood Pressure Diastolic blood pressure Numerical Measured in mmHg Heart Rate (bpm) Resting heart rate Numerical Beats per minute Daily Steps Average number of steps per day Numerical Measured in steps Sleep Disorder Reported sleep disorder Categorical [Yes, No] WHAT I HAVE DONE Step 1: Exploratory Data Analysis <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> Step 2: Data Cleaning and Preprocessing <ul> <li>Handling missing values</li> <li>Label encoding categorical features</li> <li>Standardizing numerical features</li> </ul> Step 3: Feature Engineering and Selection <ul> <li>Merging features based on domain knowledge</li> <li>Creating derived features such as \"Activity-to-Sleep Ratio\"</li> </ul> Step 4: Modeling <ul> <li>Model trained: Decision Tree</li> <li>Class imbalance handled using SMOTE</li> <li>Metric for optimization: F1-score</li> </ul> Step 5: Result Analysis <ul> <li>Visualized results using confusion matrices and classification reports</li> <li>Interpreted feature importance for tree-based models</li> </ul> MODELS USED AND THEIR ACCURACIES Model Accuracy (%) F1-Score (%) Precision (%) Recall (%) Decision Tree 74.50 75.20 73.00 77.50 CONCLUSION WHAT YOU HAVE LEARNED Insights gained from the data <ul> <li>Sleep Duration, Stress Level, and Physical Activity are the most indicative features for predicting sleep quality.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned and implemented preprocessing techniques like encoding categorical variables and handling imbalanced datasets.</li> <li>Gained insights into deploying a machine learning model using Flask for real-world use cases.</li> </ul> Challenges faced and how they were overcome <ul> <li>Managing imbalanced classes: Overcame this by using SMOTE for oversampling the minority class.</li> <li>Choosing a simple yet effective model: Selected Decision Tree for its interpretability and ease of deployment.</li> </ul> USE CASES OF THIS MODEL Application 1 <p>         A health tracker app can integrate this model to assess and suggest improvements in sleep quality based on user inputs.     </p> Application 2 <p>         Healthcare providers can use this tool to make preliminary assessments of patients' sleep health, enabling timely interventions.     </p> FEATURES PLANNED BUT NOT IMPLEMENTED Feature 1 <p>         Advanced models such as Random Forest, AdaBoost, and Gradient Boosting were not implemented due to the project's focus on simplicity and interpretability.     </p> Feature 2 <p>         Integration with wearable device data for real-time predictions was not explored but remains a potential enhancement for future work.     </p>"},{"location":"projects/machine-learning/used-cars-price-prediction/","title":"Used Cars Price Prediction","text":""},{"location":"projects/machine-learning/used-cars-price-prediction/#aim","title":"AIM","text":"<p>Predicting the prices of used cars based on their configuration and previous usage.</p>"},{"location":"projects/machine-learning/used-cars-price-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/avikasliwal/used-cars-price-prediction</p>"},{"location":"projects/machine-learning/used-cars-price-prediction/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/sid4ds/used-cars-price-prediction/</p>"},{"location":"projects/machine-learning/used-cars-price-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 required for Target Encoding)</li> <li>xgboost</li> <li>catboost</li> <li>matplotlib</li> <li>seaborn</li> </ul>"},{"location":"projects/machine-learning/used-cars-price-prediction/#description","title":"DESCRIPTION","text":"<p>Why is it necessary?</p> <ul> <li>This project aims to predict the prices of used cars listed on an online marketplace based on their features and usage by previous owners. This model can be used by sellers to estimate an approximate price for their cars when they list them on the marketplace. Buyers can use the model to check if the listed price is fair when they decide to buy a used vehicle.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Researching previous projects and articles related to the problem.</li> <li>Data exploration to understand the features.  </li> <li>Identifying different preprocessing strategies for different feature types.</li> <li>Choosing key metrics for the problem - Root Mean Squared Error (for error estimation), R2-Score (for model explainability)</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Dealing with features that have high cardinality</li> <li>Target-encoding Categorical Variables</li> <li>Cars Price Prediction</li> </ul>"},{"location":"projects/machine-learning/used-cars-price-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"projects/machine-learning/used-cars-price-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range Name Car model Categorical Names of car models Location City where the car is listed for sale Categorical Names of cities Year Year of original purchase of car Numerical Years (e.g., 2010, 2015, etc.) Kilometers_Driven Odometer reading of the car Numerical Measured in kilometers Fuel_Type Fuel type of the car Categorical [Petrol, Diesel, CNG, Electric, etc.] Transmission Transmission type of the car Categorical [Automatic, Manual] Owner_Type Number of previous owners of the car Numerical Whole numbers Mileage Current mileage provided by the car Numerical Measured in km/l or equivalent Engine Engine capacity of the car Numerical Measured in CC (Cubic Centimeters) Power Engine power output of the car Numerical Measured in BHP (Brake Horsepower) Seats Seating capacity of the car Numerical Whole numbers New_Price Original price of the car at the time of purchase Numerical Measured in currency"},{"location":"projects/machine-learning/used-cars-price-prediction/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Removing rare categories of brands</li> <li>Removing outliers for numerical features and target</li> <li>Categorical feature encoding for low-cardinality features</li> <li>Target encoding for high-cardinality categorical features (in model pipeline)</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Extracting brand name from model name for a lower-cardinality feature.</li> <li>Converting categorical Owner_Type to numerical Num_Previous_Owners.</li> <li>Feature selection based on model-based feature importances and statistical tests.</li> </ul> <p>Modeling</p> <ul> <li>Holdout dataset created for model testing</li> <li>Setting up a framework for easier testing of multiple models.</li> <li>Models trained: LLinear Regression, K-Nearest Neighbors, Decision Tree, Random Forest, AdaBoost, Multi-Layer Perceptron, XGBoost and CatBoost.</li> <li>Models were ensembled using Simple and Weighted averaging.</li> </ul> <p>Result analysis</p> <ul> <li>Predictions made on holdout test set</li> <li>Models compared based on chosen metrics: RMSE and R2-Score.</li> <li>Visualized predicted prices vs actual prices to analyze errors.</li> </ul>"},{"location":"projects/machine-learning/used-cars-price-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Training time &amp; Model complexity vs Reducing error</p> <ul> <li>Solution: Limiting depth and number of estimators for tree-based models. Overfitting detection and early stopping mechanism for neural network training.</li> </ul>"},{"location":"projects/machine-learning/used-cars-price-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Data Exploration PriceYearKM DrivenEnginePowerMileageSeats <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> Feature Selection Feature CorrelationTarget CorrelationMutual Information <p></p> <p></p> <p></p>"},{"location":"projects/machine-learning/used-cars-price-prediction/#models-used-and-their-performance","title":"MODELS USED AND THEIR PERFORMANCE","text":"Model RMSE R2-Score Linear Regression 3.5803 0.7915 K-Nearest Neighbors 2.8261 0.8701 Decision Tree 2.6790 0.8833 Random Forest 2.4619 0.9014 AdaBoost 2.3629 0.9092 Multi-layer Perceptron 2.6255 0.8879 XGBoost w/o preprocessing 2.1649 0.9238 XGBoost with preprocessing 2.0987 0.9284 CatBoost w/o preprocessing 2.1734 0.9232 Simple average ensemble 2.2804 0.9154 Weighted average ensemble 2.1296 0.9262"},{"location":"projects/machine-learning/used-cars-price-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/machine-learning/used-cars-price-prediction/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ol> <li>Features related to car configuration such as Power, Engine and Transmission are some of the most informative features. Usage-related features such as Year and current Mileage are also important.</li> <li>Seating capacity and Number of previous owners had relatively less predictive power. However, none of the features were candidates for removal.</li> </ol> Improvements in understanding machine learning concepts <ol> <li>Implemented target-encoding for high-cardinality categorical features.</li> <li>Designed pipelines to avoid data leakage.</li> <li>Ensembling models using prediction averaging.</li> </ol> Challenges faced and how they were overcome <ol> <li>Handling mixed feature types in preprocessing pipelines.</li> <li>Regularization and overfitting detection to reduce training time while maintaining performance.</li> </ol>"},{"location":"projects/machine-learning/used-cars-price-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <ul> <li>Sellers can use the model to estimate an approximate price for their cars when they list them on the marketplace.</li> </ul> <ul> <li>Buyers can use the model to check if the listed price is fair when they decide to buy a used vehicle.</li> </ul>"},{"location":"projects/machine-learning/used-cars-price-prediction/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Complex model-ensembling through stacking or hill-climbing was not implemented due to significantly longer training time.</li> </ul>"},{"location":"projects/natural-language-processing/","title":"Natural Language Processing \ud83d\udde3\ufe0f","text":""},{"location":"projects/natural-language-processing/email_spam_detection/","title":"Email Spam Detection","text":""},{"location":"projects/natural-language-processing/email_spam_detection/#aim","title":"AIM","text":"<p>To develop a machine learning-based system that classifies email content as spam or ham (not spam).</p>"},{"location":"projects/natural-language-processing/email_spam_detection/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/ashfakyeafi/spam-email-classification</p>"},{"location":"projects/natural-language-processing/email_spam_detection/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/inshak9/email-spam-detection</p>"},{"location":"projects/natural-language-processing/email_spam_detection/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>seaborn</li> </ul>"},{"location":"projects/natural-language-processing/email_spam_detection/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>A robust system to detect spam emails is essential to combat increasing spam content.</li> <li>It improves user experience by automatically filtering unwanted messages.</li> </ul> Why is it necessary? <ul> <li>Spam emails consume resources, time, and may pose security risks like phishing.</li> <li>Helps organizations and individuals streamline their email communication.</li> </ul> How is it beneficial and used? <ul> <li>Provides a quick and automated solution for spam classification.</li> <li>Used in email services, IT systems, and anti-spam software to filter messages.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Analyzed the dataset and prepared features.</li> <li>Implemented various machine learning models for comparison.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Documentation from scikit-learn</li> <li>Blog: Introduction to Spam Classification with ML</li> </ul>"},{"location":"projects/natural-language-processing/email_spam_detection/#explanation","title":"EXPLANATION","text":""},{"location":"projects/natural-language-processing/email_spam_detection/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<p>The dataset contains features like word frequency, capital letter counts, and others that help in distinguishing spam emails from ham.</p> Feature Description <code>word_freq_x</code> Frequency of specific words in the email body <code>capital_run_length</code> Length of consecutive capital letters <code>char_freq</code> Frequency of special characters like <code>;</code> and <code>$</code> <code>is_spam</code> Target variable (1 = Spam, 0 = Ham)"},{"location":"projects/natural-language-processing/email_spam_detection/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5Step 6 <p>Initial data exploration and understanding:   - Loaded the dataset using pandas.   - Explored dataset features and target variable distribution.</p> <p>Data cleaning and preprocessing:   - Checked for missing values.   - Standardized features using scaling techniques.</p> <p>Feature engineering and selection:   - Extracted relevant features for spam classification.   - Used correlation matrix to select significant features.</p> <p>Model training and evaluation:   - Trained models: KNN, Naive Bayes, SVM, and Random Forest.   - Evaluated models using accuracy, precision, and recall.</p> <p>Model optimization and fine-tuning:   - Tuned hyperparameters using GridSearchCV.</p> <p>Validation and testing:   - Tested models on unseen data to check performance.</p>"},{"location":"projects/natural-language-processing/email_spam_detection/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. Training Time:</li> <li>Models like Random Forest took longer to train but achieved higher accuracy compared to Naive Bayes.</li> </ul> <ul> <li>Complexity vs. Interpretability:</li> <li>Simpler models like Naive Bayes were more interpretable but slightly less accurate.</li> </ul>"},{"location":"projects/natural-language-processing/email_spam_detection/#screenshots","title":"SCREENSHOTS","text":"<p>Project flowchart</p> <pre><code>  graph LR\n    A[Start] --&gt; B[Load Dataset];\n    B --&gt; C[Preprocessing];\n    C --&gt; D[Train Models];\n    D --&gt; E{Compare Performance};\n    E --&gt;|Best Model| F[Deploy];\n    E --&gt;|Retry| C;</code></pre> Confusion Matrix SVMNaive BayesDecision TreeAdaBoostRandom Forest <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"projects/natural-language-processing/email_spam_detection/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy Precision Recall KNN 90% 89% 88% Naive Bayes 92% 91% 90% SVM 94% 93% 91% Random Forest 95% 94% 93% AdaBoost 97% 97% 100%"},{"location":"projects/natural-language-processing/email_spam_detection/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> Accuracy Comparison <p></p>"},{"location":"projects/natural-language-processing/email_spam_detection/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/natural-language-processing/email_spam_detection/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>Feature importance significantly impacts spam detection.</li> <li>Simple models like Naive Bayes can achieve competitive performance.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Gained hands-on experience with classification models and model evaluation techniques.</li> </ul> Challenges faced and how they were overcome <ul> <li>Balancing between accuracy and training time was challenging, solved using model tuning.</li> </ul>"},{"location":"projects/natural-language-processing/email_spam_detection/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <p>Email Service Providers - Automated filtering of spam emails for improved user experience.</p> <p>Enterprise Email Security - Used in enterprise software to detect phishing and spam emails.</p>"},{"location":"projects/natural-language-processing/email_spam_detection/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Integration of deep learning models (LSTM) for improved accuracy.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/","title":"Name Entity Recognition (NER) Project","text":""},{"location":"projects/natural-language-processing/name_entity_recognition/#aim","title":"AIM","text":"<p>To develop a system that identifies and classifies named entities (such as persons, organizations, locations, dates, etc.) in text using Named Entity Recognition (NER) with SpaCy.</p>"},{"location":"projects/natural-language-processing/name_entity_recognition/#dataset-link","title":"DATASET LINK","text":"<p>N/A (This project uses text input for NER analysis, not a specific dataset) - It uses real time data as input .</p>"},{"location":"projects/natural-language-processing/name_entity_recognition/#notebook-link","title":"NOTEBOOK LINK","text":"<p>[Note book link ] (https://colab.research.google.com/drive/1pBIEFA4a9LzyZKUFQMCypQ22M6bDbXM3?usp=sharing)</p>"},{"location":"projects/natural-language-processing/name_entity_recognition/#libraries-needed","title":"LIBRARIES NEEDED","text":"<ul> <li>SpaCy</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>Named Entity Recognition (NER) is essential to automatically extract and classify key entities from text, such as persons, organizations, locations, and more.</li> <li>This helps in analyzing and organizing data efficiently, enabling various NLP applications like document analysis and information retrieval.</li> </ul> Why is it necessary? <ul> <li>NER is used for understanding and structuring unstructured text, which is widely applied in industries such as healthcare, finance, and e-commerce.</li> <li>It allows users to extract actionable insights from large volumes of text data</li> </ul> How is it beneficial and used? <ul> <li>NER plays a key role in tasks such as document summarization, information retrieval.</li> <li>It automates the extraction of relevant entities, which reduces manual effort and improves efficiency.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>The project leverages SpaCy's pre-trained NER models, enabling easy text analysis without the need for training custom models.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#mention-any-additional-resources-used-blogs-books-chapters-articles-research-papers-etc","title":"Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.)","text":"<ul> <li>SpaCy Documentation: SpaCy NER</li> <li>NLP in Python by Steven Bird et al.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#explanation","title":"EXPLANATION","text":""},{"location":"projects/natural-language-processing/name_entity_recognition/#details-of-the-different-entity-types","title":"DETAILS OF THE DIFFERENT ENTITY TYPES","text":"<p>The system extracts the following entity types:</p> Entity Type Description PERSON Names of people (e.g., \"Anuska\") ORG Organizations (e.g., \"Google\", \"Tesla\") LOC Locations (e.g., \"New York\", \"Mount Everest\") DATE Dates (e.g., \"January 1st, 2025\") GPE Geopolitical entities (e.g., \"India\", \"California\")"},{"location":"projects/natural-language-processing/name_entity_recognition/#what-i-have-done","title":"WHAT I HAVE DONE","text":""},{"location":"projects/natural-language-processing/name_entity_recognition/#step-1-data-collection-and-preparation","title":"Step 1: Data collection and preparation","text":"<ul> <li>Gathered sample text for analysis (provided by users in the app).</li> <li>Explored the text structure and identified entity types.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#step-2-ner-model-implementation","title":"Step 2: NER model implementation","text":"<ul> <li>Integrated SpaCy's pre-trained NER model (<code>en_core_web_sm</code>).</li> <li>Extracted named entities and visualized them with labels and color coding.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#step-3-testing-and-validation","title":"Step 3: Testing and validation","text":"<ul> <li>Validated results with multiple test cases to ensure entity accuracy.</li> <li>Allowed users to input custom text for NER analysis in real-time.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":""},{"location":"projects/natural-language-processing/name_entity_recognition/#trade-off-1-pre-trained-model-vs-custom-model","title":"Trade Off 1: Pre-trained model vs. custom model","text":"<ul> <li>Pre-trained models provide quick results but may lack accuracy for domain-specific entities.</li> <li>Custom models can improve accuracy but require additional data and training time.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#trade-off-2-real-time-analysis-vs-batch-processing","title":"Trade Off 2: Real-time analysis vs. batch processing","text":"<ul> <li>Real-time analysis in a web app enhances user interaction but might slow down with large text inputs.</li> <li>Batch processing could be more efficient for larger datasets.</li> </ul>"},{"location":"projects/natural-language-processing/name_entity_recognition/#screenshots","title":"SCREENSHOTS","text":""},{"location":"projects/natural-language-processing/name_entity_recognition/#ner-example","title":"NER Example","text":"<p>``` mermaid graph LR     A[Start] --&gt; B[Text Input];     B --&gt; C[NER Analysis];     C --&gt; D{Entities Extracted};     D --&gt;|Person| E[Anuska];     D --&gt;|Location| F[New York];     D --&gt;|Organization| G[Google];     D --&gt;|Date| H[January 1st, 2025];</p>"},{"location":"projects/natural-language-processing/next-word-pred/","title":"Next Word Prediction using LSTM","text":""},{"location":"projects/natural-language-processing/next-word-pred/#aim","title":"AIM","text":"<p>To predict the next word using LSTM.</p>"},{"location":"projects/natural-language-processing/next-word-pred/#dataset-link","title":"DATASET LINK","text":"<p>Dataset</p>"},{"location":"projects/natural-language-processing/next-word-pred/#notebook-link","title":"NOTEBOOK LINK","text":"<p>Code</p>"},{"location":"projects/natural-language-processing/next-word-pred/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>seaborn</li> <li>tensorflow</li> <li>keras</li> </ul>"},{"location":"projects/natural-language-processing/next-word-pred/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>To create an intelligent system capable of predicting the next word in a sentence based on its context.</li> <li>The need for such a system arises in applications like autocomplete, chatbots, and virtual assistants.</li> </ul> Why is it necessary? <ul> <li>Enhances user experience in text-based applications by offering accurate suggestions.</li> <li>Reduces typing effort, especially in mobile applications.</li> </ul> How is it beneficial and used? <ul> <li>Improves productivity: By predicting words, users can complete sentences faster.</li> <li>Supports accessibility: Assists individuals with disabilities in typing.</li> <li>Boosts efficiency: Helps in real-time text generation in NLP applications like chatbots and email composition.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Studied LSTM architecture and its suitability for sequential data.</li> <li>Explored similar projects and research papers to understand data preprocessing techniques.</li> <li>Experimented with tokenization, padding, and sequence generation for the dataset.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Blogs on LSTM from Towards Data Science.</li> <li>TensorFlow and Keras official documentation.</li> </ul>"},{"location":"projects/natural-language-processing/next-word-pred/#explanation","title":"EXPLANATION","text":""},{"location":"projects/natural-language-processing/next-word-pred/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"projects/natural-language-processing/next-word-pred/#project-workflow","title":"PROJECT WORKFLOW","text":"Step 1Step 2Step 3Step 4Step 5Step 6 <p>Initial data exploration and understanding:</p> <ul> <li>Gathered text data from open-source datasets.</li> <li>Analyzed the structure of the data.</li> <li>Performed basic text statistics to understand word frequency and distribution.</li> </ul> <p>Data cleaning and preprocessing</p> <ul> <li>Removed punctuation and convert text to lowercase.</li> <li>Tokenized text into sequences and pad them to uniform length.</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Created input-output pairs for next-word prediction using sliding window techniques on tokenized sequences.</li> </ul> <p>Model training and evaluation:</p> <ul> <li>Used an embedding layer to represent words in a dense vector space.</li> <li>Implemented LSTM-based sequential models to learn context and dependencies in text. </li> <li>Experimented with hyperparameters like sequence length, LSTM units, learning rate, and batch size.</li> </ul> <p>Model optimization and fine-tuning</p> <ul> <li>Adjusted hyperparameters like embedding size, LSTM units, and learning rate.</li> </ul> <p>Validation and testing</p> <ul> <li>Used metrics like accuracy and perplexity to assess prediction quality.  </li> <li>Validated the model on unseen data to test generalization. </li> </ul>"},{"location":"projects/natural-language-processing/next-word-pred/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade-Off 1Trade-Off 2 <p>Accuracy vs Training Time:</p> <ul> <li>Solution: Balanced by reducing the model's complexity and using an efficient optimizer.</li> </ul> <p>Model complexity vs. Overfitting:</p> <ul> <li>Solution: Implemented dropout layers and monitored validation loss during training.</li> </ul>"},{"location":"projects/natural-language-processing/next-word-pred/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Data Preprocessed?};\n    B --&gt;|No| C[Clean and Tokenize];\n    C --&gt; D[Create Sequences];\n    D --&gt; B;\n    B --&gt;|Yes| E[Model Designed?];\n    E --&gt;|No| F[Build LSTM/Transformer];\n    F --&gt; E;\n    E --&gt;|Yes| G[Train Model];\n    G --&gt; H{Performant?};\n    H --&gt;|No| I[Optimize Hyperparameters];\n    I --&gt; G;\n    H --&gt;|Yes| J[Deploy Model];\n    J --&gt; K[End];</code></pre>"},{"location":"projects/natural-language-processing/next-word-pred/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy MSE R2 Score LSTM 72% - -"},{"location":"projects/natural-language-processing/next-word-pred/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> LSTM Loss <p></p>"},{"location":"projects/natural-language-processing/next-word-pred/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/natural-language-processing/next-word-pred/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>The importance of preprocessing for NLP tasks.</li> <li>How padding and embeddings improve the model\u2019s ability to generalize.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned how LSTMs handle sequential dependencies.</li> <li>Understood the role of softmax activation in predicting word probabilities.</li> </ul> Challenges faced and how they were overcome <ul> <li>Challenge: Large vocabulary size causing high memory usage.</li> <li>Solution: Limited vocabulary to the top frequent words.</li> </ul>"},{"location":"projects/natural-language-processing/next-word-pred/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Text Autocompletion</p> <ul> <li>Used in applications like Gmail and search engines to enhance typing speed.</li> </ul> <p>Virtual Assistants</p> <ul> <li>Enables better conversational capabilities in chatbots and AI assistants.</li> </ul>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/","title":"Twitter Sentiment Analysis","text":""},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#aim","title":"AIM","text":"<p>To analyze sentiment in Twitter data using natural language processing techniques.</p>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/kazanova/sentiment140</p>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://drive.google.com/drive/folders/1F6BLxvp6qIAgGZOZ2rC370EmKhj5W1FC?usp=sharing</p>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>seaborn</li> <li>matplotlib</li> <li>tensorflow</li> <li>keras</li> <li>nltk</li> <li>multiprocessing</li> <li>tqdm</li> <li>os</li> </ul>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project aims to perform sentiment analysis on Twitter data.</li> <li>This involves extracting tweets related to specific topics or keywords, processing these tweets using natural language processing (NLP) techniques to determine the sentiment (positive or negative), and presenting insights derived from the analysis.</li> </ul> Why is it necessary? <ul> <li>Twitter is a rich source of real-time public opinion and sentiment. Analyzing tweets can provide valuable insights into public perception of events, products, brands, or topics of interest.</li> <li>This information is crucial for businesses, governments, and researchers to make informed decisions, understand public sentiment trends, and gauge the success of marketing campaigns or policy changes.</li> </ul> How is it beneficial and used? <ul> <li>Business Insights: Companies can understand customer feedback and sentiments towards their products or services.</li> <li>Brand Management: Monitor brand sentiment and respond to customer concerns or issues in real-time.</li> <li>Market Research: Identify trends and sentiments related to specific topics or industries.</li> <li>Social Listening: Understand public opinion on current events, policies, or social issues.</li> <li>Customer Service Improvement: Improve customer service by analyzing sentiment towards customer interactions.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li> <p>Choose appropriate NLP techniques for sentiment analysis, such as:</p> <ul> <li>Bag-of-Words (BoW) and TF-IDF: Represent tweets as numerical vectors.</li> <li>Sentiment Lexicons: Use dictionaries of words annotated with sentiment scores (e.g., Vader sentiment lexicon).</li> <li>Machine Learning Models: Train supervised classifiers (e.g., Naive Bayes, SVM, or neural networks) on labeled data for sentiment prediction.</li> </ul> </li> <li> <p>Model Evaluation: Evaluate the performance of the sentiment analysis model using metrics like accuracy. Cross-validation techniques can be used to ensure robustness.</p> </li> <li> <p>Visualization and Insights: Visualize sentiment trends over time or across different categories using charts (e.g., line plots, bar charts). Generate insights based on the analysis results.</p> </li> <li> <p>Deployment: Deploy the sentiment analysis system as a standalone application or integrate it into existing systems for real-time monitoring and analysis.</p> </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.) <ul> <li>GeeksforGeeks Twitter Sentiment Analysis</li> <li>YouTube Video</li> </ul>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#explanation","title":"EXPLANATION","text":""},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5Step 6 <p>Initial data exploration and understanding:</p> <ul> <li>Gathered Twitter data using pre-existing datasets (Kaggle).</li> <li>Understand the structure of the data (e.g., tweet text, metadata like timestamps, user information).</li> <li>Explore basic statistics and distributions of data features.</li> </ul> <p>Data cleaning and preprocessing:</p> <ul> <li>Remove or handle noisy data such as URLs, special characters, and emojis.</li> <li>Tokenize tweets into individual words or tokens.</li> <li>Remove stopwords (commonly used words that do not carry significant meaning).</li> <li>Normalize text through techniques like stemming to reduce variations of words.</li> </ul> <p>Feature engineering and selection:</p> <ul> <li>Convert text data into numerical representations suitable for machine learning models (e.g., Bag-of-Words, TF-IDF).</li> <li>Select relevant features that contribute most to the sentiment analysis task.</li> </ul> <p>Model training and evaluation:</p> <ul> <li>Split the dataset into training and testing sets.</li> <li>Choose appropriate machine learning models (e.g., Naive Bayes, RNN LSTM, logistic regression) for sentiment analysis.</li> <li>Train the models on the training data and evaluate their performance using metrics like accuracy.</li> </ul> <p>Model optimization and fine-tuning:</p> <ul> <li>Fine-tune the hyperparameters of the selected models to improve performance.</li> <li>Consider techniques like grid search or random search to find optimal parameters.</li> <li>Experiment with different models or combinations of models to achieve better results.</li> </ul> <p>Validation and testing:</p> <ul> <li>Validate the trained models on a separate validation set to ensure generalizability.</li> <li>Test the final model on unseen data (testing set or new tweets) to assess its performance in real-world scenarios.</li> <li>Iterate on the model and preprocessing steps based on validation results to improve accuracy and robustness.</li> </ul>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade-off 1Trade-off 2 <p>Stemming process took a lot of computational time to process over 1.6 million datapoints.</p> <ul> <li>Solution: Divided the data into batches and applied parallel processing.</li> </ul> <p>In RNN based LSTM, overfitting problem occurred.</p> <ul> <li>Solution: Tried to fix it using Dropout layer, early stopping criteria.</li> </ul>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#screenshots","title":"SCREENSHOTS","text":"<p>Project structure or tree diagram</p> <pre><code>  graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> Visualizations and EDA of different features Sentiment Distribution <p></p> Model performance graphs LR Confusion MatrixLR ROC CurveNaive Bayes Confusion MatrixNaive Bayes ROC Curve <p></p> <p></p> <p></p> <p></p>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy MSE R2 Score Logistic Regression 77% 0.1531724703945824 0.3873101184216704 Naive Bayes 75% 0.17476773790874897 0.3009290483650041 RNN LSTM 77.84% - -"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> LSTM AccuracyLSTM Loss <p></p> <p></p>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#conclusion","title":"CONCLUSION","text":""},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Data Variety: Twitter data provides a rich source of real-time, diverse opinions and sentiments.</li> <li>Text Preprocessing: Importance of cleaning and preprocessing text data (e.g., removing stopwords, stemming/lemmatization) for better analysis.</li> <li>Feature Extraction: Techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings (e.g., Word2Vec, GloVe) to represent text numerically for machine learning models.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Model Selection: Experimenting with various algorithms to find the most suitable for sentiment classification (e.g., logistic regression, naive bayes, neural networks).</li> </ul> Challenges faced and how they were overcome <ul> <li>Noise in Data: Dealing with noise from hashtags, emojis, and slang in tweets through effective preprocessing techniques.</li> <li>Computational Resources: Managing large volumes of data and resource-intensive computations by optimizing code and leveraging cloud computing platforms if necessary.</li> </ul>"},{"location":"projects/natural-language-processing/twitter_sentiment_analysis/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Brand Monitoring and Customer Feedback Analysis</p> <ul> <li>This application allows businesses to leverage Twitter sentiment analysis as a valuable tool for customer relationship management, brand reputation management, and strategic decision-making based on real-time customer feedback and sentiment analysis.</li> </ul> <p>Financial Market Analysis and Investment Decisions</p> <ul> <li>This application showcases how Twitter sentiment analysis can be leveraged in the financial sector to gain competitive advantages, improve investment strategies, and manage risks effectively based on public sentiment towards financial markets and specific stocks.</li> </ul>"},{"location":"projects/statistics/","title":"Statistics \ud83d\udcc3","text":"Bangladesh Premier League Analysis <p>Exploring team performances, player stats, and key insights from the BPL.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 10 mins</p>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/","title":"Bangladesh Premier League Analysis","text":""},{"location":"projects/statistics/bangladesh-premier-league-analysis/#aim","title":"AIM","text":"<p>The main goal of the project is to analyze the performance of the bangladesh players in their premier league and obtaining the top 5 players in all of them in different fields like bowling, batting, toss_winner, highest runner, man of the match, etc.</p>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/abdunnoor11/bpl-data</p>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1equud2jwKnmE1qbbTJLsi2BbjuA7B1Si?usp=sharing</p>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>matplotlib</li> <li>pandas</li> <li>sklearn</li> <li>seaborn</li> <li>numpy</li> <li>scipy</li> <li>xgboost</li> <li>Tensorflow</li> <li>Keras</li> </ul>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>This project aims to analyze player performance data from the Bangladesh Premier League (BPL) to classify players into categories such as best, good, average, and poor based on their performance.</li> <li>The analysis provides valuable insights for players and coaches, highlighting who needs more training and who requires less, which can aid in strategic planning for future matches.</li> </ul> Why is it necessary? <ul> <li>Analyzing player performance helps in understanding strengths and weaknesses, which can significantly reduce the chances of losing and increase the chances of winning future matches.</li> <li>It aids in making informed decisions about team selection and match strategies.</li> </ul> How is it beneficial and used? <ul> <li>For Players: Provides feedback on their performance, helping them to improve specific aspects of their game.</li> <li>For Coaches: Helps in identifying areas where players need improvement, which can be focused on during training sessions.</li> <li>For Team Management: Assists in strategic decision-making regarding player selection and match planning.</li> <li>For Fans and Analysts: Offers insights into player performances and trends over the league, enhancing the understanding and enjoyment of the game.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Perform initial data exploration to understand the structure and contents of the dataset.</li> <li>To learn about the topic and searching the related content like <code>what is league</code>, <code>About bangladesh league</code>, <code>their players</code> and much more.</li> <li>Learn about the features in details by searching on the google or quora.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Articles on cricket analytics from websites such as ESPNcricinfo and Cricbuzz.</li> <li>https://www.linkedin.com/pulse/premier-league-202223-data-analysis-part-i-ayomide-aremu-cole-iwn4e/</li> <li>https://analyisport.com/insights/how-is-data-used-in-the-premier-league/</li> </ul>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#explanation","title":"EXPLANATION","text":""},{"location":"projects/statistics/bangladesh-premier-league-analysis/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<pre><code>There are 3 different types of the datasets.\n\n- Batsman Dataset\n- Bowler Dataset\n- BPL (Bangladesh Premier League) Dataset\n</code></pre> <ul> <li>There are 12 features in <code>Batsman Dataset</code></li> </ul> Feature Name Description id All matches unique id season Season match_no Number of matches date Date of Play player_name Player Name comment How did the batsman get out? R Batsman's run B How many balls faced the batsman? M How long their innings was in minutes? fours Fours sixs Sixes SR Strike rate <ul> <li>There are 12 features in <code>Bowler Dataset</code></li> </ul> Feature Name Description id All matches unique id season Season match_no Number of matches date Date of Play player_name Player Name O Overs M middle overs R Runs W Wickets ECON The average number of runs they have conceded per over bowled WD Wide balls NB No balls <ul> <li>There are 19 features in <code>BPL Dataset</code></li> </ul> Feature Name Description id All matches unique id season Season match_no Number of matches date Date of Play team_1 First Team team_1_score First Team Score team_2 Second Team team_2_score Second Team Score player_of_match Which team won the toss? toss_winner Which team won the toss? toss_decision Toss winner team decision winner Match Winner venue Venue city City win_by_wickets Win by wickets. win_by_runs Win by runs result Result of the winner umpire_1 First Umpire Name umpire_2 Second Umpire Name"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5Step 6Step 7Step 8 <ul> <li>Performed Exploratory Data Analysis on data.</li> </ul> <ul> <li>Created data visualisations to understand the data in a better way.</li> </ul> <ul> <li>Found strong relationships between independent features and dependent feature using correlation.</li> </ul> <ul> <li>Handled missing values using strong correlations,dropping unnecessary ones.</li> </ul> <ul> <li>Used different Regression techniques like Linear Regression,Ridge Regression,Lasso Regression and deep neural networks to predict the dependent feature in most suitable manner.</li> </ul> <ul> <li>Compared various models and used best performance model to make predictions.</li> </ul> <ul> <li>Used Mean Squared Error and R2 Score for evaluating model's performance.</li> </ul> <ul> <li>Visualized best model's performance using matplotlib and seaborn library.</li> </ul>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2Trade Off 3Trade Off 4 <p>Handling missing and inconsistent data entries.</p> <ul> <li>Solution</li> <li>Data Imputation: For missing numerical values, I used techniques such as mean, median, or mode imputation based on the distribution of the data.</li> <li>Data Cleaning: For inconsistent entries, I standardized the data by removing duplicates, correcting typos, and ensuring uniform formatting.</li> <li>Dropping Irrelevant Data: In cases where the missing data was extensive and could not be reliably imputed, I decided to drop those rows/columns to maintain data integrity.</li> </ul> <p>Extracting target variables from the dataset.</p> <ul> <li>Solution</li> <li>Feature Engineering: Created new features that could serve as target variables, such as aggregating player statistics to determine top performers.</li> <li>Domain Knowledge: Utilized cricket domain knowledge to identify relevant metrics (e.g., strike rate, economy rate) and used them to define target variables.</li> <li>Label Encoding: For categorical target variables (e.g., player categories like best, good, average, poor), I used label encoding techniques to convert them into numerical format for analysis.</li> </ul> <p>Creating clear and informative visualizations that effectively communicate the findings.</p> <ul> <li>Solution</li> <li>Tool Selection: Used powerful visualization tools like Matplotlib and Seaborn in Python, which provide a wide range of customization options.</li> <li>Visualization Best Practices: Followed best practices such as using appropriate chart types (e.g., bar charts for categorical data, scatter plots for correlations), adding labels and titles, and ensuring readability.</li> <li>Iterative Refinement: Iteratively refined visualizations based on feedback and self-review to enhance clarity and informativeness.</li> </ul> <p>Correctly interpreting the results to provide actionable insights.</p> <ul> <li>Solution</li> <li>Cross-validation: Used cross-validation techniques to ensure the reliability and accuracy of the analysis results.</li> <li>Collaboration with Experts: Engaged with cricket experts and enthusiasts to validate the findings and gain additional perspectives.</li> <li>Contextual Understanding: Interpreted results within the context of the game, considering factors such as player roles, match conditions, and historical performance to provide meaningful and actionable insights.</li> </ul>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Visualizations and EDA of different features Top 5 Player Of MatchTop 5 Batsman RunnersTop 5 Four RunnersTop 5 OversTop 5 RunsTop 5 UmpiresTop 5 WicketsToss Winners <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#models-used-and-their-accuracies","title":"MODELS USED AND THEIR ACCURACIES","text":"Model MSE R2 Random Forest Regression 19.355984 0.371316 Gradient Boosting Regression 19.420494 0.369221 XG Boost Regression 21.349168 0.306577 Ridge Regression 26.813981 0.129080 Linear Regression 26.916888 0.125737 Deep Neural Network 27.758216 0.098411 Decision Tree Regression 29.044533 0.056631"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> RF Regression PlotConclusion Graph <p></p> <p></p>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#conclusion","title":"CONCLUSION","text":"<pre><code>We can see that R2 Score and Mean Absolute Error is best for Random Forest Regression.\nBy Using Neural network, We cannot get the minimum Mean Squared Error value possible.\nRandom Forest Regression model can predict most accurate results for predicting bangladesh premier league winning team which is the highest model performance in comparison with other Models.\n</code></pre>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>Identified key performance indicators for players in the Bangladesh Premier League, such as top scorers, best bowlers, and players with the most man of the match awards.</li> <li>Discovered trends and patterns in player performances that could inform future strategies and training programs.</li> <li>Gained a deeper understanding of the distribution of player performances across different matches and seasons.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Enhanced knowledge of data cleaning and preprocessing techniques to handle real-world datasets.</li> <li>Improved skills in exploratory data analysis (EDA) to extract meaningful insights from raw data.</li> <li>Learned how to use visualization tools to effectively communicate data-driven findings.</li> </ul>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <p>Team Selection and Strategy Planning</p> <ul> <li>Coaches and team managers can use the model to analyze player performance data and make informed decisions about team selection and match strategies. By identifying top performers and areas for improvement, the model can help optimize team composition and tactics for future matches.</li> </ul> <p>Player Performance Monitoring and Training</p> <ul> <li>The model can be used to track player performance over time and identify trends in their performance. This information can be used by coaches to tailor training programs to address specific weaknesses and enhance overall player development. By monitoring performance metrics, the model can help ensure that players are continuously improving.</li> </ul>"},{"location":"projects/statistics/bangladesh-premier-league-analysis/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1Feature 2 <p>Real-time Performance Tracking</p> <ul> <li>Implementing a real-time tracking system to update player performance metrics during live matches.</li> </ul> <p>Advanced Predictive Analytics - Using advanced machine learning algorithms to predict future player performances and match outcomes.</p>"}]}