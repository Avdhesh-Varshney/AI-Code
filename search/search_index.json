{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AI Code \ud83d\udc4bAI Code","text":"<p>\ud83d\ude80 Your Gateway to Artificial Intelligence &amp; Machine Learning \ud83e\udd16</p> <p>Stars</p> <p>Forks</p> <p>Contributors</p> <p>Last Update</p> \ud83c\udf1f About AI Code <p>     AI-Code is an open-source initiative designed to democratize AI education through practical, hands-on learning. We provide structured implementations of various AI algorithms, from basic machine learning to advanced deep learning techniques, making complex concepts accessible to everyone.   </p> \u2728 Why Choose AI Code? <ul> <li> \ud83c\udfaf Learn by Doing: Practical, hands-on projects across multiple AI domains       </li> <li> \ud83d\udcda Comprehensive Resources: Detailed guides, tutorials, and documentation       </li> <li> \ud83e\udd1d Community Driven: Active community support and contributions       </li> <li> \ud83d\ude80 Industry Ready: Projects aligned with current industry practices       </li> </ul> \ud83d\udee0\ufe0f Tech Stack <p>Python 3.9+</p> <p>Documentation</p> <p>Version Control</p> <p>IDE</p> \ud83c\udfaf Featured Projects Data Visualization <p>Transform complex data into meaningful insights through powerful visualization techniques.</p> Machine Learning <p>Explore cutting-edge ML algorithms and their real-world applications.</p> Deep Learning <p>Dive deep into neural networks and advanced AI architectures.</p> Natural Language Processing <p>Build intelligent systems that understand and generate human language.</p> Computer Vision <p>Create AI systems that can see and understand visual information.</p> Generative AI <p>Master the art of creating AI-generated content with GANs.</p> \ud83c\udf1f Ready to Start Your AI Journey? <p>Join our community and start building amazing AI projects today!</p>        \ud83d\ude80 Get Started             \ud83d\udc65 Contribute"},{"location":"contribute/","title":"\ud83d\udcdd Contribute to AI-Code \ud83d\ude80","text":"<p>Welcome to AI-Code! Whether you're an expert or a beginner, your contributions matter. Let's build AI projects together!</p>"},{"location":"contribute/#getting-started","title":"Getting Started","text":"<ol> <li>Star &amp; Fork: Star \u2b50 &amp; fork the repo.</li> <li>Clone: <pre><code>git clone https://github.com/&lt;your-github-username&gt;/AI-Code.git &amp;&amp; cd AI-Code\n</code></pre></li> <li>Create Branch: <pre><code>git checkout -b &lt;your_branch_name&gt;\n</code></pre></li> <li>Set Up Environment: <pre><code>python -m venv env &amp;&amp; source env/bin/activate  # (Windows: env\\Scripts\\activate)\npip install -r requirements.txt\n</code></pre></li> <li>Preview Locally: <pre><code>mkdocs serve  # Visit http://127.0.0.1:8000/AI-Code/\n</code></pre></li> </ol>"},{"location":"contribute/#making-contributions","title":"Making Contributions","text":"<ol> <li>Edit Code: Follow project standards.</li> <li>Stage &amp; Commit: <pre><code>git add . &amp;&amp; git commit -m \"&lt;your_commit_message&gt;\"\n</code></pre></li> <li>Push Changes: <pre><code>git push -u origin &lt;your_branch_name&gt;\n</code></pre></li> <li>Create a Pull Request (PR):</li> <li>Go to GitHub \u2192 Open a PR \u2192 Provide clear details.</li> </ol>"},{"location":"contribute/#contribution-guidelines","title":"Contribution Guidelines","text":"<ul> <li>File Naming: Use <code>kebab-case</code> (e.g., <code>ai-model.py</code>).</li> <li>Docs: Follow README Template.</li> <li>Commits: Keep them concise &amp; meaningful.</li> <li>PRs: No direct commits to <code>main</code>, use PR templates, and include screenshots if relevant.</li> <li>Code Quality: Clean, maintainable &amp; well-commented.</li> </ul>"},{"location":"contribute/#resources","title":"Resources","text":"<ul> <li>Git &amp; GitHub: Fork, Clone, PR Guide</li> <li>Learn Python: LearnPython.org</li> <li>MkDocs: Documentation</li> </ul>"},{"location":"project-readme-template/","title":"\ud83d\udcc4 Template Guide","text":""},{"location":"project-readme-template/#project-title","title":"\ud83d\udcdc Project Title","text":""},{"location":"project-readme-template/#aim","title":"\ud83c\udfaf AIM","text":""},{"location":"project-readme-template/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>https://www.google.com</p>"},{"location":"project-readme-template/#kaggle-notebook","title":"\ud83d\udcd3 KAGGLE NOTEBOOK","text":"<p>https://www.google.com</p> Kaggle Notebook <p> </p>"},{"location":"project-readme-template/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python, JavaScript Libraries/Frameworks TensorFlow, Keras, Flask Databases MongoDB, PostgreSQL Tools Docker, Git, Jupyter, VS Code Deployment AWS, Heroku"},{"location":"project-readme-template/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>Write the answer here in simple bullet points. </li> </ul> How is it beneficial and used? <ul> <li>Write the answer here in simple bullet points. </li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Write the answer here in simple bullet points. </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Write the answer here in simple bullet points. </li> </ul>"},{"location":"project-readme-template/#project-explanation","title":"\ud83d\udd0d PROJECT EXPLANATION","text":""},{"location":"project-readme-template/#dataset-overview-feature-details","title":"\ud83e\udde9 DATASET OVERVIEW &amp; FEATURE DETAILS","text":"\ud83d\udcc2 dataset.csv <ul> <li>There are X features in the dataset.csv</li> </ul> Feature Name Description Datatype feature 1 explain 1 int64/object \ud83d\udee0 Developed Features from dataset.csv Feature Name Description Reason Datatype feature 1 explain 1 reason 1 int64/object"},{"location":"project-readme-template/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Step 1Step 2Step 3Step 4Step 5Step 6 <ul> <li>Explanation</li> </ul> <ul> <li>Explanation</li> </ul> <ul> <li>Explanation</li> </ul> <ul> <li>Explanation</li> </ul> <ul> <li>Explanation</li> </ul> <ul> <li>Explanation</li> </ul>"},{"location":"project-readme-template/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"Section 1 <ul> <li>Explanation</li> </ul>"},{"location":"project-readme-template/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Describe the trade-off encountered (e.g., accuracy vs. computational efficiency).</li> <li>Explain how you addressed this trade-off (e.g., by optimizing hyperparameters, using a more efficient algorithm, etc.).</li> </ul> <ul> <li>Describe another trade-off (e.g., model complexity vs. interpretability).</li> <li>Explain the solution (e.g., by selecting a model that balances both aspects effectively).</li> </ul>"},{"location":"project-readme-template/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"<p>Visualizations and EDA of different features</p> Image Topic <p></p> Model performance graphs Image Topic <p></p>"},{"location":"project-readme-template/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy MSE R2 Score Model Name 95% 0.022 0.90 Model Name 93% 0.033 0.88"},{"location":"project-readme-template/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"project-readme-template/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Write from here in bullet points</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Write from here in bullet points</li> </ul>"},{"location":"project-readme-template/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Headline 1Headline 2 <ul> <li>Explain your application</li> </ul> <ul> <li>Explain your application</li> </ul>"},{"location":"project-readme-template/#useful-links","title":"\ud83d\udd17 USEFUL LINKS","text":"Deployed ModelGitHub RepositoryBinary Model File <ul> <li>https://www.google.com </li> </ul> <ul> <li>https://www.google.com </li> </ul> <ul> <li>https://www.google.com </li> </ul>"},{"location":"computer-vision/","title":"\ud83c\udfa5 Computer Vision","text":"Face Detection Model <p>Detecting faces in images using OpenCV's powerful Haar cascades.</p> <p>\ud83d\udcc5 2025-01-16 | \u23f1\ufe0f 10 min read</p> Counting Bicep Reps <p>Real-time tracking and counting of bicep curls with MediaPipe's Pose module and OpenCV.</p> <p>\ud83d\udcc5 2025-01-18 | \u23f1\ufe0f 15 min read</p> Brightness Control <p>Adjust screen brightness dynamically using MediaPipe's Hand Tracking and OpenCV.</p> <p>\ud83d\udcc5 2025-01-24 | \u23f1\ufe0f 10 min read</p> Black and White Image Colorizer <p>Colorization of Black and White Images using OpenCV and pre-trained caffe models.</p> <p>\ud83d\udcc5 2025-01-14 | \u23f1\ufe0f 8 min read</p>"},{"location":"computer-vision/bicep-reps-counting/","title":"Counting Bicep Reps","text":""},{"location":"computer-vision/bicep-reps-counting/#aim","title":"AIM","text":"<p>To track and count bicep curls in real time using computer vision techniques with OpenCV and Mediapipe's Pose module.</p>"},{"location":"computer-vision/bicep-reps-counting/#dataset-link","title":"DATASET LINK","text":"<p>This project does not use a specific dataset as it works with real-time video from a webcam.</p>"},{"location":"computer-vision/bicep-reps-counting/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://drive.google.com/file/d/13Omm8Zy0lmtjmdHgfQbraBu3NJf3wknw/view?usp=sharing</p>"},{"location":"computer-vision/bicep-reps-counting/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>OpenCV</li> <li>Mediapipe</li> <li>NumPy</li> </ul>"},{"location":"computer-vision/bicep-reps-counting/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project aims to provide a computer vision-based solution for tracking fitness exercises like bicep curls without the need for wearable devices or sensors. </li> </ul> Why is it necessary? <ul> <li>Helps fitness enthusiasts monitor their workouts in real time.</li> <li>Provides an affordable and accessible alternative to wearable fitness trackers.</li> </ul> How is it beneficial and used? <ul> <li>Real-time feedback on workout form and repetition count.</li> <li>Can be extended to other fitness exercises and integrated into fitness apps</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Explored Mediapipe's Pose module for pose landmark detection.</li> <li>Integrated OpenCV for video frame processing and real-time feedback.</li> <li>Planned the logic for detecting curls based on elbow angle thresholds.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Mediapipe official documentation.</li> <li>OpenCV tutorials on video processing. </li> </ul>"},{"location":"computer-vision/bicep-reps-counting/#explanation","title":"EXPLANATION","text":""},{"location":"computer-vision/bicep-reps-counting/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<pre><code>- Pose Estimation: Utilized Mediapipe's Pose module to detect key landmarks on the human body.\n- Angle Calculation: Calculated angles at the elbow joints to determine curl movement.\n- Rep Tracking: Incremented rep count when alternating between full curl and relaxed positions.\n- Real-Time Feedback: Displayed the remaining curl count on the video feed.\n</code></pre>"},{"location":"computer-vision/bicep-reps-counting/#project-workflow","title":"PROJECT WORKFLOW","text":"Step 1 <p>Initial setup:</p> <ul> <li>Installed OpenCV and Mediapipe.</li> <li>Set up a webcam feed for video capture.  </li> </ul> Step 2 <p>Pose detection:</p> <ul> <li>Used Mediapipe's Pose module to identify body landmarks.</li> </ul> Step 3 <p>Angle calculation:</p> <ul> <li>Implemented a function to calculate the angle between shoulder, elbow, and wrist.</li> </ul> Step 4 <p>Rep detection:</p> <ul> <li>Monitored elbow angles to track upward and downward movements.</li> </ul> Step 5 <p>Real-time feedback:</p> <ul> <li>Displayed the remaining number of curls on the video feed using OpenCV.</li> </ul> Step 6 <p>Completion:</p> <ul> <li>Stopped the program when the target reps were completed or on manual exit.</li> </ul>"},{"location":"computer-vision/bicep-reps-counting/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. Simplicity:<ul> <li>Using elbow angles alone may not handle all body postures but ensures simplicity.</li> <li>Solution: Fine-tuned angle thresholds and added tracking for alternating arms.</li> </ul> </li> </ul> <ul> <li>Real-Time Performance vs. Model Complexity:<ul> <li>Mediapipe's lightweight solution ensured smooth processing over heavier models.</li> </ul> </li> </ul>"},{"location":"computer-vision/bicep-reps-counting/#screenshots","title":"SCREENSHOTS","text":"<ol> <li> <p>Entering no of reps you want to perform    </p> </li> <li> <p>Performing reps    </p> </li> </ol> <p>Project workflow</p> <pre><code>graph LR  \nA[Webcam Feed] --&gt; F[Enter No of Biceps Reps]\nF --&gt; B[Mediapipe Pose Detection]  \nB --&gt; C[Elbow Angle Calculation]  \nC --&gt; D[Rep Count Decrement]  \nD --&gt; E[Real-Time Update on Frsame]  </code></pre>"},{"location":"computer-vision/bicep-reps-counting/#conclusion","title":"CONCLUSION","text":""},{"location":"computer-vision/bicep-reps-counting/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Real-time video processing using OpenCV.</li> <li>Pose detection and landmark analysis with Mediapipe.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Understanding geometric computations in pose analysis.</li> <li>Effective use of pre-trained models like Mediapipe Pose.</li> </ul> Challenges faced and how they were overcome <ul> <li>Challenge: Handling incorrect postures.<ul> <li>Solution: Fine-tuning angle thresholds.</li> </ul> </li> </ul>"},{"location":"computer-vision/bicep-reps-counting/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Personal Fitness Tracker - Helps users track their workouts without additional equipment.  </p> <p>Fitness App Integration - Can be integrated into fitness apps for real-time exercise tracking.  </p>"},{"location":"computer-vision/black-and-white-image-colorizer/","title":"Black and White Image Colorizer","text":""},{"location":"computer-vision/black-and-white-image-colorizer/#aim","title":"AIM","text":"<p>Colorization of Black and White Images using OpenCV and pre-trained caffe models.</p>"},{"location":"computer-vision/black-and-white-image-colorizer/#pre-trained-models","title":"PRE-TRAINED MODELS","text":"<p>colorization_deploy_v2.prototxt -  colorization_release_v2.caffemodel -  pts_in_hull.npy</p>"},{"location":"computer-vision/black-and-white-image-colorizer/#notebook-link","title":"NOTEBOOK LINK","text":"<p>Colab Notebook</p>"},{"location":"computer-vision/black-and-white-image-colorizer/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>numpy</li> <li>cv2</li> </ul>"},{"location":"computer-vision/black-and-white-image-colorizer/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project aims to perform colorization of black and white images.</li> <li>It involves in showcase the capabilities of OpenCV's DNN module and caffe models.</li> <li>It is done by processing given image using openCV and use Lab Color space model to hallucinate an approximation of how colorized version of the image \"might look like\".</li> </ul> Why is it necessary? <ul> <li>It helps preserving historical black-and-white photos. </li> <li>It can be used adding color to grayscale images for creative industries.  </li> <li>It acts an advancing computer vision applications in artistic and research fields.</li> </ul> How is it beneficial and used? <ul> <li>Personal use : It helps in restoring old family photographs.  </li> <li>Cultural and Political : it also enhances grayscale photographs of important historic events for modern displays. </li> <li>Creativity and Art  : it improves AI-based creative tools for artists and designers.  </li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Initial approach : reading various research papers and analyze different approaches on how to deal with this project.</li> <li>Identified Richzhang research paper on the title : Colorful Image colorization.</li> <li>Did some research on pre-trained models for image colorization.  </li> <li>Understood OpenCV's DNN module and its implementation.  </li> <li>Experimented with sample images to test model outputs. </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Richzhang's Colorful Image Colorization</li> <li>Lab Color space</li> <li>openCV Documentation </li> </ul>"},{"location":"computer-vision/black-and-white-image-colorizer/#explanation","title":"EXPLANATION","text":""},{"location":"computer-vision/black-and-white-image-colorizer/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4 <p>Initial data exploration and understanding:</p> <ul> <li>Load the grayscale input image.</li> <li>Load pre-trained caffe models using openCV dnn module.</li> </ul> <p>Data cleaning and preprocessing:</p> <ul> <li>Preprocess image to normalize and convert to LAB color space.</li> <li>Resize image for the network.</li> <li>Split L channel and perform mean subtraction.</li> <li>Predict ab channel from the input of L channel.</li> </ul> <p>Feature engineering and selection:</p> <ul> <li>Resize predicted ab channel's volume to same dimension as our image.</li> <li>Join L and predicted ab channel.</li> <li>Convert image from Lab back to RGB.</li> </ul> <p>Result : </p> <ul> <li>Resize and Show the Original and Colorized image.</li> </ul>"},{"location":"computer-vision/black-and-white-image-colorizer/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Computational efficiency vs. color accuracy.  </li> <li>Solution : Used optimized preprocessing pipelines to reduce runtime. </li> </ul> <ul> <li>Pre-trained model generalization vs. custom training.  </li> <li>Solution : Choose the pre-trained model for faster implementation and reliable results.  </li> </ul>"},{"location":"computer-vision/black-and-white-image-colorizer/#screenshots","title":"SCREENSHOTS","text":"<p>Project structure or tree diagram</p> <pre><code>  graph LR  \n    A[Load Grayscale Image] --&gt; B[Preprocess Image];  \n    B --&gt; C[Load Pre-trained Model];  \n    C --&gt; D[Predict A and B Channels];  \n    D --&gt; E[Combine with L Channel];  \n    E --&gt; F[Convert to RGB];  \n    F --&gt; G[Display/Save Colorized Image];</code></pre> Visualizations of results Original ImageColorized ImageResult <p></p> <p></p> <p></p>"},{"location":"computer-vision/black-and-white-image-colorizer/#conclusion","title":"CONCLUSION","text":""},{"location":"computer-vision/black-and-white-image-colorizer/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Color Space : LAB color space facilitates colorization tasks.  </li> <li>Pre-trained Models : Pre-trained models can generalize across various grayscale images.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>OpenCV : Enhanced knowledge of OpenCV's DNN module.  </li> <li>Caffe Models : Usage of pre-trained models.</li> <li>Image Dimensionality : Understanding how Image can be manipulated.</li> </ul> Challenges faced and how they were overcome <ul> <li>Color Space Conversion : Initial difficulties with LAB to RGB conversion; resolved using OpenCV documentation. </li> </ul>"},{"location":"computer-vision/black-and-white-image-colorizer/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Image Restoration</p> <ul> <li>Restoring old family photographs to vivid colors.</li> </ul> <p>Creative Industries</p> <ul> <li>Colorizing artistic grayscale sketches for concept designs.</li> </ul>"},{"location":"computer-vision/brightness-control/","title":"\ud83d\udcdc Brightness control","text":""},{"location":"computer-vision/brightness-control/#aim","title":"\ud83c\udfaf AIM","text":"<p>To develop a real-time brightness control system using hand gestures, leveraging OpenCV and MediaPipe for hand detection and brightness adjustment.</p>"},{"location":"computer-vision/brightness-control/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>No dataset used</p>"},{"location":"computer-vision/brightness-control/#notebook-link","title":"\ud83d\udcd3 NOTEBOOK LINK","text":"<p>https://drive.google.com/file/d/1q7kraajGykfc2Kb6-84dCOjkrDGhIQcy/view?usp=sharing</p>"},{"location":"computer-vision/brightness-control/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks OpenCV, NumPy, MediaPipe, cvzone Tools Jupyter Notebook, Local Python IDE"},{"location":"computer-vision/brightness-control/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project requires a webcam to capture real-time video and detect hand gestures for brightness control.</li> </ul> How is it beneficial and used? <ul> <li>Allows users to control screen brightness without physical touch, making it useful for touchless interfaces. </li> <li>Ideal for applications in smart home systems and assistive technologies. </li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Identified the need for a touchless brightness control system. </li> <li>Selected OpenCV for video processing and MediaPipe for efficient hand tracking. </li> <li>Developed a prototype to calculate brightness based on hand distance. </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>OpenCV documentation for video processing. </li> <li>MediaPipe's official guide for hand tracking. </li> </ul>"},{"location":"computer-vision/brightness-control/#explanation","title":"\ud83d\udd0d EXPLANATION","text":""},{"location":"computer-vision/brightness-control/#details-of-the-different-features","title":"\ud83e\udde9 DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"computer-vision/brightness-control/#developed-features","title":"\ud83d\udee0 Developed Features","text":"Feature Name Description Reason Hand Detection Detects hand gestures in real-time To control brightness with gestures Distance Calculation Calculates distance between fingers To adjust brightness dynamically Brightness Mapping Maps hand distance to brightness levels Ensures smooth adjustment of brightness"},{"location":"computer-vision/brightness-control/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>Project workflow</p> <pre><code>  graph LR\nA[Start] --&gt; B[Initialize Webcam];\nB --&gt; C[Detect Hand Gestures];\nC --&gt; D[Calculate Distance];\nD --&gt; E[Adjust Brightness];\nE --&gt; F[Display Output];</code></pre> Step 1 <ul> <li>Initialize the webcam using OpenCV.</li> </ul> Step 2 <ul> <li>Use MediaPipe to detect hands in the video feed.</li> </ul> Step 3 <ul> <li>Calculate the distance between two fingers (e.g., thumb and index).</li> </ul> Step 4 <ul> <li>Map the distance to a brightness range.</li> </ul> Step 5 <ul> <li>Display the adjusted brightness on the video feed.</li> </ul>"},{"location":"computer-vision/brightness-control/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"Section 1: Webcam Initialization <ul> <li> <p>The program begins by setting up the webcam to capture frames with a resolution of 640x480 pixels. This ensures consistent processing and visualization of the video stream.</p> <pre><code>cap = cv2.VideoCapture(0)  \ncap.set(3, 640)  # Set width  \ncap.set(4, 480)  # Set height \n</code></pre> </li> </ul> Section 2: Hand Detection and Brightness Control <ul> <li> <p>Using the <code>HandDetector</code> from <code>cvzone</code>, the program tracks one hand (maxHands=1). The brightness of the video frame is dynamically adjusted based on the distance between the thumb and index finger.</p> <pre><code>detector = HandDetector(detectionCon=0.8, maxHands=1)  \nbrightness = 1.0  # Default brightness level  \n</code></pre> </li> <li> <p>The HandDetector detects hand landmarks in each frame with a confidence threshold of 0.8. The initial brightness is set to 1.0 (normal).</p> <pre><code>hands, img = detector.findHands(frame, flipType=False)  \n\nif hands:  \n    hand = hands[0]  \n    lm_list = hand['lmList']  \n    if len(lm_list) &gt; 8:  \n        thumb_tip = lm_list[4]  \n        index_tip = lm_list[8]  \n        distance = int(((thumb_tip[0] - index_tip[0]) ** 2 + (thumb_tip[1] - index_tip[1]) ** 2) ** 0.5)  \n        brightness = np.interp(distance, [20, 200], [0, 1])  \n</code></pre> </li> <li> <p>The program calculates the distance between the thumb tip (<code>lmList[4]</code>) and index finger tip (<code>lmList[8]</code>). This distance is mapped to a brightness range of 0 to 1 using np.interp.</p> </li> </ul> Section 3: Brightness Adjustment and Display  <ul> <li> <p>The captured frame's brightness is modified by scaling the value (V) channel in the HSV color space according to the calculated brightness level.</p> <pre><code>hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  \nhsv[..., 2] = np.clip(hsv[..., 2] * brightness, 0, 255).astype(np.uint8)  \nframe_bright = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)  \ncv2.imshow(\"Brightness Controller\", frame_bright)  \n</code></pre> </li> <li> <p>This technique ensures smooth, real-time brightness adjustments based on the user's hand gestures. The output frame is displayed with the adjusted brightness level.</p> </li> </ul>"},{"location":"computer-vision/brightness-control/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Real-time processing vs. computational efficiency: Optimized hand detection by limiting the maximum number of detectable hands to 1.</li> </ul> <ul> <li>Precision in brightness control vs. usability: Adjusted mapping function to ensure smooth transitions.</li> </ul>"},{"location":"computer-vision/brightness-control/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"Working of the model Image Topic"},{"location":"computer-vision/brightness-control/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"computer-vision/brightness-control/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Improved understanding of real-time video processing. </li> <li>Learned to integrate gesture detection with hardware functionalities.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Gained insights into MediaPipe's efficient hand detection algorithms.</li> </ul>"},{"location":"computer-vision/brightness-control/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Smart Homes <ul> <li>Touchless brightness control for smart home displays.</li> </ul> Assistive Technologies <ul> <li>Brightness adjustment for users with limited mobility.</li> </ul>"},{"location":"computer-vision/face-detection/","title":"Face Detection","text":""},{"location":"computer-vision/face-detection/#aim","title":"AIM","text":"<p>The goal of this project is to build a face detection system using OpenCV, which identifies faces in static images using Haar Cascades.</p>"},{"location":"computer-vision/face-detection/#dataset-link","title":"DATASET LINK","text":"<p>For this project we are going to use the pretrained Haar Cascade XML file for face detection from OpenCV's Github repository. </p> <p>https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml</p>"},{"location":"computer-vision/face-detection/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1upcl9sa5cL5fUuVLBG5IVuU0xPYs3Nwf#scrollTo=94ggAdg5AnUk</p>"},{"location":"computer-vision/face-detection/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>OpenCV</li> <li>Random</li> <li>Matplotlib</li> </ul>"},{"location":"computer-vision/face-detection/#description","title":"DESCRIPTION","text":"<p>This project involves building a face detection model using OpenCV's pre-trained Haar Cascade Classifiers to detect faces in images.</p> <p>What is the requirement of the project?</p> <ul> <li>A face detection system is needed for various applications such as security, attendance tracking, and facial recognition systems.</li> <li>This project demonstrates a basic use of computer vision techniques for detecting faces in static images.</li> </ul> Why is it necessary? <ul> <li>Face detection is the first crucial step in many computer vision applications such as face recognition and emotion analysis.</li> <li>It is an essential component in systems that require human identification or verification.</li> </ul> How is it beneficial and used? <ul> <li>Face detection can be used in automation systems, for example, in attendance tracking, photo tagging, and security surveillance.</li> <li>It enables various applications in user experience enhancement and biometric systems.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>I began by exploring OpenCV documentation, focusing on how to implement Haar Cascade for face detection.</li> <li>Initially, I focused on static image detection, planning to extend the project to video-based detection in the future.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>OpenCV documentation</li> <li>Book: \"Learning OpenCV 3\" by Adrian Kaehler and Gary Bradski</li> </ul>"},{"location":"computer-vision/face-detection/#explanation","title":"EXPLANATION","text":""},{"location":"computer-vision/face-detection/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<pre><code>- Haar Cascade Classifier: A machine learning-based approach for detecting objects in images or video. It works by training on a large set of positive and negative images of faces.\n- Cascade Classifier*: The classifier works through a series of stages, each aimed at increasing detection accuracy.\n- Face Detection: The primary feature of this project is detecting human faces in static images, which is the first step in many facial recognition systems.\n</code></pre>"},{"location":"computer-vision/face-detection/#project-workflow","title":"PROJECT WORKFLOW","text":"Step 1 <p>Initial data exploration and understanding:</p> <ul> <li>Research the Haar Cascade method for face detection in OpenCV.</li> <li>Collect sample images for testing the model's performance.</li> </ul> Step 2 <p>Data cleaning and preprocessing:</p> <ul> <li>Ensure all input images are properly formatted (e.g., grayscale images for face detection).</li> <li>Resize or crop images to ensure optimal processing speed.</li> </ul> Step 3Step 4Step 5 <p>Feature engineering and selection: - Use pre-trained Haar Cascade classifiers for detecting faces. - Select the appropriate classifier based on face orientation and conditions (e.g., frontal face, profile).</p> <p>Model training and evaluation: - Use OpenCV's pre-trained Haar Cascade models. - Test the detection accuracy on various sample images.</p> <p>Model optimization and fine-tuning:</p> <ul> <li>Adjust parameters such as scale factor and minNeighbors to enhance accuracy.</li> <li>Experiment with different input image sizes to balance speed and accuracy.</li> </ul> Step 6 <p>Validation and testing:</p> <ul> <li>Validate the model's effectiveness on different test images, ensuring robust detection.</li> <li>Evaluate the face detection accuracy based on diverse lighting and image conditions.</li> </ul>"},{"location":"computer-vision/face-detection/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. computational efficiency.<ul> <li>Solution: Fine-tuned classifier parameters to ensure a balance between accuracy and speed.</li> </ul> </li> </ul> <ul> <li>Detection performance vs. image resolution. <ul> <li>Solution: Optimized input image resolution and processing flow to ensure both fast processing and accurate detection.</li> </ul> </li> </ul>"},{"location":"computer-vision/face-detection/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\nA[Start] --&gt; B{Face Detected?}\nB --&gt;|Yes| C[Mark Face]\nC --&gt; D[Display Result]\nB --&gt;|No| F[Idle/Do Nothing]</code></pre>"},{"location":"computer-vision/face-detection/#conclusion","title":"CONCLUSION","text":""},{"location":"computer-vision/face-detection/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Gained an understanding of face detection using Haar Cascades. </li> <li>Improved ability to optimize computer vision models for accuracy and speed.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned how to handle trade-offs between accuracy and speed in real-time applications. </li> <li>Gained hands-on experience with the implementation of object detection algorithms.</li> </ul> Challenges faced and how they were overcome <ul> <li>Challenge: Low detection accuracy in poor lighting conditions. </li> <li>Solution: Adjusted classifier parameters and added preprocessing steps to improve accuracy.</li> </ul>"},{"location":"computer-vision/face-detection/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Security Surveillance Systems</p> <ul> <li>Used for identifying individuals or monitoring for intruders in secure areas.</li> </ul> <p>Attendance Systems</p> <ul> <li>Used to automate attendance tracking by detecting the faces of students or employees.</li> </ul>"},{"location":"data-visualization/","title":"\ud83d\udcca Data Visualization","text":"Bangladesh Premier League <p>Team performances, player stats &amp; key insights.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 10 min read</p>"},{"location":"data-visualization/bangladesh-premier-league-analysis/","title":"\ud83d\udcdc Bangladesh Premier League Analysis","text":""},{"location":"data-visualization/bangladesh-premier-league-analysis/#aim","title":"\ud83c\udfaf AIM","text":"<p>To analyze player performances in the Bangladesh Premier League by extracting insights from batsmen, bowlers, and match data\u2014ranging from toss outcomes to overall match results.</p>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>https://www.kaggle.com/abdunnoor11/bpl-data</p>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#kaggle-notebook","title":"\ud83d\udcd3 KAGGLE NOTEBOOK","text":"<p>https://www.kaggle.com/code/avdhesh15/bpl-analysis</p> Kaggle Notebook <p> </p>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks Matplotlib, Pandas, Seaborn, Numpy Tools Git, Jupyter, VS Code"},{"location":"data-visualization/bangladesh-premier-league-analysis/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>This project aims to analyze player performance data from the Bangladesh Premier League (BPL) to classify players into categories such as best, good, average, and poor based on their performance.</li> <li>The analysis provides valuable insights for players and coaches, highlighting who needs more training and who requires less, which can aid in strategic planning for future matches.</li> </ul> How is it beneficial and used? <ul> <li>For Players: Provides feedback on their performance, helping them to improve specific aspects of their game.</li> <li>For Coaches: Helps in identifying areas where players need improvement, which can be focused on during training sessions.</li> <li>For Team Management: Assists in strategic decision-making regarding player selection and match planning.</li> <li>For Fans and Analysts: Offers insights into player performances and trends over the league, enhancing the understanding and enjoyment of the game.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Perform initial data exploration to understand the structure and contents of the dataset.</li> <li>To learn about the topic and searching the related content like <code>what is league</code>, <code>About bangladesh league</code>, <code>their players</code> and much more.</li> <li>Learn about the features in details by searching on the google or quora.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Articles on cricket analytics from websites such as ESPNcricinfo and Cricbuzz.</li> <li>https://www.linkedin.com/pulse/premier-league-202223-data-analysis-part-i-ayomide-aremu-cole-iwn4e/</li> <li>https://analyisport.com/insights/how-is-data-used-in-the-premier-league/</li> </ul>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#project-explanation","title":"\ud83d\udd0d PROJECT EXPLANATION","text":""},{"location":"data-visualization/bangladesh-premier-league-analysis/#dataset-overview-feature-details","title":"\ud83e\udde9 DATASET OVERVIEW &amp; FEATURE DETAILS","text":"\ud83d\udcc2 bpl.csv <ul> <li>There are 19 features in <code>BPL Dataset</code></li> </ul> Feature Name Description Datatype id All matches unique id int64 season Season of the match (20XX-XX) object match_no Number of matches (For eg, Round 1st, 3rd, Final) object date Date of the match object team_1 Name of the first team object team_1_score Scoreboard of the team 1 (runs/wickets) object team_2 Second Team object team_2_score Scoreboard of the team 2 (runs/wickets) object player_of_match Player of the match object toss_winner Which team won the toss? object toss_decision Toss winner team decision took either 'field first' or 'bat first' object winner Name of the team who won the match object venue Venue of the match object city City of the match object win_by_wickets Team win by how many wickets int64 win_by_runs Team win by how many runs int64 result Conclusion of <code>win_by_wickets</code> &amp; <code>win_by_runs</code> object umpire_1 Name of the first umpire object umpire_2 Name of the second umpire object \ud83d\udee0 Developed Features from bpl.csv Feature Name Description Reason Datatype team_1_run Run scored by the team 1 To covert <code>team_1_score</code> categorical feature into numerical feature int64 team_1_wicket Wickets losed by the team 1 To covert <code>team_1_score</code> categorical feature into numerical feature int64 team_2_run Run scored by the team 2 To covert <code>team_2_score</code> categorical feature into numerical feature int64 team_2_wicket Wickets losed by the team 2 To covert <code>team_1_score</code> categorical feature into numerical feature int64 \ud83d\udcc2 batsman.csv <ul> <li>There are 12 features in <code>Batsman Dataset</code></li> </ul> Feature Name Description Datatype id All matches unique id int64 season Season of the match (20XX-XX) object match_no Number of matches (For eg, Round 1st, 3rd, Final) object date Date of the match object player_name Player Name object comment How did the batsman get out? object R Batsman's run int64 B How many balls faced the batsman? int64 M How long their innings was in minutes? int64 fours No. of fours int64 sixs No. of sixes int64 SR Strike rate <code>(R/B)*100</code> float64 \ud83d\udcc2 bowler.csv <ul> <li>There are 12 features in <code>Bowler Dataset</code></li> </ul> Feature Name Description Datatype id All matches unique id int64 season Season of the match (20XX-XX) object match_no Number of matches (For eg, Round 1st, 3rd, Final) object date Date of the match object player_name Player Name object O No. of overs bowled float64 M No. of middle overs bowled int64 R No. of runs losed int64 W No. of wickets secured int64 ECON The average number of runs they have conceded per over bowled float64 WD No. of wide balls int64 NB No. of No balls int64"},{"location":"data-visualization/bangladesh-premier-league-analysis/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>Project workflow</p> <pre><code>  graph TD\n    A[Start] --&gt; B[Load Dataset]\n    B --&gt;|BPL Data| C[Preprocess BPL Data]\n    B --&gt;|Batsman Data| D[Preprocess Batsman Data]\n    B --&gt;|Bowler Data| E[Preprocess Bowler Data]\n\n    C --&gt; F{Generate Queries?}\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt;|Yes| G[Graphical Visualizations]\n    G --&gt; H[Insights &amp; Interpretation]\n    H --&gt; I[End]\n\n    F --&gt;|No| I[End]</code></pre> Step 1Step 2Step 3Step 4Step 5 <ul> <li>Read and explore all datasets individually.</li> <li>Started with <code>bpl.csv</code>, analyzing its structure and features.</li> <li>Researched dataset attributes through Google and Bangladesh cricket series data.</li> <li>Reviewed relevant Kaggle notebooks to gain insights from existing work.</li> </ul> <ul> <li>Performed basic EDA to understand data distribution.</li> <li>Identified and handled missing values.</li> <li>Converted categorical features into numerical representations to enrich analysis.</li> <li>Examined dataset properties using <code>info()</code> and <code>describe()</code> functions.</li> </ul> <ul> <li>Developed a custom function <code>plotValueCounts(df, col, size=(10,5))</code> to visualize feature distributions.</li> <li>This function, built using Seaborn and Matplotlib, plays a key role in extracting insights.</li> <li>Generates count plots with labeled bar values for better interpretability.</li> </ul> <ul> <li>Refined <code>bpl.csv</code> by merging teams with their respective divisions to prevent duplicate counting.</li> <li>Addressed inconsistencies:<ul> <li>Resolved two tie matches recorded in the dataset.</li> <li>Extracted team runs and wickets from match scoreboards and transformed them into numerical features.</li> </ul> </li> </ul> <ul> <li>Implemented automated querying using the <code>plotValueCounts()</code> function.</li> <li>Performed grouped analysis of winners by seasons.</li> </ul>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#key-queries","title":"\u2753 KEY QUERIES","text":"<ul> <li>Key Queries on <code>bpl.csv</code> Dataset:<ul> <li>Won the toss, took the bat first and won the match.</li> <li>Won the toss, took the bat first.</li> <li>Winning rate by taking <code>bat first</code> vs <code>field first</code>.</li> </ul> </li> <li>Key Queries on <code>batsman.csv</code> Dataset:<ul> <li>Batsman who scored more than 1 century or more in a match.</li> <li>Batsman who faced 50 balls or more in a match.</li> <li>Batsman hit more than 10 fours.</li> </ul> </li> <li>Key Queries on <code>bowler.csv</code> Dataset:<ul> <li>Bowler conceded 50 runs or more</li> <li>Bowler took more than 4 wickets</li> <li>Bowler delivered 4 wide or more</li> </ul> </li> </ul>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"plotValueCounts() function <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plotValueCounts(df, col, size=(10, 5)):\n    val_counts = df[col].value_counts()\n    plt.figure(figsize=size)\n    ax = sns.countplot(y=col, data=df, order=val_counts.index, palette=\"pastel\")\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%d', label_type='edge', padding=3, fontsize=10, color='black')\n    plt.title(f\"Value Counts of {col}\")\n    plt.show()\n</code></pre> <ul> <li>It displays the visualization graph of value counts of any feature of the dataset.</li> </ul>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2Trade Off 3Trade Off 4 <ul> <li>Trade-off: The dataset includes multiple sources, but some features required external validation (e.g., team names, score formats).</li> <li>Solution: Standardized team names and resolved inconsistencies by merging duplicate team entries based on divisions.</li> </ul> <ul> <li>Trade-off: Performing detailed feature engineering (e.g., extracting runs, wickets, and match insights) can increase computational overhead.</li> <li>Solution: Optimized preprocessing by transforming scoreboard data into structured numerical features, reducing redundant calculations.</li> </ul> <ul> <li>Trade-off: Count plots with high-cardinality features could lead to cluttered or unreadable visualizations.</li> <li>Solution: Implemented <code>plotValueCounts()</code> with bar labels and sorting to enhance clarity while retaining key insights.</li> </ul> <ul> <li>Trade-off: Writing fixed queries could limit adaptability when new data is introduced.</li> <li>Solution: Developed a function-based querying approach (e.g., <code>plotValueCounts(df, col)</code>) to enable flexible, real-time insights.</li> </ul>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"<p>Visualizations and EDA of different features</p> Toss WinnerWinner TeamsWin Match By WicketsWin Match By Runswinners By SeasonVenue of MatchTop 10 Player of MatchWinning Rate By Toss DecisionBatsman scored more than 1 centuryBowler took more than 4 wickets <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"data-visualization/bangladesh-premier-league-analysis/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Extracted meaningful player statistics by analyzing batting and bowling performances across multiple seasons.</li> <li>Identified trends in match outcomes based on factors like toss decisions, innings strategies, and scoring patterns.</li> <li>Enhanced data visualization techniques to present key insights effectively for better decision-making.</li> </ul>"},{"location":"data-visualization/bangladesh-premier-league-analysis/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Application 1Application 2 <p>Team Strategy &amp; Selection  - Helps coaches analyze player performance for better team selection and match strategies.</p> <p>Player Performance Tracking  - Assists in monitoring player trends to improve training and development.</p>"},{"location":"deep-learning/","title":"Deep Learning \u2728","text":"Brain Tumor Detection Model <p>Deep learning algorithm for image and video recognition.</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 10 mins</p> Music Genre Classification Model <p>Unlock the rhythm of sound with AI-powered music genre classification using OpenCV!</p> <p>\ud83d\udcc5 2025-01-10 | \u23f1\ufe0f 12 min read</p> Time Series Anomaly Detection <p>A deep learning approach to detect anomalies in time series data.</p> <p>\ud83d\udcc5 2025-02-12 | \u23f1\ufe0f 10 mins</p>"},{"location":"deep-learning/anamoly-detection/","title":"\ud83d\udcdc Time-Series Anomaly Detection","text":""},{"location":"deep-learning/anamoly-detection/#aim","title":"\ud83c\udfaf AIM","text":"<p>To detect anomalies in time-series data using Long Short-Term Memory (LSTM) networks.</p>"},{"location":"deep-learning/anamoly-detection/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>[NOT USED]</p>"},{"location":"deep-learning/anamoly-detection/#kaggle-notebook","title":"\ud83d\udcd3 KAGGLE NOTEBOOK","text":"<p>https://www.kaggle.com/code/thatarguy/lstm-anamoly-detection/notebook</p> Kaggle Notebook <p> </p>"},{"location":"deep-learning/anamoly-detection/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks TensorFlow, Keras, scikit-learn, numpy, pandas, matplotlib Tools Jupyter Notebook, VS Code"},{"location":"deep-learning/anamoly-detection/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project focuses on identifying anomalies in time-series data using an LSTM autoencoder. The model learns normal patterns and detects deviations indicating anomalies.</li> </ul> Why is it necessary? <ul> <li>Anomaly detection is crucial in various domains such as finance, healthcare, and cybersecurity, where detecting unexpected behavior can prevent failures, fraud, or security breaches.</li> </ul> How is it beneficial and used? <ul> <li>Businesses can use it to detect irregularities in stock market trends.</li> <li>It can help monitor industrial equipment to identify faults before failures occur.</li> <li>It can be applied in fraud detection for financial transactions.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Understanding time-series anomaly detection methodologies.</li> <li>Generating synthetic data to simulate real-world scenarios.</li> <li>Implementing an LSTM autoencoder to learn normal patterns and detect anomalies.</li> <li>Evaluating model performance using Mean Squared Error (MSE).</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Research paper: \"Deep Learning for Time-Series Anomaly Detection\"</li> <li>Public notebook: LSTM Autoencoder for Anomaly Detection</li> </ul>"},{"location":"deep-learning/anamoly-detection/#project-explanation","title":"\ud83d\udd0d PROJECT EXPLANATION","text":""},{"location":"deep-learning/anamoly-detection/#dataset-overview-feature-details","title":"\ud83e\udde9 DATASET OVERVIEW &amp; FEATURE DETAILS","text":"\ud83d\udcc2 Synthetic dataset <ul> <li>The dataset consists of a sine wave with added noise.</li> </ul> Feature Name Description Datatype time Timestamp int64 value Sine wave value with noise float64"},{"location":"deep-learning/anamoly-detection/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Generate Data};\n    B --&gt; C[Normalize Data];\n    C --&gt; D[Create Sequences];\n    D --&gt; E[Train LSTM Autoencoder];\n    E --&gt; F[Compute Reconstruction Error];\n    F --&gt; G[Identify Anomalies];</code></pre> Step 1Step 2Step 3Step 4Step 5 <ul> <li>Generate synthetic data (sine wave with noise)</li> <li>Normalize data using MinMaxScaler</li> <li>Split data into training and validation sets</li> </ul> <ul> <li>Create sequential data using a rolling window approach</li> <li>Reshape data for LSTM compatibility</li> </ul> <ul> <li>Implement LSTM autoencoder for anomaly detection</li> <li>Optimize model using Adam optimizer</li> </ul> <ul> <li>Compute reconstruction error for anomaly detection</li> <li>Identify threshold for anomalies using percentile-based method</li> </ul> <ul> <li>Visualize detected anomalies using Matplotlib</li> </ul>"},{"location":"deep-learning/anamoly-detection/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"LSTM Autoencoder <ul> <li>The model consists of an encoder, bottleneck, and decoder.</li> <li>It learns normal time-series behavior and reconstructs it.</li> <li>Deviations from normal patterns are considered anomalies.</li> </ul>"},{"location":"deep-learning/anamoly-detection/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Reconstruction Error Threshold Selection <ul> <li>Setting a high threshold may miss subtle anomalies, while a low threshold might increase false positives.</li> <li>Solution: Use the 95th percentile of reconstruction errors as the threshold to balance false positives and false negatives.</li> </ul>"},{"location":"deep-learning/anamoly-detection/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"<p>Visualizations and EDA of different features</p> Synthetic Data Plot <p></p> Model performance graphs Reconstruction Error Plot <p></p>"},{"location":"deep-learning/anamoly-detection/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Reconstruction Error (MSE) LSTM Autoencoder 0.015"},{"location":"deep-learning/anamoly-detection/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"deep-learning/anamoly-detection/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Time-series anomalies often appear as sudden deviations from normal patterns.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned about LSTM autoencoders and their ability to reconstruct normal sequences.</li> </ul> Challenges faced and how they were overcome <ul> <li>Handling high reconstruction errors by tuning model hyperparameters.</li> <li>Selecting an appropriate anomaly threshold using statistical methods.</li> </ul>"},{"location":"deep-learning/anamoly-detection/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Financial Fraud DetectionPredictive Maintenance <ul> <li>Detect irregular transaction patterns using anomaly detection.</li> </ul> <ul> <li>Identify equipment failures in industrial settings before they occur.</li> </ul>"},{"location":"deep-learning/brain-tumor-detection-model/","title":"Brain Tumor Detectioon","text":""},{"location":"deep-learning/brain-tumor-detection-model/#aim","title":"AIM","text":"<p>To predict the Brain Tumor using Convolutional Neural Network</p>"},{"location":"deep-learning/brain-tumor-detection-model/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/primus11/brain-tumor-mri</p>"},{"location":"deep-learning/brain-tumor-detection-model/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://colab.research.google.com/github/11PRIMUS/ALOK/blob/main/Tumor3.ipynb</p>"},{"location":"deep-learning/brain-tumor-detection-model/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 for TunedThresholdClassifierCV)</li> <li>matplotlib</li> <li>seaborn</li> <li>streamlit</li> </ul>"},{"location":"deep-learning/brain-tumor-detection-model/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>This project aims to predict early stage brain tumor it uses Convolutional Neural Network to classify wheter tumor is present or not.</li> </ul> Why is it necessary? <ul> <li>Brain Tumor is leading case of deaths on world and most of the cases can be solved by detecting the cancer in its initial stages so one can take medication according to that without having further risks.</li> </ul> How is it beneficial and used? <ul> <li>Doctors can use it to detect cancer and the region affected by that using MRI scans an help patient to overcom ethat with right and proper guidance. It also acts as a fallback mechanism in rare cases where the diagnosis is not obvious.</li> <li>People (patients in particular) can check simply by using MRI scans to detect Tumor and take necessary medication and precautions</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Going through previous research and articles related to the problem.</li> <li>Data exploration to understand the features. </li> <li>Identifying key metrics for the problem based on ratio of target classes.</li> <li>Feature engineering and selection based on EDA.</li> <li>Setting up a framework for easier testing of multiple models even for peoples.</li> <li>Analysing results of models simply using MRI scans</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Research paper: Review of MRI-based Brain Tumor Image Segmentation Using Deep Learning Methods</li> <li>Public notebook: Brain Tumor Classification</li> </ul>"},{"location":"deep-learning/brain-tumor-detection-model/#model-architecture","title":"Model Architecture","text":"<pre><code>- The CNN architecture is designed to perform binary classification. The key layers used in the architecture are:\n- Convolutional Layers: For feature extraction from images. MaxPooling Layers: To downsample the image features. Dense Layers: To perform the classification. Dropout: For regularization to prevent overfitting.\n</code></pre>"},{"location":"deep-learning/brain-tumor-detection-model/#model-structure","title":"Model Structure","text":"<pre><code>- Input Layer 224*224 pixels.\n- Convolutionla layer followed by MaxPooling layers.\n- Flattern layer to convert feature into 1D vector\n- Fully connected layer for Classification.\n- Output Layer: Sigmoid activation for binary classification (tumor/no tumor)\n</code></pre>"},{"location":"deep-learning/brain-tumor-detection-model/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Splitting of data (70% for training, 15% for validation and 15% for testing)</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Data preperation using Image Generator</li> <li>Categorical feature encoding</li> <li>Image resized to 224*224 pixels</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Combining original features based on domain knowledge</li> <li>Using MobileNet to process input</li> </ul> <p>Modeling</p> <ul> <li>Convolutional layer followed by MaxPooling layer</li> <li>Flattering the layer to convert features into 1D vector</li> <li>Sigmoid function to actiavte binary classification</li> <li>Holdout dataset created or model testing</li> <li>Using VGG16 and ResNet for future improvement</li> </ul> <p>Result analysis</p> <ul> <li>Hosted on Streamlit to ensure one can easily upload MRI scans and detect wether canncer is present or not.</li> <li>Early stopping to ensure better accuracy is achieved.</li> <li>Experiment with differnt agumentation techniques to improve model's robustness.</li> </ul>"},{"location":"deep-learning/brain-tumor-detection-model/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Accuracy vs Validation_accuracy: The training accuracy is much higher than the validation accuracy after epoch 2, suggesting that the model may be overfitting the training data.</p> <ul> <li>Solution: It might be better to stop training around epoch 2 or 3 to avoid overfitting and ensure better generalization.</li> </ul>"},{"location":"deep-learning/brain-tumor-detection-model/#conclusion","title":"CONCLUSION","text":""},{"location":"deep-learning/brain-tumor-detection-model/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>Early detection of cancer can lead to easily reduce the leading death of person and take proper medication on that basis.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned and implemented the concept of predicting probability and tuning the prediction threshold for more accurate results, compared to directly predicting with the default thresold for models.</li> </ul> Challenges faced and how they were overcome <ul> <li>Resigning the RGB image to grayscale and reducing the pixel by 225 * 225 was big challenge. </li> <li>Lesser dataset so we reached out to some hospitals which helped us collecting the MRI scans.</li> </ul>"},{"location":"deep-learning/brain-tumor-detection-model/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <ul> <li>Doctors can identify the cancer stage and type accurately, allowing for tailored treatment approaches.</li> </ul> <ul> <li>Treatments at early stages are often less invasive and have fewer side effects compared to late-stage therapies</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/","title":"Music Genre Classification Model","text":""},{"location":"deep-learning/music-genre-classification-model/#aim","title":"AIM","text":"<p>To develop a precise and effective music genre classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), Random Forest and XGBoost Classifier algorithms for the Kaggle GTZAN Dataset Music Genre Classification. </p>"},{"location":"deep-learning/music-genre-classification-model/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/data</p>"},{"location":"deep-learning/music-genre-classification-model/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1j8RZccP2ee5XlWEFSkTyJ98lFyNrezHS?usp=sharing</p>"},{"location":"deep-learning/music-genre-classification-model/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>librosa</li> <li>matplotlib</li> <li>pandas</li> <li>sklearn</li> <li>seaborn</li> <li>numpy</li> <li>scipy</li> <li>xgboost</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The objective of this research is to develop a precise and effective music genre classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), Random Forest and XGBoost algorithms for the Kaggle GTZAN Dataset Music Genre Classification.</li> </ul> Why is it necessary? <ul> <li>Music genre classification has several real-world applications, including music recommendation, content-based music retrieval, and personalized music services. However, the task of music genre classification is challenging due to the subjective nature of music and the complexity of audio signals.</li> </ul> How is it beneficial and used? <ul> <li>For User: Provides more personalised music</li> <li>For Developers: A recommendation system for songs that are of interest to the user </li> <li>For Business: Able to charge premium for the more personalised and recommendation services provided</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Initially how the different sounds are structured. </li> <li>Learned how to represent sound signal in 2D format on graphs using the librosa library.</li> <li>Came to know about the various features of sound like <ul> <li>Mel-frequency cepstral coefficients (MFCC)</li> <li>Chromagram</li> <li>Spectral Centroid</li> <li>Zero-crossing rate</li> <li>BPM - Beats Per Minute</li> </ul> </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>https://scholarworks.calstate.edu/downloads/73666b68n</li> <li>https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/data</li> <li>https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#explanation","title":"EXPLANATION","text":""},{"location":"deep-learning/music-genre-classification-model/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<pre><code>There are 3 different types of the datasets.\n\n- genres_original\n- images_original\n- features_3_sec.csv\n- feature_30_sec.csv\n</code></pre> <ul> <li> <p>The features in <code>genres_original</code></p> <p>['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] Each and every genre has 100 WAV files</p> </li> <li> <p>The features in <code>genres_original</code></p> <p>['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] Each and every genre has 100 PNG files</p> </li> <li> <p>There are 60 features in <code>features_3_sec.csv</code></p> </li> <li> <p>There are 60 features in <code>features_30_sec.csv</code></p> </li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5Step 6Step 7 <ul> <li>Created data visual reprsentation of the data to help understand the data</li> </ul> <ul> <li>Found strong relationships between independent features and dependent feature using correlation.</li> </ul> <ul> <li>Performed Exploratory Data Analysis on data.</li> </ul> <ul> <li>Used different Classification techniques like SVM, Random Forest, </li> </ul> <ul> <li>Compared various models and used best performance model to make predictions.</li> </ul> <ul> <li>Used Mean Squared Error and R2 Score for evaluating model's performance.</li> </ul> <ul> <li>Visualized best model's performance using matplotlib and seaborn library.</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2Trade Off 3Trade Off 4 <p>How do you visualize audio signal</p> <ul> <li> <p>Solution: </p> </li> <li> <p>librosa: It is the mother of all audio file libraries</p> </li> <li>Plotting Graphs: As I have the necessary libraries to visualize the data. I started plotting the audio signals</li> <li>Spectogram:A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams. Here we convert the frequency axis to a logarithmic one.</li> </ul> <p>Features that help classify the data</p> <ul> <li> <p>Solution:</p> </li> <li> <p>Feature Engineering: What are the features present in audio signals</p> </li> <li>Spectral Centroid: Indicates where the \u201dcentre of mass\u201d for a sound is located and is calculated as the weighted mean of the frequencies present in the sound.</li> <li>Mel-Frequency Cepstral Coefficients: The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.</li> <li>Chroma Frequencies: Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.</li> </ul> <p>Performing EDA on the CSV files</p> <ul> <li> <p>Solution:</p> </li> <li> <p>Tool Selection: Used the correlation matrix on the features_30_sec.csv dataset to extract most related datasets</p> </li> <li>Visualization Best Practices: Followed best practices such as using appropriate chart types (e.g., box plots for BPM data, PCA plots for correlations), adding labels and titles, and ensuring readability.</li> <li>Iterative Refinement: Iteratively refined visualizations based on feedback and self-review to enhance clarity and informativeness.</li> </ul> <p>Implementing Machine Learning Models</p> <ul> <li> <p>Solution:</p> </li> <li> <p>Cross-validation: Used cross-validation techniques to ensure the reliability and accuracy of the analysis results.</p> </li> <li>Collaboration with Experts: Engaged with Music experts and enthusiasts to validate the findings and gain additional perspectives.</li> <li>Contextual Understanding: Interpreted results within the context of the music, considering factors such as mood of the users, surrounding, and specific events to provide meaningful and actionable insights.</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Visualizations and EDA of different features Harm PercSound WaveSTFTPop Mel-SpecBlues Mel-SpecSpec CentSpec RolloffMFCCChromogramCorr HeatmapBPM BoxplotPCA Scatter PlotConfusion Matrix <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"deep-learning/music-genre-classification-model/#models-used-and-their-accuracies","title":"MODELS USED AND THEIR ACCURACIES","text":"Model Accuracy KNN 0.80581 Random Forest 0.81415 Cross Gradient Booster 0.90123 SVM 0.75409"},{"location":"deep-learning/music-genre-classification-model/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> ACC Plot <p></p>"},{"location":"deep-learning/music-genre-classification-model/#conclusion","title":"CONCLUSION","text":"<pre><code>We can see that Accuracy plots of the different models.\nXGB Classifier can predict most accurate results for predicting the Genre of the music.\n</code></pre>"},{"location":"deep-learning/music-genre-classification-model/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>Discovered a new library that help visualize audio signal</li> <li>Discovered new features related to audio like STFT, MFCC, Spectral Centroid, Spectral Rolloff</li> <li>Gained a deeper understanding of the features of different genres of music</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Enhanced knowledge of data cleaning and preprocessing techniques to handle real-world datasets.</li> <li>Improved skills in exploratory data analysis (EDA) to extract meaningful insights from raw data.</li> <li>Learned how to use visualization tools to effectively communicate data-driven findings.</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <p>User Personalisation</p> <ul> <li>It can be used to provide more personalised music recommendation for users based on their taste in music or the various genres they listen to. This personalisation experience can be used to develop 'Premium' based business models.</li> </ul> <p>Compatability Between Users</p> <ul> <li>Based on the musical taste and the genres they listen we can identify the user behaviour and pattern come with similar users who can be friends with. This increases social interaction within the app.</li> </ul>"},{"location":"deep-learning/music-genre-classification-model/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1Feature 1 <ul> <li> <p>Real-time Compatability Tracking</p> </li> <li> <p>Implementing a real-time tracking system to view compatability between users.</p> </li> </ul> <ul> <li> <p>Predictive Analytics</p> </li> <li> <p>Using advanced machine learning algorithms to predict the next song the users is likely to listen to.</p> </li> </ul>"},{"location":"generative-adversarial-networks/","title":"Generative Adversarial Networks \ud83d\udcb1","text":""},{"location":"large-language-models/","title":"Large Language Models \ud83e\udd2a","text":""},{"location":"machine-learning/","title":"Machine Learning \ud83e\udd16","text":"Air Quality Prediction <p>Predicting Air Quality with Precision, One Sensor at a Time!</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 9 mins</p> Poker Hand Prediction <p>Predicting Poker Hands Using Machine Learning</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 7 mins</p> Heart Disease Detection <p>Early Detection of Heart Disease Using ML</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 8 mins</p> Used Cars Price Prediction <p>Accurate Price Predictions for Used Vehicles</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 6 mins</p> Sleep Quality Prediction <p>Predicting Sleep Quality Based on Lifestyle</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 5 mins</p> Insurance Cross-Sell Prediction <p>Predicting Vehicle Insurance Cross-Sell Opportunities</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 7 mins</p> Cardiovascular Disease Prediction <p>Predicting Cardiovascular Disease Risk</p> <p>\ud83d\udcc5 2025-01-26 | \u23f1\ufe0f 8 mins</p> Crop Recommendation Model <p>Smart Farming: AI-Powered Crop Recommendations for Better Yields!</p> <p>\ud83d\udcc5 2025-02-24 | \u23f1\ufe0f 10 mins</p> Autism Detection <p>Predicting Autism Using Machine Learning</p> <p>\ud83d\udcc5 2025-02-26 | \u23f1\ufe0f 8 mins</p>"},{"location":"machine-learning/air-quality-prediction/","title":"Air Quality Prediction Model","text":""},{"location":"machine-learning/air-quality-prediction/#aim","title":"\ud83c\udfaf AIM","text":"<p>To predict air quality levels based on various features such as CO (Carbon Monoxide), NO (Nitrogen Oxides), NO2 (Nitrogen Dioxide), O3 (Ozone), and other environmental factors. By applying machine learning models, this project explores how different algorithms perform in predicting air quality and understanding the key factors that influence it.</p>"},{"location":"machine-learning/air-quality-prediction/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set</p>"},{"location":"machine-learning/air-quality-prediction/#notebook","title":"\ud83d\udcd3 NOTEBOOK","text":"<p>https://www.kaggle.com/code/disha520/air-quality-predictor</p> Kaggle Notebook <p> </p>"},{"location":"machine-learning/air-quality-prediction/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks Pandas, Numpy, Matplotlib, Seaborn, Scikit-learn Tools Git, Jupyter, VS Code"},{"location":"machine-learning/air-quality-prediction/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>The project focuses on predicting air quality levels based on the features of air pollutants and environmental parameters.  The objective is to test various regression models to see which one gives the best predictions for CO (Carbon Monoxide) levels.</p> <p>What is the requirement of the project?</p> <ul> <li>Air quality is a critical issue for human health, and accurate forecasting models can provide insights to policymakers and the public. </li> <li>To accurately predict the CO levels based on environmental data.</li> </ul> How is it beneficial and used? <ul> <li>Predicting air quality can help in early detection of air pollution and assist in controlling environmental factors effectively.</li> <li>This model can be used by environmental agencies, city planners, and policymakers to predict and manage air pollution in urban areas, contributing to better public health outcomes.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Began by cleaning the dataset, handling missing data, and converting categorical features into numerical data.</li> <li>After preparing the data, various machine learning models were trained and evaluated to identify the best-performing model.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Kaggle kernels and documentation for additional dataset understanding.</li> <li>Tutorials on machine learning regression techniques, particularly for Random Forest, SVR, and Decision Trees.</li> </ul>"},{"location":"machine-learning/air-quality-prediction/#explanation","title":"\ud83d\udd0d EXPLANATION","text":""},{"location":"machine-learning/air-quality-prediction/#details-of-the-different-features","title":"\ud83e\udde9 DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"machine-learning/air-quality-prediction/#airqualitycsv","title":"\ud83d\udcc2 AirQuality.csv","text":"Feature Name Description CO(GT) Carbon monoxide in the air Date &amp; Time Record of data collection time PT08.S1(CO), PT08.S2(NMHC), PT08.S3(NOX), PT08.S4(NO2), PT08.S5(O3) These are sensor readings for different gas pollutants T, RH, AH Temperature, Humidity, and Absolute Humidity respectively, recorded as environmental factors"},{"location":"machine-learning/air-quality-prediction/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<pre><code>  graph LR\n    A[Start] --&gt; B{Is data clean?};\n    B --&gt;|Yes| C[Explore Data];\n    C --&gt; D[Data Preprocessing];\n    D --&gt; E[Feature Selection &amp; Engineering];\n    E --&gt; F[Split Data into Training &amp; Test Sets];\n    F --&gt; G[Define Models];\n    G --&gt; H[Train and Evaluate Models];\n    H --&gt; I[Visualize Evaluation Metrics];\n    I --&gt; J[Model Testing];\n    J --&gt; K[Conclusion and Observations];\n    B ----&gt;|No| L[Clean Data];</code></pre> Import Necessary LibrariesLoad DatasetData Cleaning ProcessVisualizing Correlations Between FeaturesData Preparation - Features (X) and Target (y)Split the Data into Training and Test SetsDefine ModelsTrain and Evaluate Each ModelVisualizing Model Evaluation MetricsConclusion and Observations <ul> <li>First, we import all the essential libraries needed for handling, analyzing, and modeling the dataset. </li> <li>This includes libraries like Pandas for data manipulation, Numpy for numerical computations, Matplotlib and Seaborn for data visualization, and Scikit-learn for machine learning models, evaluation, and data preprocessing. </li> <li>These libraries will enable us to perform all required tasks efficiently.</li> </ul> <ul> <li>We load the dataset using Pandas <code>read_csv()</code> function. The dataset contains air quality data, which is loaded with a semicolon delimiter. </li> <li>After loading, we inspect the first few rows to understand the structure of the data and ensure that the dataset is correctly loaded.</li> </ul> <p>Data cleaning is a crucial step in any project. In this step:</p> <ul> <li>We remove unnamed columns that aren't useful for analysis (such as 'Unnamed: 15', 'Unnamed: 16').</li> <li>We correct data consistency issues, specifically replacing commas with periods in numeric columns to ensure the correct parsing of values.</li> <li>Missing values in numeric columns are replaced with the mean of that respective column.</li> <li>We eliminate rows that consist entirely of missing values (NaN).</li> <li>A new datetime feature is created by combining the 'Date' and 'Time' columns.</li> <li>Additional temporal features such as month, day, weekday, and hour are derived from the new datetime feature.</li> <li>The original Date and Time columns are dropped as they are no longer needed.</li> </ul> <ul> <li>To understand relationships among the features, a heatmap is used to visualize correlations between all numeric columns. </li> <li>The heatmap highlights how features are correlated with each other, helping to identify possible redundancies or important predictors for the target variable.</li> </ul> <p>After cleaning the data, we separate the dataset into features (X) and the target variable (y):</p> <ul> <li>Features (X): These are the columns used to predict the target value. We exclude the target variable column \u2018CO(GT)\u2019 and include all other columns as features.</li> <li>Target (y): This is the variable we want to predict. We extract the 'CO(GT)' column and ensure all values are numeric.</li> </ul> <p>To prepare the data for machine learning, any non-numeric columns in the features (X) are encoded using <code>LabelEncoder</code>.</p> <ul> <li>We split the dataset into training and testing sets, allocating 80% of the data for training and the remaining 20% for testing. </li> <li>This split allows us to evaluate model performance on unseen data and validate the effectiveness of the model.</li> </ul> <p>We define multiple regression models to train and evaluate on the dataset:</p> <ul> <li>RandomForestRegressor: A robust ensemble method that performs well on non-linear datasets.</li> <li>LinearRegression: A fundamental regression model, useful for establishing linear relationships.</li> <li>SVR (Support Vector Regression): A regression model based on Support Vector Machines, useful for complex, non-linear relationships.</li> <li>DecisionTreeRegressor: A decision tree-based model, capturing non-linear patterns and interactions.</li> </ul> <p>Each model is trained on the training data and used to make predictions on the testing set. The performance is evaluated using two metrics:</p> <ul> <li>Mean Absolute Error (MAE): Measures the average error between predicted and actual values.</li> <li>R2 Score: Represents the proportion of the variance in the target variable that is predictable from the features.</li> </ul> <p>The evaluation metrics for each model are stored for comparison.</p> <p>We visualize the evaluation results for all models to get a comparative view of their performances. Two plots are generated:</p> <ul> <li>Mean Absolute Error (MAE) for each model, showing how much deviation there is between predicted and actual values.</li> <li>R2 Score, depicting the models' ability to explain the variability in the target variable. Higher R2 values indicate a better fit.</li> </ul> <p>These visualizations make it easy to compare model performances and understand which model is performing the best.</p> <ul> <li>In this final step, we summarize the results and draw conclusions based on the evaluation metrics. We discuss which model achieved the best performance in terms of both MAE and R2 Score, along with insights from the data cleaning and feature engineering steps. </li> <li>Key observations include the importance of feature selection, the efficacy of different models for regression tasks, and which model has the most accurate predictions based on the dataset at hand.</li> </ul>"},{"location":"machine-learning/air-quality-prediction/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":""},{"location":"machine-learning/air-quality-prediction/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Trade-off: Choosing between model accuracy and training time.</li> <li>Solution: Random Forest was chosen due to its balance between accuracy and efficiency, with SVR considered for its powerful predictive power despite longer training time.</li> </ul> <ul> <li>Trade-off: Model interpretability vs complexity.</li> <li>Solution: Decision trees were avoided in favor of Random Forest, which tends to be more robust in dealing with complex data and prevents overfitting.</li> </ul>"},{"location":"machine-learning/air-quality-prediction/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"<p>Visualizations and EDA of different features</p> HeatMapModel Comparison <p></p> <p></p>"},{"location":"machine-learning/air-quality-prediction/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Mean Absolute Error (MAE) R2 Score Random Forest Regressor 1.2391 0.885 Linear Regression 1.4592 0.82 SVR 1.3210 0.843 Decision Tree Regressor 1.5138 0.755"},{"location":"machine-learning/air-quality-prediction/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"machine-learning/air-quality-prediction/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Learned how different machine learning models perform on real-world data and gained insights into their strengths and weaknesses.</li> <li>Understood the significance of feature engineering and preprocessing to achieve better model performance.</li> <li>Data had missing values that required filling.</li> <li>Feature creation from datetime led to better prediction accuracy.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned how to effectively implement and optimize machine learning models using libraries like scikit-learn.</li> </ul>"},{"location":"machine-learning/air-quality-prediction/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Application 1Application 2 <p>Predicting Air Quality in Urban Areas</p> <ul> <li>Local governments can use this model to predict air pollution levels and take early actions to reduce pollution in cities.</li> </ul> <p>Predicting Seasonal Air Pollution Levels</p> <ul> <li>The model can help forecast air quality during different times of the year, assisting in long-term policy planning.</li> </ul>"},{"location":"machine-learning/autism-detection/","title":"\ud83c\udf1f Autism Spectrum Disorder (ASD) Detection using Machine Learning","text":""},{"location":"machine-learning/autism-detection/#aim","title":"\ud83c\udfaf AIM","text":"<p>To develop a machine learning model that predicts the likelihood of Autism Spectrum Disorder (ASD) based on behavioral and demographic features.</p>"},{"location":"machine-learning/autism-detection/#dataset-link","title":"\ud83c\udf0a DATASET LINK","text":"<p>Autism Screening Data </p>"},{"location":"machine-learning/autism-detection/#kaggle-notebook","title":"\ud83d\udcda KAGGLE NOTEBOOK","text":"<p>Autism Detection Kaggle Notebook</p> Kaggle Notebook <p></p>"},{"location":"machine-learning/autism-detection/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks Pandas, NumPy, Scikit-learn, Tools Jupyter Notebook, VS Code"},{"location":"machine-learning/autism-detection/#description","title":"\ud83d\udd8d DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The rise in Autism cases necessitates early detection.</li> <li>Traditional diagnostic methods are time-consuming and expensive.</li> <li>Machine learning can provide quick, accurate predictions to aid early intervention.</li> </ul> How is it beneficial and used? <ul> <li>Helps doctors and researchers identify ASD tendencies early.</li> <li>Reduces the time taken for ASD screening.</li> <li>Provides a scalable and cost-effective approach.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Collected and preprocessed the dataset.</li> <li>Explored different ML models for classification.</li> <li>Evaluated models based on accuracy and efficiency.</li> </ul>"},{"location":"machine-learning/autism-detection/#project-explanation","title":"\ud83d\udd0d PROJECT EXPLANATION","text":""},{"location":"machine-learning/autism-detection/#dataset-overview-feature-details","title":"\ud83e\udde9 DATASET OVERVIEW &amp; FEATURE DETAILS","text":"<p>The dataset consists of 800 rows and 22 columns, containing information related to autism spectrum disorder (ASD) detection based on various parameters.</p> Feature Name Description Datatype <code>ID</code> Unique identifier for each record <code>int64</code> <code>A1_Score</code> - <code>A10_Score</code> Responses to 10 screening questions (0 or 1) <code>int64</code> <code>age</code> Age of the individual <code>float64</code> <code>gender</code> Gender (<code>m</code> for male, <code>f</code> for female) <code>object</code> <code>ethnicity</code> Ethnic background <code>object</code> <code>jaundice</code> Whether the individual had jaundice at birth (<code>yes/no</code>) <code>object</code> <code>austim</code> Family history of autism (<code>yes/no</code>) <code>object</code> <code>contry_of_res</code> Country of residence <code>object</code> <code>used_app_before</code> Whether the individual used a screening app before (<code>yes/no</code>) <code>object</code> <code>result</code> Score calculated based on the screening test <code>float64</code> <code>age_desc</code> Age description (e.g., \"18 and more\") <code>object</code> <code>relation</code> Relation of the person filling out the form <code>object</code> <code>Class/ASD</code> ASD diagnosis label (<code>1</code> for ASD, <code>0</code> for non-ASD) <code>int64</code> <p>This dataset provides essential features for training a model to detect ASD based on questionnaire responses and demographic information.</p>"},{"location":"machine-learning/autism-detection/#project-workflow","title":"\ud83d\udee0 PROJECT WORKFLOW","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B[Data Preprocessing];\n    B --&gt; C[Feature Engineering];\n    C --&gt; D[Model Training];\n    D --&gt; E[Model Evaluation];\n    E --&gt; F[Deployment];</code></pre> Step 1Step 2Step 3Step 4Step 5 <ul> <li>Collected dataset and performed exploratory data analysis.</li> </ul> <ul> <li>Preprocessed data (handling missing values, encoding categorical data).</li> </ul> <ul> <li>Feature selection and engineering.</li> </ul> <ul> <li>Trained multiple classification models (Decision Tree, Random Forest, XGBoost).</li> </ul> <ul> <li>Evaluated models using accuracy, precision, recall, and F1-score.</li> </ul>"},{"location":"machine-learning/autism-detection/#code-explanation","title":"\ud83d\udda5\ufe0f CODE EXPLANATION","text":"Section 1: Data PreprocessingSection 2: Model Training <ul> <li>Loaded dataset and handled missing values.</li> </ul> <ul> <li>Implemented Logistic Regression and Neural Networks for classification.</li> </ul>"},{"location":"machine-learning/autism-detection/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. Model Interpretability: Used a Random Forest model instead of a deep neural network for better interpretability.</li> </ul> <ul> <li>Speed vs. Accuracy: Chose Logistic Regression for quick predictions in real-time applications.</li> </ul>"},{"location":"machine-learning/autism-detection/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"<p>Visualizations and EDA of different features</p> Age Distribution <p></p> Model performance graphs Confusion Matrix <p></p> Features Correlation Feature Correlation Heatmap <p></p>"},{"location":"machine-learning/autism-detection/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy Precision Recall F1-score Decision Tree 73% 0.71 0.73 0.72 Random Forest 82% 0.82 0.82 0.82 XGBoost 81% 0.81 0.81 081"},{"location":"machine-learning/autism-detection/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"machine-learning/autism-detection/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Behavioral screening scores are the strongest predictors of ASD.</li> <li>Family history and neonatal jaundice also show correlations with ASD diagnosis.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Feature selection and engineering play a crucial role in medical predictions.</li> <li>Trade-offs between accuracy, interpretability, and computational efficiency need to be balanced.</li> </ul>"},{"location":"machine-learning/autism-detection/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Early ASD ScreeningAssistive Diagnostic Tool <ul> <li>Helps parents and doctors identify ASD tendencies at an early stage.</li> </ul> <ul> <li>Can support psychologists in preliminary ASD assessments before clinical diagnosis.</li> </ul>"},{"location":"machine-learning/bulldozer-price-prediction/","title":"\ud83d\udcdc Project Title:  Bulldozer-Price-Prediction-using-ML","text":""},{"location":"machine-learning/bulldozer-price-prediction/#aim","title":"\ud83c\udfaf AIM","text":"<p>This project aims to predict the auction prices of bulldozers using machine learning techniques. The dataset used for this project comes from the Kaggle competition \"Blue Book for Bulldozers,\" which provides historical data on bulldozer sales.</p>"},{"location":"machine-learning/bulldozer-price-prediction/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>Kaggle Blue Book for Bulldozers</p>"},{"location":"machine-learning/bulldozer-price-prediction/#kaggle-notebook","title":"\ud83d\udcd3 KAGGLE NOTEBOOK","text":"<p>Kaggle Notebook</p>"},{"location":"machine-learning/bulldozer-price-prediction/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks Scikit Learn,Numpy,Pandas,Matplotlib"},{"location":"machine-learning/bulldozer-price-prediction/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>Requirement of the Project</p> <p>The project aims to predict the price of used bulldozers based on various factors such as equipment type, usage hours, manufacturing year, and other relevant parameters. The goal is to develop an accurate pricing model using Machine Learning (ML) techniques.</p> <p>Why is it Necessary?</p> <p>The construction and heavy machinery industry heavily relies on the resale of used equipment. Incorrect pricing can lead to financial losses for sellers or overpriced purchases for buyers. A data-driven approach helps ensure fair pricing, improving efficiency in the marketplace.</p> <p>How is it Beneficial and Used?</p> <ol> <li> <p>Helps businesses and individuals estimate bulldozer prices before buying or selling.</p> </li> <li> <p>Assists construction companies in budgeting for equipment procurement.</p> </li> <li> <p>Enables auction houses and dealerships to set competitive and data-backed prices.</p> </li> <li> <p>Reduces reliance on manual estimation, making pricing more transparent and objective.</p> </li> </ol> <p>Approach to the Project</p> <p>Data Collection \u2013 Gathered historical sales data of bulldozers, including features like sale date, equipment age, and location.</p> <p>Data Preprocessing \u2013 Cleaned missing values, handled categorical variables, and transformed data for ML models.</p> <p>Exploratory Data Analysis (EDA) \u2013 Identified key factors influencing bulldozer prices.</p> <p>Model Selection &amp; Training \u2013 Implemented and evaluated various ML models such as Random Forest, Gradient Boosting, and Linear Regression.</p> <p>Evaluation &amp; Optimization \u2013 Tuned hyperparameters and tested model performance using metrics like RMSE (Root Mean Squared Error).</p> <p>Deployment \u2013 Integrated the trained model into a user-friendly interface for real-world use.</p> <p>This project ensures a more systematic and accurate approach to bulldozer pricing, leveraging ML to enhance decision-making in the heavy equipment industry.</p>"},{"location":"machine-learning/bulldozer-price-prediction/#project-explanation","title":"\ud83d\udd0d PROJECT EXPLANATION","text":""},{"location":"machine-learning/bulldozer-price-prediction/#dataset-overview-feature-details","title":"\ud83e\udde9 DATASET OVERVIEW &amp; FEATURE DETAILS","text":"<p>SalesID   unique identifier of a particular sale of a machine at auction</p> <p>MachineID     identifier for a particular machine;  machines may have multiple sales</p> <p>ModelID   identifier for a unique machine model (i.e. fiModelDesc)</p> <p>datasource    source of the sale record;  some sources are more diligent about reporting attributes of the machine than others.  Note that a particular datasource may report on multiple auctioneerIDs.</p> <p>auctioneerID      identifier of a particular auctioneer, i.e. company that sold the machine at auction.  Not the same as datasource.</p> <p>YearMade      year of manufacturer of the Machine</p> <p>MachineHoursCurrentMeter      current usage of the machine in hours at time of sale (saledate);  null or 0 means no hours have been reported for that sale</p> <p>UsageBand     value (low, medium, high) calculated comparing this particular Machine-Sale hours to average usage for the fiBaseModel;  e.g. 'Low' means this machine has less hours given it's lifespan relative to average of fiBaseModel.</p> <p>Saledate      time of sale</p> <p>Saleprice     cost of sale in USD</p> <p>fiModelDesc   Description of a unique machine model (see ModelID); concatenation of fiBaseModel &amp; fiSecondaryDesc &amp; fiModelSeries &amp; fiModelDescriptor</p> <p>fiBaseModel   disaggregation of fiModelDesc</p> <p>fiSecondaryDesc   disaggregation of fiModelDesc</p> <p>fiModelSeries     disaggregation of fiModelDesc</p> <p>fiModelDescriptor     disaggregation of fiModelDesc</p> <p>ProductSize   Don't know what this is </p> <p>ProductClassDesc      description of 2nd level hierarchical grouping (below ProductGroup) of fiModelDesc</p> <p>State     US State in which sale occurred</p> <p>ProductGroup      identifier for top-level hierarchical grouping of fiModelDesc</p> <p>ProductGroupDesc      description of top-level hierarchical grouping of fiModelDesc</p> <p>Drive_System  machine configuration;  typcially describes whether 2 or 4 wheel drive</p> <p>Enclosure machine configuration - does machine have an enclosed cab or not</p> <p>Forks machine configuration - attachment used for lifting</p> <p>Pad_Type  machine configuration - type of treads a crawler machine uses</p> <p>Ride_Control  machine configuration - optional feature on loaders to make the ride smoother</p> <p>Stick machine configuration - type of control </p> <p>Transmission  machine configuration - describes type of transmission;  typically automatic or manual</p> <p>Turbocharged  machine configuration - engine naturally aspirated or turbocharged</p> <p>Blade_Extension   machine configuration - extension of standard blade</p> <p>Blade_Width   machine configuration - width of blade</p> <p>Enclosure_Type    machine configuration - does machine have an enclosed cab or not</p> <p>Engine_Horsepower machine configuration - engine horsepower rating</p> <p>Hydraulics    machine configuration - type of hydraulics</p> <p>Pushblock machine configuration - option</p> <p>Ripper    machine configuration - implement attached to machine to till soil</p> <p>Scarifier machine configuration - implement attached to machine to condition soil</p> <p>Tip_control   machine configuration - type of blade control</p> <p>Tire_Size machine configuration - size of primary tires</p> <p>Coupler   machine configuration - type of implement interface</p> <p>Coupler_System    machine configuration - type of implement interface</p> <p>Grouser_Tracks    machine configuration - describes ground contact interface</p> <p>Hydraulics_Flow   machine configuration - normal or high flow hydraulic system</p> <p>Track_Type    machine configuration - type of treads a crawler machine uses</p> <p>Undercarriage_Pad_Width   machine configuration - width of crawler treads</p> <p>Stick_Length  machine configuration - length of machine digging implement</p> <p>Thumb machine configuration - attachment used for grabbing</p> <p>Pattern_Changer   machine configuration - can adjust the operator control configuration to suit the user</p> <p>Grouser_Type  machine configuration - type of treads a crawler machine uses</p> <p>Backhoe_Mounting  machine configuration - optional interface used to add a backhoe attachment</p> <p>Blade_Type    machine configuration - describes type of blade</p> <p>Travel_Controls   machine configuration - describes operator control configuration</p> <p>Differential_Type machine configuration - differential type, typically locking or standard</p> <p>Steering_Controls machine configuration - describes operator control configuration</p>"},{"location":"machine-learning/bulldozer-price-prediction/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>The following steps are followed in building the machine learning model: 1. Data Preprocessing    - Handling missing values    - Feature engineering    - Encoding categorical variables</p> <ol> <li>Exploratory Data Analysis (EDA)</li> <li>Identifying trends and relationships</li> <li> <p>Visualizing key insights</p> </li> <li> <p>Model Selection and Training</p> </li> <li>Random Forest Regressor</li> <li> <p>Hyperparameter tuning using RandomizedSearchCV</p> </li> <li> <p>Model Evaluation</p> </li> <li>Root Mean Squared Log Error (RMSLE)</li> <li>R\u00b2 Score</li> </ol>"},{"location":"machine-learning/bulldozer-price-prediction/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy MSE R2 Score RandomForestRegressor 95% 0.022 0.832588403039663"},{"location":"machine-learning/bulldozer-price-prediction/#conclusion","title":"\u2705 CONCLUSION","text":"<p>The Bulldozer Price Prediction using ML project successfully demonstrates the power of machine learning in estimating the resale price of used bulldozers. By leveraging historical sales data and applying predictive modeling techniques, the project provides a data-driven approach to price estimation, reducing uncertainty and improving decision-making in the heavy equipment market. The final model helps sellers, buyers, and auction houses determine fair market prices, making the process more transparent and efficient.</p>"},{"location":"machine-learning/bulldozer-price-prediction/#key-learnings","title":"Key Learnings","text":"<ol> <li> <p>Data Quality Matters \u2013 Handling missing values, feature engineering, and proper data preprocessing significantly impact model performance.</p> </li> <li> <p>Feature Importance \u2013 Certain factors, such as equipment age, sale date, and operational hours, play a crucial role in price prediction.</p> </li> <li> <p>Model Selection &amp; Tuning \u2013 Experimenting with different machine learning models (Random Forest, Gradient Boosting, etc.) and optimizing hyperparameters enhances prediction accuracy.</p> </li> <li> <p>Evaluation Metrics \u2013 Understanding and applying RMSE and other performance metrics helps assess and improve model reliability.</p> </li> </ol>"},{"location":"machine-learning/bulldozer-price-prediction/#5-real-world-deployment-preparing-a-model-for-deployment-requires-considering-scalability-usability-and-integration-with-business-applications","title":"5. Real-World Deployment \u2013 Preparing a model for deployment requires considering scalability, usability, and integration with business applications.","text":""},{"location":"machine-learning/cardiovascular-disease-prediction/","title":"Cardiovascular Disease Prediction","text":""},{"location":"machine-learning/cardiovascular-disease-prediction/#aim","title":"AIM","text":"<p>To predict the risk of cardiovascular disease based on lifestyle factors.</p>"},{"location":"machine-learning/cardiovascular-disease-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/alphiree/cardiovascular-diseases-risk-prediction-dataset</p>"},{"location":"machine-learning/cardiovascular-disease-prediction/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/sid4ds/cardiovascular-disease-risk-prediction</p>"},{"location":"machine-learning/cardiovascular-disease-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 for TunedThresholdClassifierCV)</li> <li>matplotlib</li> <li>seaborn</li> <li>joblib</li> </ul>"},{"location":"machine-learning/cardiovascular-disease-prediction/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>This project aims to predict the risk of cardivascular diseases (CVD) based on data provided by people about their lifestyle factors. Predicting the risk in advance can minimize cases which reach a terminal stage.</li> </ul> Why is it necessary? <ul> <li>CVD is one of the leading causes of death globally. Using machine learning models to predict risk of CVD can be an important tool in helping the people affected by it.</li> </ul> How is it beneficial and used? <ul> <li>Doctors can use it as a second opinion to support their diagnosis. It also acts as a fallback mechanism in rare cases where the diagnosis is not obvious.</li> <li>People (patients in particular) can track their risk of CVD based on their own lifestyle and schedule an appointment with a doctor in advance to mitigate the risk.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Going through previous research and articles related to the problem.</li> <li>Data exploration to understand the features. Using data visualization to check their distributions.</li> <li>Identifying key metrics for the problem based on ratio of target classes.</li> <li>Feature engineering and selection based on EDA.</li> <li>Setting up a framework for easier testing of multiple models.</li> <li>Analysing results of models using confusion matrix.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Research paper: Integrated Machine Learning Model for Comprehensive Heart Disease Risk Assessment Based on Multi-Dimensional Health Factors</li> <li>Public notebook: Cardiovascular-Diseases-Risk-Prediction</li> </ul>"},{"location":"machine-learning/cardiovascular-disease-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"machine-learning/cardiovascular-disease-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range General_Health \"Would you say that in general your health is\u2014\" Categorical [Poor, Fair, Good, Very Good, Excellent] Checkup \"About how long has it been since you last visited a doctor for a routine checkup?\" Categorical [Never, 5 or more years ago, Within last 5 years, Within last 2 years, Within the last year] Exercise \"Did you participate in any physical activities like running, walking, or gardening?\" Categorical [Yes, No] Skin_Cancer Respondents that reported having skin cancer Categorical [Yes, No] Other_Cancer Respondents that reported having any other types of cancer Categorical [Yes, No] Depression Respondents that reported having a depressive disorder Categorical [Yes, No] Diabetes Respondents that reported having diabetes. If yes, specify the type. Categorical [Yes, No, No pre-diabetes or borderline diabetes, Yes but female told only during pregnancy] Arthritis Respondents that reported having arthritis Categorical [Yes, No] Sex Respondent's gender Categorical [Yes, No] Age_Category Respondent's age range Categorical ['18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-80', '80+'] Height_(cm) Respondent's height in cm Numerical Measured in cm Weight_(kg) Respondent's weight in kg Numerical Measured in kg BMI Respondent's Body Mass Index in kg/cm\u00b2 Numerical Measured in kg/cm\u00b2 Smoking_History Respondent's smoking history Categorical [Yes, No] Alcohol_Consumption Number of days of alcohol consumption in a month Numerical Integer values Fruit_Consumption Number of servings of fruit consumed in a month Numerical Integer values Green_Vegetables_Consumption Number of servings of green vegetables consumed in a month Numerical Integer values FriedPotato_Consumption Number of servings of fried potato consumed in a month Numerical Integer values"},{"location":"machine-learning/cardiovascular-disease-prediction/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Regrouping rare categories</li> <li>Categorical feature encoding</li> <li>Outlier clipping for numerical features</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Combining original features based on domain knowledge</li> <li>Discretizing numerical features</li> </ul> <p>Modeling</p> <ul> <li>Holdout dataset created or model testing</li> <li>Models trained: Logistic Regression, Decision Tree, Random Forest, AdaBoost, HistGradient Boosting, Multi-Layer Perceptron</li> <li>Class imbalance handled through:</li> <li>Class weights, when supported by model architecture</li> <li>Threshold tuning using TunedThresholdClassifierCV</li> <li>Metric for model-tuning: F2-score (harmonic weighted mean of precision and recall, with twice the weightage for recall)</li> </ul> <p>Result analysis</p> <ul> <li>Confusion matrix using predictions made on holdout test set</li> </ul>"},{"location":"machine-learning/cardiovascular-disease-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Accuracy vs Recall: Data is extremely imbalanced, with only ~8% representing the positive class. This makes accuracy unsuitable as a metric for our problem. It is critical to correctly predict all the positive samples, due to which we must focus on recall. However, this lowers the overall accuracy since some negative samples may be predicted as positive.</p> <ul> <li>Solution: Prediction threshold for models is tuned using F2-score to create a balance between precision and recall, with more importance given to recall. This maintains overall accuracy at an acceptable level while boosting recall.</li> </ul>"},{"location":"machine-learning/cardiovascular-disease-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Numerical feature distributions Height_(cm)Weight_(kg)BMIAlcoholFruitVegetableFried Patato <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> Correlations PearsonSpearman's RankKendall-Tau <p></p> <p></p> <p></p>"},{"location":"machine-learning/cardiovascular-disease-prediction/#models-used-and-their-accuracies","title":"MODELS USED AND THEIR ACCURACIES","text":"Model + Feature set Accuracy (%) Recall (%) Logistic Regression + Original 76.29 74.21 Logistic Regression + Extended 76.27 74.41 Logistic Regression + Selected 72.66 78.09 Decision Tree + Original 72.76 78.61 Decision Tree + Extended 74.09 76.69 Decision Tree + Selected 75.52 73.61 Random Forest + Original 73.97 77.33 Random Forest + Extended 74.10 76.61 Random Forest + Selected 74.80 74.05 AdaBoost + Original 76.03 74.49 AdaBoost + Extended 74.99 76.25 AdaBoost + Selected 74.76 75.33 Multi-Layer Perceptron + Original 76.91 72.81 Multi-Layer Perceptron + Extended 73.26 79.01 Multi-Layer Perceptron + Selected 74.86 75.05 Hist-Gradient Boosting + Original 75.98 73.49 Hist-Gradient Boosting + Extended 75.63 74.73 Hist-Gradient Boosting + Selected 74.40 75.85"},{"location":"machine-learning/cardiovascular-disease-prediction/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Logistic Regression</p> LR OriginalLR ExtendedLR Selected <p></p> <p></p> <p></p> Decision Tree DT OriginalDT ExtendedDT Selected <p></p> <p></p> <p></p> Random Forest RF OriginalRF ExtendedRF Selected <p></p> <p></p> <p></p> Ada Boost AB OriginalAB ExtendedAB Selected <p></p> <p></p> <p></p> Multi-Layer Perceptron MLP OriginalMLP ExtendedMLP Selected <p></p> <p></p> <p></p> Hist-Gradient Boosting HGB OriginalHGB ExtendedHGB Selected <p></p> <p></p> <p></p>"},{"location":"machine-learning/cardiovascular-disease-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"machine-learning/cardiovascular-disease-prediction/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ul> <li>General Health, Age and Co-morbities (such as Diabetes &amp; Arthritis) are the most indicative features for CVD risk.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned and implemented the concept of predicting probability and tuning the prediction threshold for more accurate results, compared to directly predicting with the default thresold for models.</li> </ul> Challenges faced and how they were overcome <ul> <li>Deciding the correct metric for evaluation of models due to imbalanced nature of the dataset. Since positive class is more important, Recall was used as the final metric for ranking models. </li> <li>F2-score was used to tune the threshold for models to maintain a balance between precision and recall, thereby maintaining overall accuracy.</li> </ul>"},{"location":"machine-learning/cardiovascular-disease-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <ul> <li>Doctors can use it as a second opinion when assessing a new patient. Model trained on cases from previous patients can be used to predict the risk.</li> </ul> <ul> <li>People (patients in particular) can use this tool to track the risk of CVD based on their own lifestyle factors and take preventive measures when the risk is high.</li> </ul>"},{"location":"machine-learning/cardiovascular-disease-prediction/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Different implementations of gradient-boosting models such as XGBoost, CatBoost, LightGBM, etc. were not implemented since none of the tree ensemble models such as Random Forest, AdaBoost or Hist-Gradient Boosting were among the best performers. Hence, avoid additional dependencies based on such models.</li> </ul>"},{"location":"machine-learning/crop-recommendation/","title":"Crop Recommendation Model","text":""},{"location":"machine-learning/crop-recommendation/#aim","title":"\ud83c\udfaf AIM","text":"<p>It is an AI-powered Crop Recommendation System that helps farmers and agricultural stakeholders determine the most suitable crops for cultivation based on environmental conditions. The system uses machine learning models integrated with Flask to analyze key parameters and suggest the best crop to grow in a given region.</p>"},{"location":"machine-learning/crop-recommendation/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>https://www.kaggle.com/datasets/atharvaingle/crop-recommendation-dataset/data</p>"},{"location":"machine-learning/crop-recommendation/#notebook","title":"\ud83d\udcd3 NOTEBOOK","text":"<p>https://www.kaggle.com/code/kashishkhurana1204/crop-recommendation-system</p> Kaggle Notebook <p> </p>"},{"location":"machine-learning/crop-recommendation/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks Pandas, Numpy, Matplotlib, Scikit-learn Tools Github, Jupyter, VS Code"},{"location":"machine-learning/crop-recommendation/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>To provide accurate crop recommendations based on environmental conditions.</li> <li>To assist farmers in maximizing yield and efficiency.</li> </ul> How is it beneficial and used? <ul> <li>Helps in optimizing agricultural planning.</li> <li>Reduces trial-and-error farming practices.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li> <p>Initial thoughts : The goal is to help farmers determine the most suitable crops based on their field\u2019s environmental conditions.</p> </li> <li> <p>Dataset Selection : I searched for relevant datasets on Kaggle that include soil properties, weather conditions, and nutrient levels such as nitrogen (N), phosphorus (P), and potassium (K).</p> </li> <li> <p>Initial Data Exploration : I analyzed the dataset structure to understand key attributes like soil pH, humidity, rainfall, and nutrient values, which directly impact crop suitability.</p> </li> <li> <p>Feature Analysis : Studied how different environmental factors influence crop growth and identified the most significant parameters for prediction.</p> </li> <li> <p>Model Selection &amp; Implementation : Researched various ML models and implemented algorithms like Na\u00efve Bayes, Decision Trees, and Random Forest to predict the best-suited crops.</p> </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>https://www.kaggle.com/datasets/atharvaingle/crop-recommendation-dataset/data</li> </ul>"},{"location":"machine-learning/crop-recommendation/#explanation","title":"\ud83d\udd0d EXPLANATION","text":""},{"location":"machine-learning/crop-recommendation/#dataset-overview-feature-details","title":"DATASET OVERVIEW &amp; FEATURE DETAILS","text":"<p>\ud83d\udcc2 dataset.csv | Feature| Description | Data Type  | |------------|-----------------|----------------| | Soil_pH    | Soil pH level   | float          | | Humidity   | Humidity level  | float          | | Rainfall   | Rainfall amount | float          | | N          | Nitrogen level  | int64          | | P          | Phosphorus level| int64          | | K          | Potassium level | int64          | |Temperature | Temperature     | float          | | crop       | Recommended crop| categorical    |</p>"},{"location":"machine-learning/crop-recommendation/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<pre><code>  graph \n  Start --&gt;|No| End;\n  Start --&gt;|Yes| Import_Libraries --&gt; Load_Dataset --&gt; Data_Cleaning --&gt; Feature_Selection --&gt; Train_Test_Split --&gt; Define_Models;\n  Define_Models --&gt; Train_Models --&gt; Evaluate_Models --&gt; Save_Best_Model --&gt; Develop_Flask_API --&gt; Deploy_Application --&gt; Conclusion;\n  Deploy_Application --&gt;|Error?| Debug --&gt; Yay!;\n</code></pre> Import Necessary LibrariesLoad DatasetData Cleaning ProcessVisualizing Correlations Between FeaturesData Preparation - Features (X) and Target (y)Split the Data into Training and Test SetsDefine ModelsTrain and Evaluate Each ModelVisualizing Model Evaluation Metrics <ul> <li>First, we import all the essential libraries needed for handling, analyzing, and modeling the dataset. </li> <li>This includes libraries like Pandas for data manipulation, Numpy for numerical computations, Matplotlib and Seaborn for data visualization, and Scikit-learn for machine learning models, evaluation, and data preprocessing. </li> <li>These libraries will enable us to perform all required tasks efficiently.</li> </ul> <ul> <li>We load the dataset using Pandas <code>read_csv()</code> function. The dataset contains crop data, which is loaded with a semicolon delimiter. </li> <li>After loading, we inspect the first few rows to understand the structure of the data and ensure that the dataset is correctly loaded.</li> </ul> <p>Data cleaning is a crucial step in any project. In this step:</p> <ul> <li>Handle missing values, remove duplicates, and ensure data consistency.</li> <li>Convert categorical variables if necessary and normalize numerical values.</li> </ul> <ul> <li>Use heatmaps and scatter plots to understand relationships between features and how they impact crop recommendations.</li> </ul> <ul> <li>Separate independent variables (environmental parameters) and the target variable (recommended crop).</li> </ul> <ul> <li>Use train_test_split() from Scikit-learn to divide data into training and testing sets, ensuring model generalization. </li> </ul> <p>We define multiple regression models to train and evaluate on the dataset:</p> <ul> <li>RandomForestRegressor: A robust ensemble method that performs well on non-linear datasets.</li> <li>Naive Bayes: A probabilistic classifier based on Bayes' theorem, which assumes independence between features and is effective for classification tasks.</li> <li>DecisionTreeRegressor: A decision tree-based model, capturing non-linear patterns and interactions.</li> </ul> <ul> <li>Fit models using training data and evaluate performance using accuracy, precision, recall, and F1-score metrics.</li> </ul> <ul> <li>Use confusion matrices, precision-recall curves, and ROC curves to assess model performance.</li> </ul> <p>== \"Conclusion and Observations\"</p> <pre><code>**Best-Performing Models and Insights Gained:**\n\n- The Random Forest model provided the highest accuracy and robustness in predictions.\n\n- Decision Tree performed well but was prone to overfitting on training data.\n\n- Na\u00efve Bayes, though simple, showed competitive performance for certain crop categories.\n\n- Feature importance analysis revealed that soil pH and nitrogen levels had the most significant impact on crop recommendation.\n\n**Potential Improvements and Future Enhancements:**\n\n- Implement deep learning models for better feature extraction and prediction accuracy.\n\n- Expand the dataset by incorporating satellite and real-time sensor data.\n\n- Integrate weather forecasting models to enhance crop suitability predictions.\n\n- Develop a mobile-friendly UI for better accessibility to farmers.\n</code></pre>"},{"location":"machine-learning/crop-recommendation/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"Code to compute F1-score, Precision, and Recall <pre><code>from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\n# Initialize a dictionary to store model scores\nmodel_scores = {}\n\n# Iterate through each model and compute evaluation metrics\nfor name, model in models.items():\n    print(f\"Evaluating {name}...\")\n\n    # Train the model\n    model.fit(x_train, y_train)\n\n    # Predict on the test set\n    y_pred = model.predict(x_test)\n\n    # Compute metrics\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n\n    # Store results\n    model_scores[name] = {\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1\n    }\n\n    # Print results for each model\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"\\nClassification Report:\\n\")\n    print(classification_report(y_test, y_pred))\n    print(\"-\" * 50)\n\n# Print a summary of all model scores\nprint(\"\\nSummary of Model Performance:\\n\")\nfor name, scores in model_scores.items():\n    print(f\"{name}: Precision={scores['Precision']:.4f}, Recall={scores['Recall']:.4f}, F1 Score={scores['F1 Score']:.4f}\")\n</code></pre> <ul> <li>This code evaluates multiple machine learning models and displays performance metrics such as Precision, Recall, F1 Score, and a Classification Report for each model.</li> </ul>"},{"location":"machine-learning/crop-recommendation/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Trade-off: Accuracy vs. Computational Efficiency</li> <li>Solution:  Optimized hyperparameters and used efficient algorithms.</li> </ul> <ul> <li>Trade-off: Model interpretability vs complexity.</li> <li>Solution: Selected models balancing accuracy and interpretability.</li> </ul>"},{"location":"machine-learning/crop-recommendation/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"<p>Visualizations of different features</p> HeatMapModel Comparison <p></p> <p></p>"},{"location":"machine-learning/crop-recommendation/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy Precision Recall F1-score Naive Bayes 99.54% 99.58% 99.55% 99.54% Random Forest Regressor 99.31% 99.37% 99.32% 99.32% Decision Tree Regressor 98.63% 98.68% 98.64% 98.63%"},{"location":"machine-learning/crop-recommendation/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"machine-learning/crop-recommendation/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Soil conditions play a crucial role in crop recommendation.</li> <li>Environmental factors significantly impact crop yield.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Feature engineering and hyperparameter tuning.</li> <li>Deployment of ML models in real-world applications.</li> </ul>"},{"location":"machine-learning/crop-recommendation/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Application 1Application 2 <p>Application of FarmSmart in precision farming.</p> <ul> <li>FarmSmart helps optimize resource allocation, enabling farmers to make data-driven decisions for sustainable and profitable crop production. https://github.com/Kashishkh/FarmSmart</li> </ul> <p>Use in government agricultural advisory services.</p> <ul> <li>Government agencies can use FarmSmart to provide region-specific crop recommendations, improving food security and agricultural productivity through AI-driven insights.</li> </ul>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/","title":"Health Insurance Cross-Sell Prediction","text":""},{"location":"machine-learning/health-insurance-cross-sell-prediction/#aim","title":"AIM","text":"<p>To predict whether a Health Insurance customer would be interested in buying Vehicle Insurance.</p>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction</p>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/sid4ds/insurance-cross-sell-prediction-eda-modeling</p>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 for TunedThresholdClassifierCV)</li> <li>xgboost</li> <li>catboost</li> <li>lightgbm</li> <li>matplotlib</li> <li>seaborn</li> <li>joblib</li> </ul>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#description","title":"DESCRIPTION","text":"<p>Why is it necessary?</p> <ul> <li>This project aims to predict the chances of cross-selling Vehicle insurance to existing Health insurance customers. This would be extremely helpful for companies because they can then accordingly plan communication strategy to reach out to those customers and optimise their business model and revenue.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Going through previous research and articles related to the problem.</li> <li>Data exploration to understand the features. Using data visualization to check their distributions.</li> <li>Identifying key metrics for the problem based on ratio of target classes - ROC-AUC &amp; Matthew's Correlation Coefficient (MCC) instead of Accuracy.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Feature Engineering: Tutorial notebook</li> <li>Public notebook: Vehicle Insurance EDA and boosting models</li> </ul>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"machine-learning/health-insurance-cross-sell-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range id Unique ID for the customer Numerical Unique numerical values Gender Binary gender of the customer Binary [0: Male, 1: Female] (or other binary representations as applicable) Age Numerical age of the customer Numerical Measured in years Driving_License Indicates if the customer has a Driving License Binary [0: No, 1: Yes] Region_Code Unique code for the customer's region Numerical Unique numerical values Previously_Insured Indicates if the customer already has Vehicle Insurance Binary [0: No, 1: Yes] Vehicle_Age Age of the vehicle categorized as ordinal values Categorical [&lt; 1 year, 1-2 years, &gt; 2 years] Vehicle_Damage Indicates if the vehicle was damaged in the past Binary [0: No, 1: Yes] Annual_Premium Amount to be paid as premium over the year Numerical Measured in currency Policy_Sales_Channel Anonymized code for the channel of customer outreach Numerical Unique numerical codes representing various channels Vintage Number of days the customer has been associated with the company Numerical Measured in days"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Removing duplicates</li> <li>Categorical feature encoding</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Discretizing numerical features</li> <li>Feature selection based on model-based feature importances and statistical tests.</li> </ul> <p>Modeling</p> <ul> <li>Holdout dataset created or model testing</li> <li>Setting up a framework for easier testing of multiple models.</li> <li>Models trained: Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive-Bayes, Decision Tree, Random Forest, AdaBoost, Multi-Layer Perceptron, XGBoost, CatBoost, LightGBM</li> <li>Class imbalance handled through:</li> <li>Class weights, when supported by model architecture</li> <li>Threshold tuning using TunedThresholdClassifierCV</li> <li>Metric for model-tuning: F1-score (harmonic weighted mean of precision and recall)</li> </ul> <p>Result analysis</p> <ul> <li>Predictions made on holdout test set</li> <li>Models compared based on classification report and chosen metrics: ROC-AUC and MCC.</li> </ul>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Accuracy vs Recall &amp; Precision</p> <p>Data is heavily imbalanced, with only ~12% representing the positive class. This makes accuracy unsuitable as a metric for our problem. Our goal is to correctly predict all the positive samples, due to which we must focus on recall. However, this lowers the overall accuracy since some negative samples may be predicted as positive.</p> <ul> <li>Solution: Prediction threshold for models is tuned using F1-score to create a balance between precision and recall. This maintains overall accuracy at an acceptable level while boosting recall.</li> </ul>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Feature distributions (Univariate Analysis) AgeLicenseRegion CodePreviously InsuredVehical AgeVehical DamageAnnual PremiumPolicy ChannelVintage <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> Engineered Features Age GroupPolicy Group <p></p> <p></p> Feature Distributions (Bivariate Analysis) Pair PlotsSpearman-Rank CorrelationPoint-Biserial CorrelationTetrachoric Correlation <p></p> <p></p> <p></p> <p></p> Feature Selection Point-Biserial CorrelationANOVA F-TestTetrachoric CorrelationChi-Squared Test of IndependenceMutual InformationXGBoost Feature ImportancesExtra Trees Feature Importances <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#models-used-and-their-performance","title":"MODELS USED AND THEIR PERFORMANCE","text":"<pre><code>Best threshold after threshold tuning is also mentioned.\n</code></pre> Model + Feature set ROC-AUC MCC Best threshold Logistic Regression + Original 0.8336 0.3671 0.65 Logistic Regression + Extended 0.8456 0.3821 0.66 Logistic Regression + Reduced 0.8455 0.3792 0.67 Logistic Regression + Minimal 0.8177 0.3507 0.60 Linear DA + Original 0.8326 0.3584 0.19 Linear DA + Extended 0.8423 0.3785 0.18 Linear DA + Reduced 0.8421 0.3768 0.18 Linear DA + Minimal 0.8185 0.3473 0.15 Quadratic DA + Original 0.8353 0.3779 0.45 Quadratic DA + Extended 0.8418 0.3793 0.54 Quadratic DA + Reduced 0.8422 0.3807 0.44 Quadratic DA + Minimal 0.8212 0.3587 0.28 Gaussian Naive Bayes + Original 0.8230 0.3879 0.78 Gaussian Naive Bayes + Extended 0.8242 0.3914 0.13 Gaussian Naive Bayes + Reduced 0.8240 0.3908 0.08 Gaussian Naive Bayes + Minimal 0.8055 0.3605 0.15 K-Nearest Neighbors + Original 0.7819 0.3461 0.09 K-Nearest Neighbors + Extended 0.7825 0.3469 0.09 K-Nearest Neighbors + Reduced 0.7710 0.3405 0.01 K-Nearest Neighbors + Minimal 0.7561 0.3201 0.01 Decision Tree + Original 0.8420 0.3925 0.67 Decision Tree + Extended 0.8420 0.3925 0.67 Decision Tree + Reduced 0.8419 0.3925 0.67 Decision Tree + Minimal 0.8262 0.3683 0.63 Random Forest + Original 0.8505 0.3824 0.70 Random Forest + Extended 0.8508 0.3832 0.70 Random Forest + Reduced 0.8508 0.3820 0.71 Random Forest + Minimal 0.8375 0.3721 0.66 Extra-Trees + Original 0.8459 0.3770 0.70 Extra-Trees + Extended 0.8504 0.3847 0.71 Extra-Trees + Reduced 0.8515 0.3836 0.72 Extra-Trees + Minimal 0.8337 0.3682 0.67 AdaBoost + Original 0.8394 0.3894 0.83 AdaBoost + Extended 0.8394 0.3894 0.83 AdaBoost + Reduced 0.8404 0.3839 0.84 AdaBoost + Minimal 0.8269 0.3643 0.86 Multi-Layer Perceptron + Original 0.8512 0.3899 0.22 Multi-Layer Perceptron + Extended 0.8528 0.3865 0.23 Multi-Layer Perceptron + Reduced 0.8517 0.3892 0.23 Multi-Layer Perceptron + Minimal 0.8365 0.3663 0.21 XGBoost + Original 0.8585 0.3980 0.68 XGBoost + Extended 0.8585 0.3980 0.68 XGBoost + Reduced 0.8584 0.3967 0.68 XGBoost + Minimal 0.8459 0.3765 0.66 CatBoost + Original 0.8579 0.3951 0.46 CatBoost + Extended 0.8578 0.3981 0.45 CatBoost + Reduced 0.8577 0.3975 0.45 CatBoost + Minimal 0.8449 0.3781 0.42 LightGBM + Original 0.8587 0.3978 0.67 LightGBM + Extended 0.8587 0.3976 0.67 LightGBM + Reduced 0.8587 0.3983 0.67 LightGBM + Minimal 0.8462 0.3753 0.66"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"machine-learning/health-insurance-cross-sell-prediction/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ol> <li>Previously_Insured, Vehicle_Damage, Policy_Sales_Channel and Age are the most informative features for predicting cross-sell probability.</li> <li>Vintage and Driving_License have no predictive power. They are not included in the best model.</li> </ol> Improvements in understanding machine learning concepts <ol> <li>Implemented threshold-tuning for more accurate results.</li> <li>Researched and utilized statistical tests for feature selection.</li> </ol> Challenges faced and how they were overcome <ol> <li>Shortlisting the apropriate statistical test for bivariate analysis and feature selection.</li> <li>Deciding the correct metric for evaluation of models due to imbalanced nature of the dataset.</li> <li>F1-score was used for threshold-tuning. ROC-AUC score and MCC were used for model comparison.</li> </ol>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1 <ul> <li>Companies can use customer data to predict which customers to target for cross-sell marketing. This saves cost and effort for the company, and protects uninterested customers from unnecessary marketing calls.</li> </ul>"},{"location":"machine-learning/health-insurance-cross-sell-prediction/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Complex model-ensembling through stacking or hill-climbing was not implemented due to significantly longer training time.</li> </ul>"},{"location":"machine-learning/heart-disease-detection-model/","title":"Heart Disease Detection Model","text":""},{"location":"machine-learning/heart-disease-detection-model/#aim","title":"AIM","text":"<p>The aim of this project is to develop a reliable and efficient machine learning-based system for the early detection and diagnosis of heart disease. By leveraging advanced algorithms, the system seeks to analyze patient data, identify significant patterns, and predict the likelihood of heart disease, thereby assisting healthcare professionals in making informed decisions.</p>"},{"location":"machine-learning/heart-disease-detection-model/#dataset-link","title":"DATASET LINK","text":"<p>This project uses a publicly available heart disease dataset from UCI Machine Learning Repository</p>"},{"location":"machine-learning/heart-disease-detection-model/#notebook-link","title":"NOTEBOOK LINK","text":"<p>This is notebook of the following project Kaggle</p>"},{"location":"machine-learning/heart-disease-detection-model/#libraries-needed","title":"LIBRARIES NEEDED","text":"<pre><code>- pandas\n- numpy\n- scikit-learn\n- matplotlib\n- seaborn\n</code></pre>"},{"location":"machine-learning/heart-disease-detection-model/#description","title":"DESCRIPTION","text":"<p>what is the requirement of the project?,  The project requires a dataset containing patient health records, including attributes like age, cholesterol levels, blood pressure, and medical history. Additionally, it needs machine learning tools and frameworks (e.g., Python, scikit-learn) for building and evaluating predictive models.</p> <p>why is it necessary?,  Early detection of heart disease is crucial to prevent severe complications and reduce mortality rates. A machine learning-based system provides accurate, fast, and cost-effective predictions, aiding timely medical intervention and improved patient outcomes.</p> <p>how is it beneficial and used?,  This system benefits healthcare by improving diagnostic accuracy and reducing reliance on invasive procedures. It can be used by doctors for decision support, by patients for risk assessment, and in hospitals for proactive healthcare management.</p> <p>how did you start approaching this project?,  The project begins by collecting and preprocessing a heart disease dataset, ensuring it is clean and ready for analysis. Next, machine learning models are selected, trained, and evaluated to identify the most accurate algorithm for predicting heart disease.</p> <p>Any additional resources used like blogs reading, books reading (mention the name of book along with the pages you have read)? Kaggle kernels and documentation for additional dataset understanding. Tutorials on machine learning regression techniques, particularly for Random Forest, SVR, and Decision Trees.</p>"},{"location":"machine-learning/heart-disease-detection-model/#explanation","title":"EXPLANATION","text":""},{"location":"machine-learning/heart-disease-detection-model/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"<p>Age: Patient's age in years.</p> <p>Sex: Gender of the patient (1 = male; 0 = female).</p> <p>Chest Pain Type (cp): Categorized as:</p> <p>0: Typical angina 1: Atypical angina 2: Non-anginal pain 3: Asymptomatic Resting Blood Pressure (trestbps): Measured in mm Hg upon hospital admission.</p> <p>Serum Cholesterol (chol): Measured in mg/dL.</p> <p>Fasting Blood Sugar (fbs): Indicates if fasting blood sugar &gt; 120 mg/dL (1 = true; 0 = false).</p> <p>Resting Electrocardiographic Results (restecg):</p> <p>0: Normal 1: Having ST-T wave abnormality (e.g., T wave inversions and/or ST elevation or depression &gt; 0.05 mV) 2: Showing probable or definite left ventricular hypertrophy by Estes' criteria Maximum Heart Rate Achieved (thalach): Peak heart rate during exercise.</p> <p>Exercise-Induced Angina (exang): Presence of angina induced by exercise (1 = yes; 0 = no).</p> <p>Oldpeak: ST depression induced by exercise relative to rest.</p> <p>Slope of the Peak Exercise ST Segment (slope):</p> <p>0: Upsloping 1: Flat 2: Downsloping Number of Major Vessels Colored by Fluoroscopy (ca): Ranges from 0 to 3.</p> <p>Thalassemia (thal):</p> <p>1: Normal 2: Fixed defect 3: Reversible defect Target: Diagnosis of heart disease (0 = no disease; 1 = disease).</p>"},{"location":"machine-learning/heart-disease-detection-model/#project-workflow","title":"PROJECT WORKFLOW","text":""},{"location":"machine-learning/heart-disease-detection-model/#1problem-definition","title":"1.Problem Definition","text":"<p>Identify the objective: To predict the presence or absence of heart disease based on patient data. Define the outcome variable (target) and input features.</p>"},{"location":"machine-learning/heart-disease-detection-model/#2data-collection","title":"2.Data Collection","text":"<p>Gather a reliable dataset, such as the Cleveland Heart Disease dataset, which includes features relevant to heart disease prediction.</p>"},{"location":"machine-learning/heart-disease-detection-model/#3data-preprocessing","title":"3.Data Preprocessing","text":"<p>Handle missing values: Fill or remove records with missing data. Normalize/standardize data to ensure all features have comparable scales. Encode categorical variables like sex, cp, and thal using techniques like one-hot encoding or label encoding.</p>"},{"location":"machine-learning/heart-disease-detection-model/#4exploratory-data-analysis-eda","title":"4.Exploratory Data Analysis (EDA)","text":"<p>Visualize data distributions using histograms, boxplots, or density plots. Identify relationships between features using correlation matrices and scatterplots. Detect and handle outliers to improve model performance.</p>"},{"location":"machine-learning/heart-disease-detection-model/#5feature-selection","title":"5.Feature Selection","text":"<p>Use statistical methods or feature importance metrics to identify the most relevant features for prediction. Remove redundant or less significant features.</p>"},{"location":"machine-learning/heart-disease-detection-model/#6data-splitting","title":"6.Data Splitting","text":"<p>Divide the dataset into training, validation, and testing sets (e.g., 70%-15%-15%). Ensure a balanced distribution of the target variable in all splits.</p>"},{"location":"machine-learning/heart-disease-detection-model/#7model-selection","title":"7.Model Selection","text":"<p>Experiment with multiple machine learning algorithms such as Logistic Regression, Random Forest, Decision Trees, Support Vector Machines (SVM), and Neural Networks. Select models based on the complexity and nature of the dataset.</p>"},{"location":"machine-learning/heart-disease-detection-model/#8model-training","title":"8.Model Training","text":"<p>Train the chosen models using the training dataset. Tune hyperparameters using grid search or random search techniques.</p>"},{"location":"machine-learning/heart-disease-detection-model/#9model-evaluation","title":"9.Model Evaluation","text":"<p>Assess models on validation and testing datasets using metrics such as: Accuracy Precision, Recall, and F1-score Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC). Compare models to identify the best-performing one.</p>"},{"location":"machine-learning/heart-disease-detection-model/#10deployment-and-prediction","title":"10.##Deployment and Prediction","text":"<p>Save the trained model using frameworks like joblib or pickle. Develop a user interface (UI) or API for end-users to input data and receive predictions.</p>"},{"location":"machine-learning/heart-disease-detection-model/#11iterative-improvement","title":"11.Iterative Improvement","text":"<p>Continuously refine the model using new data or advanced algorithms. Address feedback and optimize the system based on real-world performance.</p>"},{"location":"machine-learning/heart-disease-detection-model/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Accuracy vs. Interpretability</li> <li>Complex models like Random Forests or Neural Networks offer higher accuracy but are less interpretable compared to simpler models like Logistic Regression.</li> </ul> <ul> <li>Overfitting vs. Generalization</li> <li>Models with high complexity may overfit the training data, leading to poor generalization on unseen data.</li> </ul>"},{"location":"machine-learning/heart-disease-detection-model/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Score Logistic regression 88% K-Nearest Classifier 68% Random Forest Classifier 86%"},{"location":"machine-learning/heart-disease-detection-model/#conclusion","title":"CONCLUSION","text":""},{"location":"machine-learning/heart-disease-detection-model/#key-learnings","title":"KEY LEARNINGS","text":"<ol> <li> <p>Data Insights Understanding Healthcare Data: Learned how medical attributes (e.g., age, cholesterol, chest pain type) influence heart disease risk. Data Imbalance: Recognized the challenges posed by imbalanced datasets and explored techniques like SMOTE and class weighting to address them. Importance of Preprocessing: Gained expertise in handling missing values, scaling data, and encoding categorical variables, which are crucial for model performance.</p> </li> <li> <p>Techniques Mastered Exploratory Data Analysis (EDA): Applied visualization tools (e.g., histograms, boxplots, heatmaps) to uncover patterns and correlations in data. Feature Engineering: Identified and prioritized key features using statistical methods and feature importance metrics. Modeling: Implemented various machine learning algorithms, including Logistic Regression, Random Forest, Gradient Boosting, and Support Vector Machines. Evaluation Metrics: Learned to evaluate models using metrics like Precision, Recall, F1-score, and ROC-AUC to optimize for healthcare-specific goals. Hyperparameter Tuning: Used grid search and random search to optimize model parameters and improve performance. Interpretability Tools: Utilized SHAP and feature importance analysis to explain model predictions.</p> </li> <li> <p>Skills Developed Problem-Solving: Addressed trade-offs such as accuracy vs. interpretability, and overfitting vs. generalization. Critical Thinking: Improved decision-making on model selection, preprocessing methods, and evaluation strategies. Programming: Strengthened Python programming skills, including the use of libraries like scikit-learn, pandas, matplotlib, and TensorFlow. Collaboration: Enhanced communication and teamwork when discussing medical insights and technical challenges with domain experts. Time Management: Balanced experimentation with computational efficiency, focusing on techniques that maximized impact. Ethical Considerations: Gained awareness of ethical issues like ensuring fairness in predictions and minimizing false negatives, which are critical in healthcare applications.</p> </li> <li> <p>Broader Understanding Interdisciplinary Knowledge: Combined expertise from data science, healthcare, and statistics to create a meaningful application. Real-World Challenges: Understood the complexities of translating machine learning models into practical tools for healthcare. Continuous Learning: Learned that model development is iterative, requiring continuous refinement based on feedback and new data. </p> </li> </ol>"},{"location":"machine-learning/heart-disease-detection-model/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Clinical Decision Support Systems (CDSS)</p> <ul> <li>ML models can be integrated into Electronic Health Record (EHR) systems to assist doctors in diagnosing heart disease. The model can provide predictions based on patient data, helping clinicians make faster and more accurate decisions.</li> </ul> <p>Early Screening and Risk Assessment</p> <ul> <li>Patients can undergo routine screening using a heart disease detection system to assess their risk level. The system can predict whether a patient is at high, moderate, or low risk, prompting early interventions or lifestyle changes.</li> </ul>"},{"location":"machine-learning/poker-hand-prediction/","title":"Poker Hand Prediction","text":""},{"location":"machine-learning/poker-hand-prediction/#aim","title":"AIM","text":"<p>The aim of this project is to develop a machine learning model using a Multi-Layer Perceptron (MLP) classifier to accurately classify different types of poker hands based on the suit and rank of five cards.</p>"},{"location":"machine-learning/poker-hand-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/dysphoria/poker-hand-classification</p>"},{"location":"machine-learning/poker-hand-prediction/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/supratikbhowal/poker-hand-prediction-model</p>"},{"location":"machine-learning/poker-hand-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"<ul> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>seaborn</li> <li>scikit-learn</li> </ul>"},{"location":"machine-learning/poker-hand-prediction/#description","title":"DESCRIPTION","text":"<p>This project involves building a classification model to predict poker hands using a Multi-Layer Perceptron (MLP) classifier. The dataset consists of features representing the suit and rank of five cards, with the target variable being the type of poker hand (e.g., one pair, two pair, royal flush). The model is trained on a standardized dataset, with class weights computed to address class imbalance. Performance is evaluated using metrics such as accuracy, classification report, confusion matrix, prediction error, ROC curve, and AUC, providing a comprehensive analysis of the model's effectiveness in classifying poker hands.</p> <p>What is the requirement of the project?  - To accurately predict the Poker Hand type.</p> <p>Why is it necessary?  - The project demonstrates how machine learning can solve structured data problems, bridging the gap between theoretical knowledge and practical implementation.</p> <p>How is it beneficial and used?  - The project automates the classification of poker hands, enabling players to quickly and accurately identify the type of hand they have, such as a straight, flush, or royal flush, without manual effort.  - By understanding the probabilities and patterns of certain hands appearing, players can make informed decisions on whether to bet, raise, or fold, improving their gameplay strategy.</p> <p>How did you start approaching this project?  - Analyzed the poker hand classification problem, reviewed the dataset structure (suits, ranks, and hand types), and identified the multi-class nature of the target variable. Studied the class imbalance issue and planned data preprocessing steps, including scaling and class weight computation.  - Chose the Multi-Layer Perceptron (MLP) Classifier for its capability to handle complex patterns in data. Defined model hyperparameters, trained the model using standardized features, and evaluated its performance using metrics like accuracy, ROC-AUC, and confusion matrix</p> <p>Mention any additional resources used:  - Kaggle kernels and documentation for additional dataset understanding.  - Tutorials on machine learning regression techniques, particularly for MLP</p>"},{"location":"machine-learning/poker-hand-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"machine-learning/poker-hand-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range S1 Suit of card #1 Ordinal (1-4) representing C1 Rank of card #1 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S2 Suit of card #2 Ordinal (1-4) representing C2 Rank of card #2 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S3 Suit of card #3 Ordinal (1-4) representing C3 Rank of card #3 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S4 Suit of card #4 Ordinal (1-4) representing C4 Rank of card #4 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) S5 Suit of card #5 Ordinal (1-4) representing C5 Rank of card #5 Numerical (1-13) representing (Ace, 2, 3, \u2026 , Queen, King) Poker Hand Type of Card in Hand Ordinal (0-9) types* <p>Poker Hands  0: Nothing in hand, not a recognized poker hand  1: One pair, one pair of equal ranks within five cards  2: Two pairs, two pairs of equal ranks within five cards  3: Three of a kind, three equal ranks within five cards  4: Straight, five cards, sequentially ranked with no gaps  5: Flush, five cards with the same suit  6: Full house, pair + different rank three of a kind  7: Four of a kind, four equal ranks within five cards  8: Straight flush, straight + flush  9: Royal flush, {Ace, King, Queen, Jack, Ten} + flush </p>"},{"location":"machine-learning/poker-hand-prediction/#project-workflow","title":"PROJECT WORKFLOW","text":""},{"location":"machine-learning/poker-hand-prediction/#1-dataset-loading-and-inspection","title":"1. Dataset Loading and Inspection","text":"<p>The dataset is loaded into the environment, and its structure, features, and target classes are analyzed. Column names are assigned to enhance clarity and understanding.</p>"},{"location":"machine-learning/poker-hand-prediction/#2-data-preprocessing","title":"2. Data Preprocessing","text":"<p>Target variables are mapped to descriptive hand labels, features and target variables are separated, and data is standardized using 'StandardScaler' for uniform scaling.</p>"},{"location":"machine-learning/poker-hand-prediction/#3-handling-class-imbalance","title":"3. Handling Class Imbalance","text":"<p>Class weights are computed using 'compute_class_weight' to address the imbalance in target classes, ensuring fair training for all hand types.</p>"},{"location":"machine-learning/poker-hand-prediction/#4-model-selection-and-training","title":"4. Model Selection and Training","text":"<p>The MLPClassifier is selected for its capability to handle multi-class problems, configured with suitable hyperparameters, and trained on the preprocessed training data.</p>"},{"location":"machine-learning/poker-hand-prediction/#5-model-evaluation","title":"5. Model Evaluation","text":"<p>The model's performance is assessed using metrics such as accuracy, classification reports, and confusion matrices. Predictions are generated for the test dataset.</p>"},{"location":"machine-learning/poker-hand-prediction/#6-visualization-and-error-analysis","title":"6. Visualization and Error Analysis","text":"<p>Class prediction errors are visualized using bar charts, and ROC curves with AUC values are generated for each hand type to evaluate classification effectiveness.</p>"},{"location":"machine-learning/poker-hand-prediction/#7-insights-and-interpretation","title":"7. Insights and Interpretation","text":"<p>Strengths and weaknesses of the model are identified through error analysis, and the findings are presented using visual tools like heatmaps for better understanding.</p>"},{"location":"machine-learning/poker-hand-prediction/#8-conclusion","title":"8. Conclusion","text":"<p>The project outcomes are summarized, practical applications are discussed, and suggestions for further research or improvements are proposed.</p>"},{"location":"machine-learning/poker-hand-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <ul> <li>Trade-off: Accuracy vs. Complexity.<ul> <li>Solution: Using a Multi-Layer Perceptron (MLP) introduces complexity due to its hidden layers and numerous parameters. While this improves accuracy, it requires more computational resources and training time compared to simpler models like decision trees or logistic regression.</li> </ul> </li> </ul> Trade Off 2 <ul> <li>Trade-off: Bias vs. Variance<ul> <li>Solution: The project's hyperparameter tuning (e.g., learning rate, number of hidden layers) aims to reduce bias and variance. However, achieving a perfect balance is a trade-off, as overly complex models may increase variance (overfitting), while overly simplified models may increase bias (underfitting).</li> </ul> </li> </ul> Trade Off 3 <ul> <li>Trade-off: Generalization vs. Overfitting<ul> <li>Solution:The model's flexibility with complex hyperparameters (e.g., hidden layers, activation functions) risks overfitting, especially on small or imbalanced datasets. Regularization techniques like adjusting 'alpha' help mitigate this but may compromise some accuracy.</li> </ul> </li> </ul>"},{"location":"machine-learning/poker-hand-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Is data clean?};\n    B --&gt;|Yes| C[Explore Data];\n    C --&gt; D[Data Preprocessing];\n    D --&gt; E[Define Models];\n    E --&gt; F[Train the Model];\n    F --&gt; G[Evaluate Performance];\n    G --&gt; H[Visualize Evaluation Metrics];\n    H --&gt; I[Model Testing];\n    I --&gt; J[Conclusion and Observations];\n    B ----&gt;|No| K[Clean Data];\n    K --&gt; C;</code></pre> Model Evaluation Metrics Model vs AccuracyROC Curve &amp; AUC ScoreClassification Report Heatmap <p></p> <p></p> <p></p>"},{"location":"machine-learning/poker-hand-prediction/#models-evaluation-metrics","title":"MODELS EVALUATION METRICS","text":"Poker Hand precision recall f1-score support flush 0.52 0.07 0.12 1996 four_of_a_kind 0.00 0.00 0.00 230 full_house 0.77 0.34 0.47 1424 one_pair 1.00 1.00 1.00 422498 royal_flush 0.00 0.00 0.00 3 straight 0.96 0.44 0.60 3885 straight_flush 0.00 0.00 0.00 12 three_of_a_kind 0.95 0.00 0.00 21121 two_pair 1.00 1.00 1.00 47622 zilch 0.99 1.00 1.00 501209 Model Accuracy Random Forest Regression 0.6190 Gradient Boosting Regression 0.3204 MLP Classifier 0.9924 StackingClassifier 0.9382"},{"location":"machine-learning/poker-hand-prediction/#-","title":"---","text":""},{"location":"machine-learning/poker-hand-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"machine-learning/poker-hand-prediction/#key-learnings","title":"KEY LEARNINGS","text":"<ul> <li>Learned how different machine learning models perform on real-world data and gained insights into their strengths and weaknesses.</li> <li>Realized how important feature engineering and data preprocessing are for enhancing model performance.</li> </ul> <p>Insights gained from the data:</p> <ul> <li>Class imbalance can bias the model, requiring class weighting for fair performance.</li> <li>Suits and ranks of cards are crucial features, necessitating preprocessing for better model input.</li> </ul> <p>Improvements in understanding machine learning concepts:</p> <ul> <li>Learned how to effectively implement and optimize machine learning models using libraries like scikit-learn.</li> <li>Learned how to effectively use visualizations (e.g., ROC curves, confusion matrices, and heatmaps) to interpret and communicate model performance.</li> </ul> <p>Challenges faced and how they were overcome:</p> <ul> <li>Some features had different scales, which could affect model performance. This was overcome by applying standardization to the features, ensuring they were on a similar scale for better convergence during training.</li> <li>Some rare hands, such as \"royal_flush\" and \"straight_flush,\" had very low prediction accuracy. This was mitigated by analyzing misclassification patterns and considering potential improvements like generating synthetic samples or using other models.</li> </ul>"},{"location":"machine-learning/poker-hand-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1 <p>Poker Strategy Development and Simulation      -&gt; Developers or researchers studying poker strategies can use this model to simulate various hand combinations and evaluate strategic decisions. The model's classification can help assess the strength of different hands and optimize strategies.</p> Application 2 <p>Real-time Poker Hand Evaluation for Mobile Apps      -&gt; Mobile apps that allow users to practice or play poker could incorporate this model to provide real-time hand evaluation, helping users understand the strength of their hands during gameplay.</p>"},{"location":"machine-learning/sleep-quality-prediction/","title":"Sleep quality prediction","text":"Sleep Quality Prediction AIM <p>To predict sleep quality based on lifestyle and health factors.</p> DATASET LINK <p>          Sleep Health and Lifestyle Dataset      </p> DESCRIPTION What is the requirement of the project? <ul> <li>This project aims to predict the quality of sleep using various health and lifestyle metrics. Predicting sleep quality helps individuals and healthcare professionals address potential sleep-related health issues early.</li> </ul> Why is it necessary? <ul> <li>Sleep quality significantly impacts physical and mental health. Early predictions can prevent chronic conditions linked to poor sleep, such as obesity, heart disease, and cognitive impairment.</li> </ul> How is it beneficial and used? <ul> <li>Individuals: Assess their sleep health and make lifestyle changes to improve sleep quality.</li> <li>Healthcare Professionals: Use the model as an auxiliary diagnostic tool to recommend personalized interventions.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Researching sleep health factors and existing literature.</li> <li>Exploring and analyzing the dataset to understand feature distributions.</li> <li>Preprocessing data for effective feature representation.</li> <li>Iterating over machine learning models to find the optimal balance between accuracy and interpretability.</li> </ul> Mention any additional resources used <ul> <li>Research Paper: Analyzing Sleep Patterns Using AI</li> <li>Public Notebook: Sleep Quality Prediction with 96% Accuracy</li> </ul> LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>seaborn</li> <li>joblib</li> <li>flask</li> </ul> EXPLANATION DETAILS OF THE DIFFERENT FEATURES Feature Name Description Type Values/Range Gender Respondent's gender Categorical [Male, Female] Age Respondent's age Numerical Measured in years Sleep Duration (hours) Hours of sleep per day Numerical Measured in hours Physical Activity Level Daily physical activity in minutes Numerical Measured in minutes Stress Level Stress level on a scale Numerical 1 to 5 (low to high) BMI Category Body Mass Index category Categorical [Underweight, Normal, Overweight, Obese] Systolic Blood Pressure Systolic blood pressure Numerical Measured in mmHg Diastolic Blood Pressure Diastolic blood pressure Numerical Measured in mmHg Heart Rate (bpm) Resting heart rate Numerical Beats per minute Daily Steps Average number of steps per day Numerical Measured in steps Sleep Disorder Reported sleep disorder Categorical [Yes, No] WHAT I HAVE DONE Step 1: Exploratory Data Analysis <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> Step 2: Data Cleaning and Preprocessing <ul> <li>Handling missing values</li> <li>Label encoding categorical features</li> <li>Standardizing numerical features</li> </ul> Step 3: Feature Engineering and Selection <ul> <li>Merging features based on domain knowledge</li> <li>Creating derived features such as \"Activity-to-Sleep Ratio\"</li> </ul> Step 4: Modeling <ul> <li>Model trained: Decision Tree</li> <li>Class imbalance handled using SMOTE</li> <li>Metric for optimization: F1-score</li> </ul> Step 5: Result Analysis <ul> <li>Visualized results using confusion matrices and classification reports</li> <li>Interpreted feature importance for tree-based models</li> </ul> MODELS USED AND THEIR ACCURACIES Model Accuracy (%) F1-Score (%) Precision (%) Recall (%) Decision Tree 74.50 75.20 73.00 77.50 CONCLUSION WHAT YOU HAVE LEARNED Insights gained from the data <ul> <li>Sleep Duration, Stress Level, and Physical Activity are the most indicative features for predicting sleep quality.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned and implemented preprocessing techniques like encoding categorical variables and handling imbalanced datasets.</li> <li>Gained insights into deploying a machine learning model using Flask for real-world use cases.</li> </ul> Challenges faced and how they were overcome <ul> <li>Managing imbalanced classes: Overcame this by using SMOTE for oversampling the minority class.</li> <li>Choosing a simple yet effective model: Selected Decision Tree for its interpretability and ease of deployment.</li> </ul> USE CASES OF THIS MODEL Application 1 <p>         A health tracker app can integrate this model to assess and suggest improvements in sleep quality based on user inputs.     </p> Application 2 <p>         Healthcare providers can use this tool to make preliminary assessments of patients' sleep health, enabling timely interventions.     </p> FEATURES PLANNED BUT NOT IMPLEMENTED Feature 1 <p>         Advanced models such as Random Forest, AdaBoost, and Gradient Boosting were not implemented due to the project's focus on simplicity and interpretability.     </p> Feature 2 <p>         Integration with wearable device data for real-time predictions was not explored but remains a potential enhancement for future work.     </p>"},{"location":"machine-learning/used-cars-price-prediction/","title":"Used Cars Price Prediction","text":""},{"location":"machine-learning/used-cars-price-prediction/#aim","title":"AIM","text":"<p>Predicting the prices of used cars based on their configuration and previous usage.</p>"},{"location":"machine-learning/used-cars-price-prediction/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/avikasliwal/used-cars-price-prediction</p>"},{"location":"machine-learning/used-cars-price-prediction/#my-notebook-link","title":"MY NOTEBOOK LINK","text":"<p>https://www.kaggle.com/code/sid4ds/used-cars-price-prediction/</p>"},{"location":"machine-learning/used-cars-price-prediction/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn (&gt;=1.5.0 required for Target Encoding)</li> <li>xgboost</li> <li>catboost</li> <li>matplotlib</li> <li>seaborn</li> </ul>"},{"location":"machine-learning/used-cars-price-prediction/#description","title":"DESCRIPTION","text":"<p>Why is it necessary?</p> <ul> <li>This project aims to predict the prices of used cars listed on an online marketplace based on their features and usage by previous owners. This model can be used by sellers to estimate an approximate price for their cars when they list them on the marketplace. Buyers can use the model to check if the listed price is fair when they decide to buy a used vehicle.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Researching previous projects and articles related to the problem.</li> <li>Data exploration to understand the features.  </li> <li>Identifying different preprocessing strategies for different feature types.</li> <li>Choosing key metrics for the problem - Root Mean Squared Error (for error estimation), R2-Score (for model explainability)</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Dealing with features that have high cardinality</li> <li>Target-encoding Categorical Variables</li> <li>Cars Price Prediction</li> </ul>"},{"location":"machine-learning/used-cars-price-prediction/#explanation","title":"EXPLANATION","text":""},{"location":"machine-learning/used-cars-price-prediction/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":"Feature Name Description Type Values/Range Name Car model Categorical Names of car models Location City where the car is listed for sale Categorical Names of cities Year Year of original purchase of car Numerical Years (e.g., 2010, 2015, etc.) Kilometers_Driven Odometer reading of the car Numerical Measured in kilometers Fuel_Type Fuel type of the car Categorical [Petrol, Diesel, CNG, Electric, etc.] Transmission Transmission type of the car Categorical [Automatic, Manual] Owner_Type Number of previous owners of the car Numerical Whole numbers Mileage Current mileage provided by the car Numerical Measured in km/l or equivalent Engine Engine capacity of the car Numerical Measured in CC (Cubic Centimeters) Power Engine power output of the car Numerical Measured in BHP (Brake Horsepower) Seats Seating capacity of the car Numerical Whole numbers New_Price Original price of the car at the time of purchase Numerical Measured in currency"},{"location":"machine-learning/used-cars-price-prediction/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis</p> <ul> <li>Summary statistics</li> <li>Data visualization for numerical feature distributions</li> <li>Target splits for categorical features</li> </ul> <p>Data cleaning and Preprocessing</p> <ul> <li>Removing rare categories of brands</li> <li>Removing outliers for numerical features and target</li> <li>Categorical feature encoding for low-cardinality features</li> <li>Target encoding for high-cardinality categorical features (in model pipeline)</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Extracting brand name from model name for a lower-cardinality feature.</li> <li>Converting categorical Owner_Type to numerical Num_Previous_Owners.</li> <li>Feature selection based on model-based feature importances and statistical tests.</li> </ul> <p>Modeling</p> <ul> <li>Holdout dataset created for model testing</li> <li>Setting up a framework for easier testing of multiple models.</li> <li>Models trained: LLinear Regression, K-Nearest Neighbors, Decision Tree, Random Forest, AdaBoost, Multi-Layer Perceptron, XGBoost and CatBoost.</li> <li>Models were ensembled using Simple and Weighted averaging.</li> </ul> <p>Result analysis</p> <ul> <li>Predictions made on holdout test set</li> <li>Models compared based on chosen metrics: RMSE and R2-Score.</li> <li>Visualized predicted prices vs actual prices to analyze errors.</li> </ul>"},{"location":"machine-learning/used-cars-price-prediction/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1 <p>Training time &amp; Model complexity vs Reducing error</p> <ul> <li>Solution: Limiting depth and number of estimators for tree-based models. Overfitting detection and early stopping mechanism for neural network training.</li> </ul>"},{"location":"machine-learning/used-cars-price-prediction/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Error?};\n    B --&gt;|Yes| C[Hmm...];\n    C --&gt; D[Debug];\n    D --&gt; B;\n    B ----&gt;|No| E[Yay!];</code></pre> Data Exploration PriceYearKM DrivenEnginePowerMileageSeats <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> Feature Selection Feature CorrelationTarget CorrelationMutual Information <p></p> <p></p> <p></p>"},{"location":"machine-learning/used-cars-price-prediction/#models-used-and-their-performance","title":"MODELS USED AND THEIR PERFORMANCE","text":"Model RMSE R2-Score Linear Regression 3.5803 0.7915 K-Nearest Neighbors 2.8261 0.8701 Decision Tree 2.6790 0.8833 Random Forest 2.4619 0.9014 AdaBoost 2.3629 0.9092 Multi-layer Perceptron 2.6255 0.8879 XGBoost w/o preprocessing 2.1649 0.9238 XGBoost with preprocessing 2.0987 0.9284 CatBoost w/o preprocessing 2.1734 0.9232 Simple average ensemble 2.2804 0.9154 Weighted average ensemble 2.1296 0.9262"},{"location":"machine-learning/used-cars-price-prediction/#conclusion","title":"CONCLUSION","text":""},{"location":"machine-learning/used-cars-price-prediction/#what-you-have-learned","title":"WHAT YOU HAVE LEARNED","text":"<p>Insights gained from the data</p> <ol> <li>Features related to car configuration such as Power, Engine and Transmission are some of the most informative features. Usage-related features such as Year and current Mileage are also important.</li> <li>Seating capacity and Number of previous owners had relatively less predictive power. However, none of the features were candidates for removal.</li> </ol> Improvements in understanding machine learning concepts <ol> <li>Implemented target-encoding for high-cardinality categorical features.</li> <li>Designed pipelines to avoid data leakage.</li> <li>Ensembling models using prediction averaging.</li> </ol> Challenges faced and how they were overcome <ol> <li>Handling mixed feature types in preprocessing pipelines.</li> <li>Regularization and overfitting detection to reduce training time while maintaining performance.</li> </ol>"},{"location":"machine-learning/used-cars-price-prediction/#use-cases-of-this-model","title":"USE CASES OF THIS MODEL","text":"Application 1Application 2 <ul> <li>Sellers can use the model to estimate an approximate price for their cars when they list them on the marketplace.</li> </ul> <ul> <li>Buyers can use the model to check if the listed price is fair when they decide to buy a used vehicle.</li> </ul>"},{"location":"machine-learning/used-cars-price-prediction/#features-planned-but-not-implemented","title":"FEATURES PLANNED BUT NOT IMPLEMENTED","text":"Feature 1 <ul> <li>Complex model-ensembling through stacking or hill-climbing was not implemented due to significantly longer training time.</li> </ul>"},{"location":"natural-language-processing/","title":"Natural Language Processing \ud83d\udde3\ufe0f","text":"Chatbot Implementation <p>Modern Chatbot System Using NLP &amp; AI</p> <p>\ud83d\udcc5 2025-01-21 | \u23f1\ufe0f 15 mins</p> Twitter Sentiment Analysis <p>Analyzing Sentiment in Twitter Data</p> <p>\ud83d\udcc5 2025-01-21 | \u23f1\ufe0f 12 mins</p> Email Spam Detection <p>ML-Based Email Spam Classification</p> <p>\ud83d\udcc5 2025-01-21 | \u23f1\ufe0f 10 mins</p> Next Word Prediction <p>LSTM-Based Word Prediction System</p> <p>\ud83d\udcc5 2025-01-21 | \u23f1\ufe0f 8 mins</p> Named Entity Recognition <p>Identifying &amp; Classifying Named Entities</p> <p>\ud83d\udcc5 2025-01-21 | \u23f1\ufe0f 7 mins</p> Text Summarization <p>Summarizing Long Articles Concisely</p> <p>\ud83d\udcc5 2025-01-21 | \u23f1\ufe0f 11 mins</p>"},{"location":"natural-language-processing/chatbot-implementation/","title":"Chatbot Implementation Project","text":""},{"location":"natural-language-processing/chatbot-implementation/#aim","title":"AIM","text":"<p>To develop a chatbot using Natural Language Processing (NLP) and a Naive Bayes classifier for intent classification. The chatbot takes user input, predicts the intent, and generates an appropriate response based on predefined intents and responses stored in a CSV file.</p>"},{"location":"natural-language-processing/chatbot-implementation/#dataset-link","title":"DATASET LINK","text":"<p>https://drive.google.com/file/d/1J7mGS16EkgCEtN7UJtBlJACeqoDbdS4F/view?usp=drive_link</p>"},{"location":"natural-language-processing/chatbot-implementation/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1L2LKfbVv4pb4yzczcRnnU4AkEW-kCZSZ?usp=sharing</p>"},{"location":"natural-language-processing/chatbot-implementation/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>nltk</li> <li>scikit-learn</li> <li>numpy</li> <li>pickle</li> </ul>"},{"location":"natural-language-processing/chatbot-implementation/#description","title":"DESCRIPTION","text":"What is the Requirement of the Project? <p>-A chatbot is required to automate conversations and provide immediate responses to user queries. It can be used to answer  FAQs, provide customer support, and improve user interaction.</p> Why is it Necessary? <ul> <li>Chatbots are essential for improving user engagement and providing 24/7 service.</li> <li>They automate responses, saving time and providing immediate help.</li> </ul> How is it Beneficial and Used? <ul> <li>Chatbots can be used for:</li> <li>Customer service automation.</li> <li>Answering user questions.</li> <li>Guiding users through processes on websites or apps.</li> </ul>"},{"location":"natural-language-processing/chatbot-implementation/#initial-thoughts-and-planning","title":"\"Initial Thoughts and Planning\"","text":"<ul> <li>Intents and Responses: Gathered and stored in CSV format.</li> <li>Preprocessing: Tokenized and lemmatized text to prepare for model training.</li> <li>Model: Built a Naive Bayes classifier to predict intents.</li> <li>Deployment: Deployed the model to predict user queries and return appropriate responses.</li> </ul>"},{"location":"natural-language-processing/chatbot-implementation/#additional-resources-used","title":"Additional Resources Used","text":"<ul> <li>Scikit-learn Documentation</li> <li>Tutorial: Building Chatbots with NLP and Machine Learning</li> </ul>"},{"location":"natural-language-processing/chatbot-implementation/#features-in-the-dataset","title":"FEATURES IN THE DATASET","text":"Feature Description <code>intents</code> User query categories like greetings, farewells. <code>responses</code> Predefined chatbot responses for each intent."},{"location":"natural-language-processing/chatbot-implementation/#steps-and-implementation","title":"STEPS AND IMPLEMENTATION","text":"step 1step 2step 3step 4step 5 <p>Data Preprocessing - Loaded the intents from CSV files. - Cleaned data by removing duplicates and handling null values.</p> <p>Vectorization - Used <code>TfidfVectorizer</code> to convert text into vectors. - Split data into training and testing sets.</p> <p>Model Training - Trained a Naive Bayes classifier on the preprocessed data. - Saved the model for future use with <code>pickle</code>. - Created an intent-response mapping.</p> <p>Prediction and Response Generation** - The chatbot predicts the intent based on user input. - Fetches and returns the appropriate response.</p> <p>Testing - Conducted live interaction tests with the chatbot.</p>"},{"location":"natural-language-processing/chatbot-implementation/#features-not-implemented-yet","title":"Features Not Implemented Yet","text":"<ul> <li>Integration of a deep learning model (e.g., RNN or LSTM) for better context handling.</li> </ul>"},{"location":"natural-language-processing/chatbot-implementation/#flow-chart","title":"Flow Chart","text":"<p><pre><code>graph TD\n    A[Data Preprocessing] --&gt; B[Vectorization]\n    B --&gt; C[Model Training]\n    C --&gt; D[Prediction and Response Generation]\n    D --&gt; E[Testing the Chatbot]\n\n    A1[Load intents from CSV] --&gt; A2[Clean data: remove duplicates and handle nulls]\n    A --&gt; A1\n    A --&gt; A2\n\n    B1[Use TfidfVectorizer to convert text into vectors] --&gt; B2[Split data into training and testing sets]\n    B --&gt; B1\n    B --&gt; B2\n\n    C1[Train Naive Bayes classifier] --&gt; C2[Save model with pickle] --&gt; C3[Create intent-response mapping]\n    C --&gt; C1\n    C --&gt; C2\n    C --&gt; C3\n\n    D1[Chatbot predicts intent] --&gt; D2[Fetch appropriate response based on intent] --&gt; D3[Return response to user]\n    D --&gt; D1\n    D --&gt; D2\n    D --&gt; D3\n\n    E1[Live interaction with chatbot] --&gt; E2[Test accuracy and responses]\n    E --&gt; E1\n    E --&gt; E2</code></pre> <pre><code>#### Example Chatbot Interaction:\n\n```text\nYou: Hello\nBot: Hi, How can I assist you?\n</code></pre></p>"},{"location":"natural-language-processing/chatbot-implementation/#models-and-evaluation-metrics","title":"MODELS AND EVALUATION METRICS","text":"Model Accuracy Precision Recall Naive Bayes 92% 91% 90%"},{"location":"natural-language-processing/chatbot-implementation/#conclusion","title":"CONCLUSION","text":"What Have You Learned? <ul> <li>Building a chatbot using NLP techniques can automate interactions and provide user-friendly interfaces for businesses.   -The Naive Bayes classifier is an effective yet simple model for intent prediction.</li> </ul>"},{"location":"natural-language-processing/chatbot-implementation/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Customer Support Automation</p> <p>-Provide 24/7 automated support for customers.</p> <p>FAQ Automation</p> <ul> <li>Automatically respond to frequently asked questions on websites or apps.</li> </ul>"},{"location":"natural-language-processing/email-spam-detection/","title":"\ud83c\udf1f Email Spam Detection","text":""},{"location":"natural-language-processing/email-spam-detection/#aim","title":"\ud83c\udfaf AIM","text":"<p>To classify emails as spam or ham using machine learning models, ensuring better email filtering and security.</p>"},{"location":"natural-language-processing/email-spam-detection/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>Email Spam Detection Dataset</p>"},{"location":"natural-language-processing/email-spam-detection/#kaggle-notebook","title":"\ud83d\udcda KAGGLE NOTEBOOK","text":"<p>Notebook Link</p> Kaggle Notebook <p></p>"},{"location":"natural-language-processing/email-spam-detection/#tech-stack","title":"\u2699\ufe0f TECH STACK","text":"Category Technologies Languages Python Libraries/Frameworks Scikit-learn, NumPy, Pandas, Matplotlib, Seaborn Databases NOT USED Tools Kaggle, Jupyter Notebook Deployment NOT USED"},{"location":"natural-language-processing/email-spam-detection/#description","title":"\ud83d\udcdd DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>To efficiently classify emails as spam or ham.</li> <li>To improve email security by filtering out spam messages.</li> </ul> How is it beneficial and used? <ul> <li>Helps in reducing unwanted spam emails in user inboxes.</li> <li>Enhances productivity by filtering out irrelevant emails.</li> <li>Can be integrated into email service providers for automatic filtering.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Collected and preprocessed the dataset.</li> <li>Explored various machine learning models.</li> <li>Evaluated models based on performance metrics.</li> <li>Visualized results for better understanding.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Scikit-learn documentation.</li> <li>Various Kaggle notebooks related to spam detection.</li> </ul>"},{"location":"natural-language-processing/email-spam-detection/#project-explanation","title":"\ud83d\udd0d PROJECT EXPLANATION","text":""},{"location":"natural-language-processing/email-spam-detection/#dataset-overview-feature-details","title":"\ud83e\udde9 DATASET OVERVIEW &amp; FEATURE DETAILS","text":"\ud83d\udcc2 spam.csv <ul> <li>The dataset contains the following features:</li> </ul> Feature Name Description Datatype Category Spam or Ham object Text Email text object Length Length of email int64 \ud83d\udee0 Developed Features from spam.csv Feature Name Description Reason Datatype Length Email text length Helps in spam detection int64"},{"location":"natural-language-processing/email-spam-detection/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B[Load Dataset]\n    B --&gt; C[Preprocess Data]\n    C --&gt; D[Vectorize Text]\n    D --&gt; E[Train Models]\n    E --&gt; F[Evaluate Models]\n    F --&gt; G[Visualize Results]</code></pre> Step 1Step 2Step 3Step 4Step 5Step 6 <ul> <li>Load the dataset and clean unnecessary columns.</li> </ul> <ul> <li>Preprocess text and convert categorical labels.</li> </ul> <ul> <li>Convert text into numerical features using CountVectorizer.</li> </ul> <ul> <li>Train machine learning models.</li> </ul> <ul> <li>Evaluate models using accuracy, precision, recall, and F1 score.</li> </ul> <ul> <li>Visualize performance using confusion matrices and heatmaps.</li> </ul>"},{"location":"natural-language-processing/email-spam-detection/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"Section 1Section 2Section 3Section 4Section 5 <ul> <li>Data loading and preprocessing.</li> </ul> <ul> <li>Text vectorization using CountVectorizer.</li> </ul> <ul> <li>Training models (MLP Classifier, MultinomialNB, BernoulliNB).</li> </ul> <ul> <li>Evaluating models using various metrics.</li> </ul> <ul> <li>Visualizing confusion matrices and metric comparisons.</li> </ul>"},{"location":"natural-language-processing/email-spam-detection/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade Off 1Trade Off 2 <ul> <li>Balancing accuracy and computational efficiency.</li> <li>Used Naive Bayes for speed and MLP for improved accuracy.</li> </ul> <ul> <li>Handling false positives vs. false negatives.</li> <li>Tuned models to improve precision for spam detection.</li> </ul>"},{"location":"natural-language-processing/email-spam-detection/#screenshots","title":"\ud83c\udfae SCREENSHOTS","text":"<p>Visualizations and EDA of different features</p> Confusion Matrix comparision <p></p> Model performance graphs Meteric comparison <p></p>"},{"location":"natural-language-processing/email-spam-detection/#models-used-and-their-evaluation-metrics","title":"\ud83d\udcc9 MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy Precision Recall F1 Score MLP Classifier 95% 0.94 0.90 0.92 Multinomial NB 93% 0.91 0.88 0.89 Bernoulli NB 92% 0.89 0.85 0.87"},{"location":"natural-language-processing/email-spam-detection/#conclusion","title":"\u2705 CONCLUSION","text":""},{"location":"natural-language-processing/email-spam-detection/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Text length plays a role in spam detection.</li> <li>Certain words appear more frequently in spam emails.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Gained insights into text vectorization techniques.</li> <li>Understood trade-offs between different classification models.</li> </ul>"},{"location":"natural-language-processing/email-spam-detection/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Email Filtering SystemsSMS Spam Detection <ul> <li>Can be integrated into email services like Gmail and Outlook.</li> </ul> <ul> <li>Used in mobile networks to block spam messages.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/","title":"Name Entity Recognition (NER) Project","text":""},{"location":"natural-language-processing/name-entity-recognition/#aim","title":"AIM","text":"<p>To develop a system that identifies and classifies named entities (such as persons, organizations, locations, dates, etc.) in text using Named Entity Recognition (NER) with SpaCy.</p>"},{"location":"natural-language-processing/name-entity-recognition/#dataset-link","title":"DATASET LINK","text":"<p>N/A (This project uses text input for NER analysis, not a specific dataset) - It uses real time data as input .</p>"},{"location":"natural-language-processing/name-entity-recognition/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://colab.research.google.com/drive/1pBIEFA4a9LzyZKUFQMCypQ22M6bDbXM3?usp=sharing</p>"},{"location":"natural-language-processing/name-entity-recognition/#libraries-needed","title":"LIBRARIES NEEDED","text":"<ul> <li>SpaCy</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>Named Entity Recognition (NER) is essential to automatically extract and classify key entities from text, such as persons, organizations, locations, and more.</li> <li>This helps in analyzing and organizing data efficiently, enabling various NLP applications like document analysis and information retrieval.</li> </ul> Why is it necessary? <ul> <li>NER is used for understanding and structuring unstructured text, which is widely applied in industries such as healthcare, finance, and e-commerce.</li> <li>It allows users to extract actionable insights from large volumes of text data</li> </ul> How is it beneficial and used? <ul> <li>NER plays a key role in tasks such as document summarization, information retrieval.</li> <li>It automates the extraction of relevant entities, which reduces manual effort and improves efficiency.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>The project leverages SpaCy's pre-trained NER models, enabling easy text analysis without the need for training custom models.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#mention-any-additional-resources-used-blogs-books-chapters-articles-research-papers-etc","title":"Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.)","text":"<ul> <li>SpaCy Documentation: SpaCy NER</li> <li>NLP in Python by Steven Bird et al.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#explanation","title":"EXPLANATION","text":""},{"location":"natural-language-processing/name-entity-recognition/#details-of-the-different-entity-types","title":"DETAILS OF THE DIFFERENT ENTITY TYPES","text":"<p>The system extracts the following entity types:</p> Entity Type Description PERSON Names of people (e.g., \"Anuska\") ORG Organizations (e.g., \"Google\", \"Tesla\") LOC Locations (e.g., \"New York\", \"Mount Everest\") DATE Dates (e.g., \"January 1st, 2025\") GPE Geopolitical entities (e.g., \"India\", \"California\")"},{"location":"natural-language-processing/name-entity-recognition/#what-i-have-done","title":"WHAT I HAVE DONE","text":""},{"location":"natural-language-processing/name-entity-recognition/#step-1-data-collection-and-preparation","title":"Step 1: Data collection and preparation","text":"<ul> <li>Gathered sample text for analysis (provided by users in the app).</li> <li>Explored the text structure and identified entity types.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#step-2-ner-model-implementation","title":"Step 2: NER model implementation","text":"<ul> <li>Integrated SpaCy's pre-trained NER model (<code>en_core_web_sm</code>).</li> <li>Extracted named entities and visualized them with labels and color coding.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#step-3-testing-and-validation","title":"Step 3: Testing and validation","text":"<ul> <li>Validated results with multiple test cases to ensure entity accuracy.</li> <li>Allowed users to input custom text for NER analysis in real-time.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":""},{"location":"natural-language-processing/name-entity-recognition/#trade-off-1-pre-trained-model-vs-custom-model","title":"Trade Off 1: Pre-trained model vs. custom model","text":"<ul> <li>Pre-trained models provide quick results but may lack accuracy for domain-specific entities.</li> <li>Custom models can improve accuracy but require additional data and training time.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#trade-off-2-real-time-analysis-vs-batch-processing","title":"Trade Off 2: Real-time analysis vs. batch processing","text":"<ul> <li>Real-time analysis in a web app enhances user interaction but might slow down with large text inputs.</li> <li>Batch processing could be more efficient for larger datasets.</li> </ul>"},{"location":"natural-language-processing/name-entity-recognition/#screenshots","title":"SCREENSHOTS","text":""},{"location":"natural-language-processing/name-entity-recognition/#ner-example","title":"NER Example","text":"<p>``` mermaid graph LR     A[Start] --&gt; B[Text Input];     B --&gt; C[NER Analysis];     C --&gt; D{Entities Extracted};     D --&gt;|Person| E[Anuska];     D --&gt;|Location| F[New York];     D --&gt;|Organization| G[Google];     D --&gt;|Date| H[January 1st, 2025];</p>"},{"location":"natural-language-processing/next-word-pred/","title":"Next Word Prediction using LSTM","text":""},{"location":"natural-language-processing/next-word-pred/#aim","title":"AIM","text":"<p>To predict the next word using LSTM.</p>"},{"location":"natural-language-processing/next-word-pred/#dataset-link","title":"DATASET LINK","text":"<p>Dataset</p>"},{"location":"natural-language-processing/next-word-pred/#notebook-link","title":"NOTEBOOK LINK","text":"<p>Code</p>"},{"location":"natural-language-processing/next-word-pred/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>seaborn</li> <li>tensorflow</li> <li>keras</li> </ul>"},{"location":"natural-language-processing/next-word-pred/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>To create an intelligent system capable of predicting the next word in a sentence based on its context.</li> <li>The need for such a system arises in applications like autocomplete, chatbots, and virtual assistants.</li> </ul> Why is it necessary? <ul> <li>Enhances user experience in text-based applications by offering accurate suggestions.</li> <li>Reduces typing effort, especially in mobile applications.</li> </ul> How is it beneficial and used? <ul> <li>Improves productivity: By predicting words, users can complete sentences faster.</li> <li>Supports accessibility: Assists individuals with disabilities in typing.</li> <li>Boosts efficiency: Helps in real-time text generation in NLP applications like chatbots and email composition.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Studied LSTM architecture and its suitability for sequential data.</li> <li>Explored similar projects and research papers to understand data preprocessing techniques.</li> <li>Experimented with tokenization, padding, and sequence generation for the dataset.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Blogs on LSTM from Towards Data Science.</li> <li>TensorFlow and Keras official documentation.</li> </ul>"},{"location":"natural-language-processing/next-word-pred/#explanation","title":"EXPLANATION","text":""},{"location":"natural-language-processing/next-word-pred/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"natural-language-processing/next-word-pred/#project-workflow","title":"PROJECT WORKFLOW","text":"Step 1Step 2Step 3Step 4Step 5Step 6 <p>Initial data exploration and understanding:</p> <ul> <li>Gathered text data from open-source datasets.</li> <li>Analyzed the structure of the data.</li> <li>Performed basic text statistics to understand word frequency and distribution.</li> </ul> <p>Data cleaning and preprocessing</p> <ul> <li>Removed punctuation and convert text to lowercase.</li> <li>Tokenized text into sequences and pad them to uniform length.</li> </ul> <p>Feature engineering and selection</p> <ul> <li>Created input-output pairs for next-word prediction using sliding window techniques on tokenized sequences.</li> </ul> <p>Model training and evaluation:</p> <ul> <li>Used an embedding layer to represent words in a dense vector space.</li> <li>Implemented LSTM-based sequential models to learn context and dependencies in text. </li> <li>Experimented with hyperparameters like sequence length, LSTM units, learning rate, and batch size.</li> </ul> <p>Model optimization and fine-tuning</p> <ul> <li>Adjusted hyperparameters like embedding size, LSTM units, and learning rate.</li> </ul> <p>Validation and testing</p> <ul> <li>Used metrics like accuracy and perplexity to assess prediction quality.  </li> <li>Validated the model on unseen data to test generalization. </li> </ul>"},{"location":"natural-language-processing/next-word-pred/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade-Off 1Trade-Off 2 <p>Accuracy vs Training Time:</p> <ul> <li>Solution: Balanced by reducing the model's complexity and using an efficient optimizer.</li> </ul> <p>Model complexity vs. Overfitting:</p> <ul> <li>Solution: Implemented dropout layers and monitored validation loss during training.</li> </ul>"},{"location":"natural-language-processing/next-word-pred/#screenshots","title":"SCREENSHOTS","text":"<p>Project workflow</p> <pre><code>  graph LR\n    A[Start] --&gt; B{Data Preprocessed?};\n    B --&gt;|No| C[Clean and Tokenize];\n    C --&gt; D[Create Sequences];\n    D --&gt; B;\n    B --&gt;|Yes| E[Model Designed?];\n    E --&gt;|No| F[Build LSTM/Transformer];\n    F --&gt; E;\n    E --&gt;|Yes| G[Train Model];\n    G --&gt; H{Performant?};\n    H --&gt;|No| I[Optimize Hyperparameters];\n    I --&gt; G;\n    H --&gt;|Yes| J[Deploy Model];\n    J --&gt; K[End];</code></pre>"},{"location":"natural-language-processing/next-word-pred/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy MSE R2 Score LSTM 72% - -"},{"location":"natural-language-processing/next-word-pred/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> LSTM Loss <p></p>"},{"location":"natural-language-processing/next-word-pred/#conclusion","title":"CONCLUSION","text":""},{"location":"natural-language-processing/next-word-pred/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>The importance of preprocessing for NLP tasks.</li> <li>How padding and embeddings improve the model\u2019s ability to generalize.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Learned how LSTMs handle sequential dependencies.</li> <li>Understood the role of softmax activation in predicting word probabilities.</li> </ul> Challenges faced and how they were overcome <ul> <li>Challenge: Large vocabulary size causing high memory usage.</li> <li>Solution: Limited vocabulary to the top frequent words.</li> </ul>"},{"location":"natural-language-processing/next-word-pred/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Text Autocompletion</p> <ul> <li>Used in applications like Gmail and search engines to enhance typing speed.</li> </ul> <p>Virtual Assistants</p> <ul> <li>Enables better conversational capabilities in chatbots and AI assistants.</li> </ul>"},{"location":"natural-language-processing/text-summarization/","title":"\ud83d\udcdcText Summarization","text":""},{"location":"natural-language-processing/text-summarization/#aim","title":"\ud83c\udfaf AIM","text":"<p>Develop a model to summarize long articles into short, concise summaries.</p>"},{"location":"natural-language-processing/text-summarization/#dataset-link","title":"\ud83d\udcca DATASET LINK","text":"<p>CNN DailyMail News Dataset</p>"},{"location":"natural-language-processing/text-summarization/#notebook-link","title":"\ud83d\udcd3 NOTEBOOK LINK","text":"Kaggle Notebook"},{"location":"natural-language-processing/text-summarization/#libraries-needed","title":"\u2699\ufe0f LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>matplotlib</li> <li>keras</li> <li>tensorflow</li> <li>spacy</li> <li>pytextrank</li> <li>TfidfVectorizer</li> <li>Transformer (Bart)</li> </ul>"},{"location":"natural-language-processing/text-summarization/#description","title":"\ud83d\udcdd DESCRIPTION","text":"What is the requirement of the project? <ul> <li>A robust system to summarize text efficiently is essential for handling large volumes of information.</li> <li>It helps users quickly grasp key insights without reading lengthy documents.</li> </ul> Why is it necessary? <ul> <li>Large amounts of text can be overwhelming and time-consuming to process.</li> <li>Automated summarization improves productivity and aids decision-making in various fields like journalism, research, and customer support.</li> </ul> How is it beneficial and used? <ul> <li>Provides a concise summary while preserving essential information.</li> <li>Used in news aggregation, academic research, and AI-powered assistants for quick content consumption.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li>Explored different text summarization techniques, including extractive and abstractive methods.</li> <li>Implemented models like TextRank, BART, and T5 to compare their effectiveness.</li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.). <ul> <li>Documentation from Hugging Face Transformers</li> <li>Research Paper: \"Text Summarization using Deep Learning\"</li> <li>Blog: \"Introduction to NLP-based Summarization Techniques\"</li> </ul>"},{"location":"natural-language-processing/text-summarization/#explanation","title":"\ud83d\udd0d EXPLANATION","text":""},{"location":"natural-language-processing/text-summarization/#details-of-the-different-features","title":"\ud83e\udde9 DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"natural-language-processing/text-summarization/#datasetcsv","title":"\ud83d\udcc2 dataset.csv","text":"<p>The dataset contains features like sentence importance, word frequency, and linguistic structures that help in generating meaningful summaries.</p> Feature Name Description Id A unique Id for each row Article Entire article written on CNN Daily mail Highlights Key Notes of the article"},{"location":"natural-language-processing/text-summarization/#developed-features","title":"\ud83d\udee0 Developed Features","text":"Feature Description <code>sentence_rank</code> Rank of a sentence based on importance using TextRank <code>word_freq</code> Frequency of key terms in the document <code>tf-idf_score</code> Term Frequency-Inverse Document Frequency for words <code>summary_length</code> Desired length of the summary <code>generated_summary</code> AI-generated condensed version of the original text"},{"location":"natural-language-processing/text-summarization/#project-workflow","title":"\ud83d\udee4 PROJECT WORKFLOW","text":"<p>Project flowchart</p> <pre><code>  graph LR\nA[Start] --&gt; B[Load Dataset]\nB --&gt; C[Preprocessing]\nC --&gt; D[TextRank + TF-IDF / Transformer Models]\nD --&gt; E{Compare Performance}\nE --&gt;|Best Model| F[Deploy]\nE --&gt;|Retry| C;</code></pre>"},{"location":"natural-language-processing/text-summarization/#procedure","title":"PROCEDURE","text":"Step 1Step 2Step 3Step 4Step 5 <p>Exploratory Data Analysis:</p> <ul> <li>Loaded the CNN/DailyMail dataset using pandas.</li> <li>Explored dataset features like article and highlights, ensuring the correct format for summarization.</li> <li>Analyzed the distribution of articles and their corresponding summaries.</li> </ul> <p>Data cleaning and preprocessing:</p> <ul> <li>Removed unnecessary columns (like id) and checked for missing values.</li> <li>Tokenized articles into sentences and words, removing stopwords and special characters.</li> <li>Preprocessed the text using basic NLP techniques such as lowercasing, lemmatization, and removing non-alphanumeric characters.</li> </ul> <p>Feature engineering and selection:</p> <ul> <li>For TextRank-based summarization, calculated sentence similarity using TF-IDF (Term Frequency-Inverse Document Frequency) and Cosine Similarity.</li> <li>Selected top-ranked sentences based on their importance and relevance to the article.</li> <li>Applied transformers-based models like BART and T5 for abstractive summarization.</li> <li>Applied transformers-based models like BART and T5 for abstractive summarization.</li> </ul> <p>Model training and evaluation:</p> <ul> <li>For the TextRank summarization approach, created a similarity matrix based on TF-IDF and Cosine Similarity.</li> <li>For transformer-based methods, used Hugging Face's BART and T5 models, summarizing articles with their pre-trained weights.</li> <li>Evaluated the summarization models based on BLEU, ROUGE, and Cosine Similarity metrics.</li> </ul> <p>Validation and testing:</p> <ul> <li>Tested both extractive and abstractive summarization models on unseen data to ensure generalizability.</li> <li>Plotted confusion matrices to visualize True Positives, False Positives, and False Negatives, ensuring effective model performance.</li> </ul>"},{"location":"natural-language-processing/text-summarization/#code-explanation","title":"\ud83d\udda5 CODE EXPLANATION","text":"TextRank algorithmTransformersTTF-IDF Algorithm <p>Important Function:</p> <pre><code>graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(graph)\n\nExample Input: \nsimilarity_matrix = np.array([\n    [0.0, 0.2, 0.1],  # Sentence 1\n    [0.2, 0.0, 0.3],  # Sentence 2 \n    [0.1, 0.3, 0.0]]) # Sentence 3\n\ngraph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(graph)\n\nOutput:\n{0: 0.25, 1: 0.45, 2: 0.30} #That means sentence 2(0.45) has more importance than others\n</code></pre> <p>Important Function:</p> <pre><code>pipeline(\"summarization\") - Initializes a pre-trained transformer model for summarization.\ngenerated_summary = summarization_pipeline(article, max_length=150, min_length=50, do_sample=False) \nThis Generates a summary using a transformer model.\n\nExample Input:\narticle = \"The Apollo program was a NASA initiative that landed humans on the Moon between 1969 and 1972, \nwith Apollo 11 being the first mission.\"\n\nOutput:\nThe Apollo program was a NASA initiative that landed humans on the Moon between 1969 and 1972. \nApollo 11 was the first mission.\n</code></pre> <p>Important Function:</p> <pre><code>vectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(processed_sentences)\n\nExample Input:\nprocessed_sentences = [\n\"apollo program nasa initiative landed humans moon 1969 1972\",\n\"apollo 11 first mission land moon neil armstrong buzz aldrin walked surface\",\n\"apollo program significant achievement space exploration cold war space race\"]\n\nOutput:\n['1969', '1972', 'achievement', 'aldrin', 'apollo', 'armstrong', 'buzz', 'cold', 'exploration', \n'first', 'humans', 'initiative', 'land', 'landed', 'moon', 'nasa', 'neil', 'program', 'race', \n'significant', 'space', 'surface', 'walked', 'war']\n</code></pre>"},{"location":"natural-language-processing/text-summarization/#project-trade-offs-and-solutions","title":"\u2696\ufe0f PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade-off 1Trade-off 2Trade-off 3 <p>Training Dataset being over 1.2Gb, which is too large for local machines.</p> <ul> <li>Solution: Instead of Training a model on train dataset, Used Test Dataset for training and validation.</li> </ul> <p>Transformer models (BART/T5) required high computational resources and long inference times for summarizing large articles.</p> <ul> <li>Solution: Model Pruning: Used smaller versions of transformer models (e.g., distilBART or distilT5) to reduce the computational load without compromising much on performance.</li> </ul> <p>TextRank summary might miss nuances and context, leading to less accurate or overly simplistic outputs compared to transformer-based models.</p> <ul> <li>Solution: Combined TextRank and Transformer-based summarization models in a hybrid approach to leverage the best of both worlds\u2014speed from TextRank and accuracy from transformers.</li> </ul>"},{"location":"natural-language-processing/text-summarization/#screenshots","title":"\ud83d\uddbc SCREENSHOTS","text":"Confusion Matrix TF-IDF Confusion MatrixTextRank Confusion MatrixTransformers Confusion Matrix"},{"location":"natural-language-processing/text-summarization/#conclusion","title":"\u2705CONCLUSION","text":""},{"location":"natural-language-processing/text-summarization/#key-learnings","title":"\ud83d\udd11 KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Data Complexity: News articles vary in length and structure, requiring different summarization techniques.</li> <li>Text Preprocessing: Cleaning text (e.g., stopword removal, tokenization) significantly improves summarization quality.</li> <li>Feature Extraction: Techniques like TF-IDF, TextRank, and Transformer embeddings help in effective text representation for summarization models.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Model Selection: Comparing extractive (TextRank, TF-IDF) and abstractive (Transformers) models to determine the best summarization approach.</li> </ul> Challenges faced and how they were overcome <ul> <li>Long Text Processing: Splitting lengthy articles into manageable sections before summarization.</li> <li>Computational Efficiency: Used batch processing and model optimization to handle large datasets efficiently.</li> </ul>"},{"location":"natural-language-processing/text-summarization/#use-cases","title":"\ud83c\udf0d USE CASES","text":"Application 1Application 2 <p>News Aggregation &amp; Personalized Summaries</p> <ul> <li>Automating news summarization helps users quickly grasp key events without reading lengthy articles.</li> <li>Used in news apps, digital assistants, and content curation platforms.</li> </ul> <p>Legal &amp; Academic Document Summarization</p> <ul> <li>Helps professionals extract critical insights from lengthy legal or research documents.</li> <li>Reduces the time needed for manual reading and analysis.</li> </ul>"},{"location":"natural-language-processing/twitter-sentiment-analysis/","title":"Twitter Sentiment Analysis","text":""},{"location":"natural-language-processing/twitter-sentiment-analysis/#aim","title":"AIM","text":"<p>To analyze sentiment in Twitter data using natural language processing techniques.</p>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#dataset-link","title":"DATASET LINK","text":"<p>https://www.kaggle.com/datasets/kazanova/sentiment140</p>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#notebook-link","title":"NOTEBOOK LINK","text":"<p>https://drive.google.com/drive/folders/1F6BLxvp6qIAgGZOZ2rC370EmKhj5W1FC?usp=sharing</p>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#libraries-needed","title":"LIBRARIES NEEDED","text":"LIBRARIES USED <ul> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> <li>seaborn</li> <li>matplotlib</li> <li>tensorflow</li> <li>keras</li> <li>nltk</li> <li>multiprocessing</li> <li>tqdm</li> <li>os</li> </ul>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#description","title":"DESCRIPTION","text":"<p>What is the requirement of the project?</p> <ul> <li>The project aims to perform sentiment analysis on Twitter data.</li> <li>This involves extracting tweets related to specific topics or keywords, processing these tweets using natural language processing (NLP) techniques to determine the sentiment (positive or negative), and presenting insights derived from the analysis.</li> </ul> Why is it necessary? <ul> <li>Twitter is a rich source of real-time public opinion and sentiment. Analyzing tweets can provide valuable insights into public perception of events, products, brands, or topics of interest.</li> <li>This information is crucial for businesses, governments, and researchers to make informed decisions, understand public sentiment trends, and gauge the success of marketing campaigns or policy changes.</li> </ul> How is it beneficial and used? <ul> <li>Business Insights: Companies can understand customer feedback and sentiments towards their products or services.</li> <li>Brand Management: Monitor brand sentiment and respond to customer concerns or issues in real-time.</li> <li>Market Research: Identify trends and sentiments related to specific topics or industries.</li> <li>Social Listening: Understand public opinion on current events, policies, or social issues.</li> <li>Customer Service Improvement: Improve customer service by analyzing sentiment towards customer interactions.</li> </ul> How did you start approaching this project? (Initial thoughts and planning) <ul> <li> <p>Choose appropriate NLP techniques for sentiment analysis, such as:</p> <ul> <li>Bag-of-Words (BoW) and TF-IDF: Represent tweets as numerical vectors.</li> <li>Sentiment Lexicons: Use dictionaries of words annotated with sentiment scores (e.g., Vader sentiment lexicon).</li> <li>Machine Learning Models: Train supervised classifiers (e.g., Naive Bayes, SVM, or neural networks) on labeled data for sentiment prediction.</li> </ul> </li> <li> <p>Model Evaluation: Evaluate the performance of the sentiment analysis model using metrics like accuracy. Cross-validation techniques can be used to ensure robustness.</p> </li> <li> <p>Visualization and Insights: Visualize sentiment trends over time or across different categories using charts (e.g., line plots, bar charts). Generate insights based on the analysis results.</p> </li> <li> <p>Deployment: Deploy the sentiment analysis system as a standalone application or integrate it into existing systems for real-time monitoring and analysis.</p> </li> </ul> Mention any additional resources used (blogs, books, chapters, articles, research papers, etc.) <ul> <li>GeeksforGeeks Twitter Sentiment Analysis</li> <li>YouTube Video</li> </ul>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#explanation","title":"EXPLANATION","text":""},{"location":"natural-language-processing/twitter-sentiment-analysis/#details-of-the-different-features","title":"DETAILS OF THE DIFFERENT FEATURES","text":""},{"location":"natural-language-processing/twitter-sentiment-analysis/#what-i-have-done","title":"WHAT I HAVE DONE","text":"Step 1Step 2Step 3Step 4Step 5Step 6 <p>Initial data exploration and understanding:</p> <ul> <li>Gathered Twitter data using pre-existing datasets (Kaggle).</li> <li>Understand the structure of the data (e.g., tweet text, metadata like timestamps, user information).</li> <li>Explore basic statistics and distributions of data features.</li> </ul> <p>Data cleaning and preprocessing:</p> <ul> <li>Remove or handle noisy data such as URLs, special characters, and emojis.</li> <li>Tokenize tweets into individual words or tokens.</li> <li>Remove stopwords (commonly used words that do not carry significant meaning).</li> <li>Normalize text through techniques like stemming to reduce variations of words.</li> </ul> <p>Feature engineering and selection:</p> <ul> <li>Convert text data into numerical representations suitable for machine learning models (e.g., Bag-of-Words, TF-IDF).</li> <li>Select relevant features that contribute most to the sentiment analysis task.</li> </ul> <p>Model training and evaluation:</p> <ul> <li>Split the dataset into training and testing sets.</li> <li>Choose appropriate machine learning models (e.g., Naive Bayes, RNN LSTM, logistic regression) for sentiment analysis.</li> <li>Train the models on the training data and evaluate their performance using metrics like accuracy.</li> </ul> <p>Model optimization and fine-tuning:</p> <ul> <li>Fine-tune the hyperparameters of the selected models to improve performance.</li> <li>Consider techniques like grid search or random search to find optimal parameters.</li> <li>Experiment with different models or combinations of models to achieve better results.</li> </ul> <p>Validation and testing:</p> <ul> <li>Validate the trained models on a separate validation set to ensure generalizability.</li> <li>Test the final model on unseen data (testing set or new tweets) to assess its performance in real-world scenarios.</li> <li>Iterate on the model and preprocessing steps based on validation results to improve accuracy and robustness.</li> </ul>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#project-trade-offs-and-solutions","title":"PROJECT TRADE-OFFS AND SOLUTIONS","text":"Trade-off 1Trade-off 2 <p>Stemming process took a lot of computational time to process over 1.6 million datapoints.</p> <ul> <li>Solution: Divided the data into batches and applied parallel processing.</li> </ul> <p>In RNN based LSTM, overfitting problem occurred.</p> <ul> <li>Solution: Tried to fix it using Dropout layer, early stopping criteria.</li> </ul>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#screenshots","title":"SCREENSHOTS","text":"<p>Project structure or tree diagram</p> <pre><code>  graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> Visualizations and EDA of different features Sentiment Distribution <p></p> Model performance graphs LR Confusion MatrixLR ROC CurveNaive Bayes Confusion MatrixNaive Bayes ROC Curve <p></p> <p></p> <p></p> <p></p>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#models-used-and-their-evaluation-metrics","title":"MODELS USED AND THEIR EVALUATION METRICS","text":"Model Accuracy MSE R2 Score Logistic Regression 77% 0.1531724703945824 0.3873101184216704 Naive Bayes 75% 0.17476773790874897 0.3009290483650041 RNN LSTM 77.84% - -"},{"location":"natural-language-processing/twitter-sentiment-analysis/#models-comparison-graphs","title":"MODELS COMPARISON GRAPHS","text":"<p>Models Comparison Graphs</p> LSTM AccuracyLSTM Loss <p></p> <p></p>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#conclusion","title":"CONCLUSION","text":""},{"location":"natural-language-processing/twitter-sentiment-analysis/#key-learnings","title":"KEY LEARNINGS","text":"<p>Insights gained from the data</p> <ul> <li>Data Variety: Twitter data provides a rich source of real-time, diverse opinions and sentiments.</li> <li>Text Preprocessing: Importance of cleaning and preprocessing text data (e.g., removing stopwords, stemming/lemmatization) for better analysis.</li> <li>Feature Extraction: Techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings (e.g., Word2Vec, GloVe) to represent text numerically for machine learning models.</li> </ul> Improvements in understanding machine learning concepts <ul> <li>Model Selection: Experimenting with various algorithms to find the most suitable for sentiment classification (e.g., logistic regression, naive bayes, neural networks).</li> </ul> Challenges faced and how they were overcome <ul> <li>Noise in Data: Dealing with noise from hashtags, emojis, and slang in tweets through effective preprocessing techniques.</li> <li>Computational Resources: Managing large volumes of data and resource-intensive computations by optimizing code and leveraging cloud computing platforms if necessary.</li> </ul>"},{"location":"natural-language-processing/twitter-sentiment-analysis/#use-cases","title":"USE CASES","text":"Application 1Application 2 <p>Brand Monitoring and Customer Feedback Analysis</p> <ul> <li>This application allows businesses to leverage Twitter sentiment analysis as a valuable tool for customer relationship management, brand reputation management, and strategic decision-making based on real-time customer feedback and sentiment analysis.</li> </ul> <p>Financial Market Analysis and Investment Decisions</p> <ul> <li>This application showcases how Twitter sentiment analysis can be leveraged in the financial sector to gain competitive advantages, improve investment strategies, and manage risks effectively based on public sentiment towards financial markets and specific stocks.</li> </ul>"}]}