{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### What is Word2Vec?\n\n**Word2Vec** is a technique to learn word embeddings using neural networks. The primary goal is to represent words in a continuous vector space where semantically similar words are mapped to nearby points. Word2Vec can be implemented using two main architectures:\n\n1. **Continuous Bag of Words (CBOW)**: Predicts the target word based on the context words (surrounding words).\n2. **Skip-gram**: Predicts the context words based on a given target word.\n\nIn this example, we'll focus on the Skip-gram approach, which is more commonly used in practice. The Skip-gram model tries to maximize the probability of context words given a target word.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\n\nclass Word2Vec:\n    def __init__(self, window_size=2, embedding_dim=10, learning_rate=0.01):\n        # Initialize parameters\n        self.window_size = window_size\n        self.embedding_dim = embedding_dim\n        self.learning_rate = learning_rate\n        self.vocabulary = {}\n        self.word_index = {}\n        self.index_word = {}\n        self.W1 = None\n        self.W2 = None\n\n    def tokenize(self, documents):\n        # Tokenize documents and build vocabulary\n        vocabulary = set()\n        for doc in documents:\n            words = doc.split()\n            vocabulary.update(words)\n        \n        self.vocabulary = list(vocabulary)\n        self.word_index = {word: idx for idx, word in enumerate(self.vocabulary)}\n        self.index_word = {idx: word for idx, word in enumerate(self.vocabulary)}\n\n    def generate_training_data(self, documents):\n        # Generate training data for the Skip-gram model\n        training_data = []\n        for doc in documents:\n            words = doc.split()\n            for idx, word in enumerate(words):\n                target_word = self.word_index[word]\n                context = [self.word_index[words[i]] for i in range(max(0, idx - self.window_size), min(len(words), idx + self.window_size + 1)) if i != idx]\n                for context_word in context:\n                    training_data.append((target_word, context_word))\n        return training_data\n\n    def train(self, documents, epochs=1000):\n        # Tokenize the documents and generate training data\n        self.tokenize(documents)\n        training_data = self.generate_training_data(documents)\n        \n        # Initialize weight matrices with random values\n        vocab_size = len(self.vocabulary)\n        self.W1 = np.random.uniform(-1, 1, (vocab_size, self.embedding_dim))\n        self.W2 = np.random.uniform(-1, 1, (self.embedding_dim, vocab_size))\n        \n        for epoch in range(epochs):\n            loss = 0\n            for target_word, context_word in training_data:\n                # Forward pass\n                h = self.W1[target_word]  # Hidden layer representation of the target word\n                u = np.dot(h, self.W2)    # Output layer scores\n                y_pred = self.softmax(u) # Predicted probabilities\n                \n                # Calculate error\n                e = np.zeros(vocab_size)\n                e[context_word] = 1\n                error = y_pred - e\n                \n                # Backpropagation\n                self.W1[target_word] -= self.learning_rate * np.dot(self.W2, error)\n                self.W2 -= self.learning_rate * np.outer(h, error)\n                \n                # Calculate loss (cross-entropy)\n                loss -= np.log(y_pred[context_word])\n            \n            if (epoch + 1) % 100 == 0:\n                print(f'Epoch {epoch + 1}, Loss: {loss}')\n\n    def softmax(self, x):\n        # Softmax function to convert scores into probabilities\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n\n    def get_word_vector(self, word):\n        # Retrieve the vector representation of a word\n        return self.W1[self.word_index[word]]\n\n    def get_vocabulary(self):\n        # Retrieve the vocabulary\n        return self.vocabulary","metadata":{"execution":{"iopub.status.busy":"2024-07-20T10:18:01.791652Z","iopub.execute_input":"2024-07-20T10:18:01.792119Z","iopub.status.idle":"2024-07-20T10:18:01.808809Z","shell.execute_reply.started":"2024-07-20T10:18:01.792084Z","shell.execute_reply":"2024-07-20T10:18:01.807756Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n    # Basic example usage\n    documents = [\n        \"the cat sat on the mat\",\n        \"the dog ate my homework\",\n        \"the cat ate the dog food\"\n    ]\n\n    word2vec = Word2Vec()\n    word2vec.train(documents)\n\n    # Getting the word vector for 'cat'\n    word_vector = word2vec.get_word_vector('cat')\n    print(\"Vector for 'cat':\", word_vector)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T10:14:57.090284Z","iopub.execute_input":"2024-07-20T10:14:57.091266Z","iopub.status.idle":"2024-07-20T10:14:58.587081Z","shell.execute_reply.started":"2024-07-20T10:14:57.091227Z","shell.execute_reply":"2024-07-20T10:14:58.586006Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 100, Loss: 75.35100287508789\nEpoch 200, Loss: 72.30321253446704\nEpoch 300, Loss: 71.89275194982119\nEpoch 400, Loss: 71.8635105655411\nEpoch 500, Loss: 71.92973913785089\nEpoch 600, Loss: 72.0327548745859\nEpoch 700, Loss: 72.15655555085135\nEpoch 800, Loss: 72.29658835738203\nEpoch 900, Loss: 72.45216981302312\nEpoch 1000, Loss: 72.62419809127482\nVector for 'cat': [-0.07713173 -1.51443462 -0.67274315  0.31749278  0.37492043 -0.29362281\n  0.83514656 -0.73169726 -0.60048742  1.68112817]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Additional example usage\nif __name__ == \"__main__\":\n    # Sample documents\n    documents = [\n        \"I love programming in Python\",\n        \"Machine learning is fun\",\n        \"Python is a versatile language\",\n        \"Learning new skills is always beneficial\"\n    ]\n\n    # Initialize and train the Word2Vec model\n    word2vec = Word2Vec()\n    word2vec.train(documents)\n\n    # Print the vocabulary\n    print(\"Vocabulary:\", word2vec.get_vocabulary())\n\n    # Print the word vectors for each word in the vocabulary\n    print(\"Word Vectors:\")\n    for word in word2vec.get_vocabulary():\n        vector = word2vec.get_word_vector(word)\n        print(f\"Vector for '{word}':\", vector)\n\n    # More example documents with mixed content\n    more_documents = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"a journey of a thousand miles begins with a single step\",\n        \"to be or not to be that is the question\",\n        \"the rain in Spain stays mainly in the plain\",\n        \"all human beings are born free and equal in dignity and rights\"\n    ]\n\n    # Initialize and train the Word2Vec model on new documents\n    word2vec_more = Word2Vec()\n    word2vec_more.train(more_documents)\n\n    # Print the word vectors for selected words\n    print(\"\\nWord Vectors for new documents:\")\n    for word in ['quick', 'journey', 'be', 'rain', 'human']:\n        vector = word2vec_more.get_word_vector(word)\n        print(f\"Vector for '{word}':\", vector)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T10:18:04.093039Z","iopub.execute_input":"2024-07-20T10:18:04.093849Z","iopub.status.idle":"2024-07-20T10:18:11.208990Z","shell.execute_reply.started":"2024-07-20T10:18:04.093806Z","shell.execute_reply":"2024-07-20T10:18:11.207981Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 100, Loss: 87.29077775759023\nEpoch 200, Loss: 79.18780389421288\nEpoch 300, Loss: 77.94910795980299\nEpoch 400, Loss: 77.66245836714485\nEpoch 500, Loss: 77.60853401053721\nEpoch 600, Loss: 77.63645020161269\nEpoch 700, Loss: 77.70072895043553\nEpoch 800, Loss: 77.78464829390958\nEpoch 900, Loss: 77.88140561618562\nEpoch 1000, Loss: 77.98820209880577\nVocabulary: ['love', 'learning', 'Learning', 'a', 'beneficial', 'new', 'Machine', 'fun', 'always', 'I', 'language', 'Python', 'programming', 'skills', 'versatile', 'is', 'in']\nWord Vectors:\nVector for 'love': [ 1.25611249  0.72399226  0.73932742 -1.1793396  -0.03000625  0.76406502\n -1.51633466 -0.21015759 -0.5542326  -0.18235466]\nVector for 'learning': [-0.05385896 -1.602021    1.4943953   0.44231527 -0.07299037 -0.74754454\n -1.3903911   1.00685072  0.4544704  -1.36141874]\nVector for 'Learning': [-1.74465271 -0.68294311 -1.5425367  -0.05822687 -0.26804989 -0.1100379\n  0.64812036  0.93995388  0.00906527  0.51813513]\nVector for 'a': [ 0.42309774  0.36851908 -1.21744857  0.03453108  0.56061874  0.66453021\n  1.48116814 -1.41757756  1.6581559   1.24310819]\nVector for 'beneficial': [-0.64162578 -0.05537255  1.84063196  1.11458998 -0.18536477 -0.51397477\n  1.1567393  -0.08573849  1.02205477 -0.10391583]\nVector for 'new': [-0.84236249 -1.1861667   0.47188798 -0.90436565 -1.54968604  0.26117848\n  0.31966193  0.60843338  1.44536674  0.45516902]\nVector for 'Machine': [ 0.56576103 -0.22504546  0.37463336  2.0133637  -0.98459966 -0.09227832\n  0.09937559  0.87758714  0.33328967  0.62105041]\nVector for 'fun': [ 0.74045829 -0.10269314  0.38566757  1.49587936 -0.46728908  0.6369226\n  0.35471937  1.21234412  0.62831945  1.05840841]\nVector for 'always': [-0.50630359 -1.0742406  -1.41055437  0.88033565 -1.75850862  0.06157641\n -0.07544321 -0.34542803  1.01462117 -0.42767684]\nVector for 'I': [ 0.08341753  0.90589692 -0.11179104  0.21613275 -1.58854584 -1.02474237\n -1.15777817 -1.53751946 -0.30385788  1.19445792]\nVector for 'language': [-0.14495823  0.89041417 -0.2826522   0.55803733  1.67530985 -1.50716127\n  0.44839956 -0.75002047  0.19454055  0.41988745]\nVector for 'Python': [ 1.3746919  -0.69313117  1.20280712  0.33296481  1.0498198  -0.30617252\n -0.68207893 -1.63528049  0.11746197  0.18804705]\nVector for 'programming': [ 0.88953518  0.3960943   0.08324377 -0.81845185  0.45232069  0.50849579\n  0.31470319 -0.28788132 -2.10671769  0.9837999 ]\nVector for 'skills': [-0.61114903 -1.23867723  0.67501036 -0.20070013  0.45251135  1.06565594\n  1.98702809  0.97876004  0.29505154  0.0113592 ]\nVector for 'versatile': [ 0.6889747  -0.8286536  -0.93369097  0.45645706  1.77826296 -0.11065061\n -0.17642369 -0.38036503  1.33092187 -0.6485383 ]\nVector for 'is': [-1.42321806  1.03466665 -0.45712698  1.15218139  0.04484756 -0.21603108\n  0.33568069  0.13245389 -0.43471618 -0.76258933]\nVector for 'in': [ 1.22477025  0.09302424  0.48840564  0.34210232 -1.54938386  0.06713506\n  0.48047235 -1.5520969  -1.28586721  0.29676856]\nEpoch 100, Loss: 295.9373094225889\nEpoch 200, Loss: 274.5912114033107\nEpoch 300, Loss: 271.589257628458\nEpoch 400, Loss: 270.94384683000214\nEpoch 500, Loss: 270.887818905955\nEpoch 600, Loss: 271.04736753192935\nEpoch 700, Loss: 271.3131621354528\nEpoch 800, Loss: 271.648120274235\nEpoch 900, Loss: 272.0401492641082\nEpoch 1000, Loss: 272.4875456710171\n\nWord Vectors for new documents:\nVector for 'quick': [ 0.84963171 -0.40584142  0.17137242 -1.8239235   1.30300479  1.18797836\n  0.83016292  0.34557914 -0.67271966 -1.01562793]\nVector for 'journey': [-0.33640099  0.42400908 -1.28853498  1.11235157 -0.68733637 -1.4828937\n  1.16960258  1.64052887 -0.04964284  0.26274727]\nVector for 'be': [-1.29584666  1.08229277  1.21521283 -0.76696754  0.70692083  0.58232736\n -0.28705896 -1.90282741  0.43638555  0.51532593]\nVector for 'rain': [-1.75428039 -1.2910116  -1.03591684 -0.72590607 -0.19088637  1.23074524\n -0.7232419   0.01864839  1.13582975 -0.6059139 ]\nVector for 'human': [ 0.89800051  0.99213515 -1.3657362  -0.33661478  0.05095791  1.71125124\n -0.48803223 -0.31073697  0.51263003  1.35208204]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Explanation of the Code\n\n1. **Initialization**:\n   - `window_size`: Defines the size of the context window around the target word.\n   - `embedding_dim`: Dimension of the word vectors (embedding space).\n   - `learning_rate`: Rate at which weights are updated.\n\n2. **Tokenization**:\n   - The `tokenize` method creates a vocabulary from the documents and builds mappings between words and their indices.\n\n3. **Generate Training Data**:\n   - The `generate_training_data` method creates pairs of target words and context words based on the window size.\n\n4. **Training**:\n   - The `train` method initializes the weight matrices and updates them using gradient descent.\n   - For each word-context pair, it computes the hidden layer representation, predicts context probabilities, calculates the error, and updates the weights.\n\n5. **Softmax Function**:\n   - The `softmax` function converts the output layer scores into probabilities, which are used to compute the error and update the weights.\n\n6. **Retrieve Word Vector**:\n   - The `get_word_vector` method retrieves the embedding of a specific word.","metadata":{}}]}